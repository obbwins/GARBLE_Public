{"document_text": "Probability - wikipedia <H1> Probability </H1> Jump to : navigation , search For the mathematical field of probability specifically rather than a general discussion , see Probability theory . For other uses , see Probability ( disambiguation ) . <Table> <Tr> <Th> Probability </Th> </Tr> <Tr> <Td> </Td> </Tr> <Tr> <Td> <Ul> <Li> Outline </Li> <Li> Catalog of articles </Li> <Li> Probabilists </Li> <Li> Glossary </Li> <Li> Notation </Li> <Li> Journals </Li> <Li> Category </Li> </Ul> </Td> </Tr> <Tr> <Td> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> </Td> </Tr> </Table> <Table> <Tr> <Td> Part of a series on </Td> </Tr> <Tr> <Th> Certainty </Th> </Tr> <Tr> <Td> <Ul> <Li> Approximation </Li> <Li> Belief </Li> <Li> Certainty </Li> <Li> Doubt </Li> <Li> Determinism </Li> <Li> Fallibilism </Li> <Li> Fatalism </Li> <Li> Hypothesis </Li> <Li> Justification </Li> <Li> Nihilism </Li> <Li> Proof </Li> <Li> Scientific theory </Li> <Li> Skepticism </Li> <Li> Solipsism </Li> <Li> Theory </Li> <Li> Truth </Li> <Li> Uncertainty </Li> </Ul> </Td> </Tr> <Tr> <Td> <P> Related concepts and fundamentals : </P> <Ul> <Li> Agnosticism </Li> <Li> Epistemology </Li> <Li> Presupposition </Li> <Li> Probability </Li> </Ul> </Td> </Tr> <Tr> <Td> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> </Td> </Tr> </Table> <P> Probability is the measure of the likelihood that an event will occur . See glossary of probability and statistics . Probability is quantified as a number between 0 and 1 , where , loosely speaking , 0 indicates impossibility and 1 indicates certainty . The higher the probability of an event , the more likely it is that the event will occur . A simple example is the tossing of a fair ( unbiased ) coin . Since the coin is fair , the two outcomes ( `` heads '' and `` tails '' ) are both equally probable ; the probability of `` heads '' equals the probability of `` tails '' ; and since no other outcomes are possible , the probability of either `` heads '' or `` tails '' is 1 / 2 ( which could also be written as 0.5 or 50 % ) . </P> <P> These concepts have been given an axiomatic mathematical formalization in probability theory , which is used widely in such areas of study as mathematics , statistics , finance , gambling , science ( in particular physics ) , artificial intelligence / machine learning , computer science , game theory , and philosophy to , for example , draw inferences about the expected frequency of events . Probability theory is also used to describe the underlying mechanics and regularities of complex systems . </P> <P> </P> <H2> Contents </H2> ( hide ) <Ul> <Li> 1 Interpretations </Li> <Li> 2 Etymology </Li> <Li> 3 History </Li> <Li> 4 Theory </Li> <Li> 5 Applications </Li> <Li> 6 Mathematical treatment <Ul> <Li> 6.1 Independent events </Li> <Li> 6.2 Mutually exclusive events </Li> <Li> 6.3 Not mutually exclusive events </Li> <Li> 6.4 Conditional probability </Li> <Li> 6.5 Inverse probability </Li> <Li> 6.6 Summary of probabilities </Li> </Ul> </Li> <Li> 7 Relation to randomness and probability in quantum mechanics </Li> <Li> 8 See also </Li> <Li> 9 Notes </Li> <Li> 10 Bibliography </Li> <Li> 11 External links </Li> </Ul> <P> </P> <H2> Interpretations ( edit ) </H2> Main article : Probability interpretations <P> When dealing with experiments that are random and well - defined in a purely theoretical setting ( like tossing a fair coin ) , probabilities can be numerically described by the number of desired outcomes divided by the total number of all outcomes . For example , tossing a fair coin twice will yield `` head - head '' , `` head - tail '' , `` tail - head '' , and `` tail - tail '' outcomes . The probability of getting an outcome of `` head - head '' is 1 out of 4 outcomes or 1 / 4 or 0.25 ( or 25 % ) . When it comes to practical application however , there are two major competing categories of probability interpretations , whose adherents possess different views about the fundamental nature of probability : </P> <Ol> <Li> Objectivists assign numbers to describe some objective or physical state of affairs . The most popular version of objective probability is frequentist probability , which claims that the probability of a random event denotes the relative frequency of occurrence of an experiment 's outcome , when repeating the experiment . This interpretation considers probability to be the relative frequency `` in the long run '' of outcomes . A modification of this is propensity probability , which interprets probability as the tendency of some experiment to yield a certain outcome , even if it is performed only once . </Li> <Li> Subjectivists assign numbers per subjective probability , i.e. , as a degree of belief . The degree of belief has been interpreted as , `` the price at which you would buy or sell a bet that pays 1 unit of utility if E , 0 if not E . '' The most popular version of subjective probability is Bayesian probability , which includes expert knowledge as well as experimental data to produce probabilities . The expert knowledge is represented by some ( subjective ) prior probability distribution . These data are incorporated in a likelihood function . The product of the prior and the likelihood , normalized , results in a posterior probability distribution that incorporates all the information known to date . By Aumann 's agreement theorem , Bayesian agents whose prior beliefs are similar will end up with similar posterior beliefs . However , sufficiently different priors can lead to different conclusions regardless of how much information the agents share . </Li> </Ol> <H2> Etymology ( edit ) </H2> See also : History of probability \u00a7 Etymology Further information : Likelihood <P> The word probability derives from the Latin probabilitas , which can also mean `` probity '' , a measure of the authority of a witness in a legal case in Europe , and often correlated with the witness 's nobility . In a sense , this differs much from the modern meaning of probability , which , in contrast , is a measure of the weight of empirical evidence , and is arrived at from inductive reasoning and statistical inference . </P> <H2> History ( edit ) </H2> Main article : History of probability <P> The scientific study of probability is a modern development of mathematics . Gambling shows that there has been an interest in quantifying the ideas of probability for millennia , but exact mathematical descriptions arose much later . There are reasons for the slow development of the mathematics of probability . Whereas games of chance provided the impetus for the mathematical study of probability , fundamental issues are still obscured by the superstitions of gamblers . </P> Christiaan Huygens likely published the first book on probability <P> According to Richard Jeffrey , `` Before the middle of the seventeenth century , the term ' probable ' ( Latin probabilis ) meant approvable , and was applied in that sense , unequivocally , to opinion and to action . A probable action or opinion was one such as sensible people would undertake or hold , in the circumstances . '' However , in legal contexts especially , ' probable ' could also apply to propositions for which there was good evidence . </P> Gerolamo Cardano <P> The sixteenth century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes ( which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes ) . Aside from the elementary work by Cardano , the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal ( 1654 ) . Christiaan Huygens ( 1657 ) gave the earliest known scientific treatment of the subject . Jakob Bernoulli 's Ars Conjectandi ( posthumous , 1713 ) and Abraham de Moivre 's Doctrine of Chances ( 1718 ) treated the subject as a branch of mathematics . See Ian Hacking 's The Emergence of Probability and James Franklin 's The Science of Conjecture for histories of the early development of the very concept of mathematical probability . </P> <P> The theory of errors may be traced back to Roger Cotes 's Opera Miscellanea ( posthumous , 1722 ) , but a memoir prepared by Thomas Simpson in 1755 ( printed 1756 ) first applied the theory to the discussion of errors of observation . The reprint ( 1757 ) of this memoir lays down the axioms that positive and negative errors are equally probable , and that certain assignable limits define the range of all errors . Simpson also discusses continuous errors and describes a probability curve . </P> <P> The first two laws of error that were proposed both originated with Pierre - Simon Laplace . The first law was published in 1774 and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the error , disregarding sign . The second law of error was proposed in 1778 by Laplace and stated that the frequency of the error is an exponential function of the square of the error . The second law of error is called the normal distribution or the Gauss law . `` It is difficult historically to attribute that law to Gauss , who in spite of his well - known precocity had probably not made this discovery before he was two years old . '' </P> <P> Daniel Bernoulli ( 1778 ) introduced the principle of the maximum product of the probabilities of a system of concurrent errors . </P> Carl Friedrich Gauss <P> Adrien - Marie Legendre ( 1805 ) developed the method of least squares , and introduced it in his Nouvelles m\u00e9thodes pour la d\u00e9termination des orbites des com\u00e8tes ( New Methods for Determining the Orbits of Comets ) . In ignorance of Legendre 's contribution , an Irish - American writer , Robert Adrain , editor of `` The Analyst '' ( 1808 ) , first deduced the law of facility of error , </P> <Dl> <Dd> \u03c6 ( x ) = c e \u2212 h 2 x 2 , ( \\ displaystyle \\ phi ( x ) = ce ^ ( - h ^ ( 2 ) x ^ ( 2 ) ) , ) </Dd> </Dl> <P> where h ( \\ displaystyle h ) is a constant depending on precision of observation , and c ( \\ displaystyle c ) is a scale factor ensuring that the area under the curve equals 1 . He gave two proofs , the second being essentially the same as John Herschel 's ( 1850 ) . Gauss gave the first proof that seems to have been known in Europe ( the third after Adrain 's ) in 1809 . Further proofs were given by Laplace ( 1810 , 1812 ) , Gauss ( 1823 ) , James Ivory ( 1825 , 1826 ) , Hagen ( 1837 ) , Friedrich Bessel ( 1838 ) , W.F. Donkin ( 1844 , 1856 ) , and Morgan Crofton ( 1870 ) . Other contributors were Ellis ( 1844 ) , De Morgan ( 1864 ) , Glaisher ( 1872 ) , and Giovanni Schiaparelli ( 1875 ) . Peters 's ( 1856 ) formula for r , the probable error of a single observation , is well known . </P> <P> In the nineteenth century authors on the general theory included Laplace , Sylvestre Lacroix ( 1816 ) , Littrow ( 1833 ) , Adolphe Quetelet ( 1853 ) , Richard Dedekind ( 1860 ) , Helmert ( 1872 ) , Hermann Laurent ( 1873 ) , Liagre , Didion , and Karl Pearson . Augustus De Morgan and George Boole improved the exposition of the theory . </P> <P> Andrey Markov introduced the notion of Markov chains ( 1906 ) , which played an important role in stochastic processes theory and its applications . The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov ( 1931 ) . </P> <P> On the geometric side ( see integral geometry ) contributors to The Educational Times were influential ( Miller , Crofton , McColl , Wolstenholme , Watson , and Artemas Martin ) . </P> Further information : History of statistics <H2> Theory ( edit ) </H2> Main article : Probability theory <P> Like other theories , the theory of probability is a representation of its concepts in formal terms -- that is , in terms that can be considered separately from their meaning . These formal terms are manipulated by the rules of mathematics and logic , and any results are interpreted or translated back into the problem domain . </P> <P> There have been at least two successful attempts to formalize probability , namely the Kolmogorov formulation and the Cox formulation . In Kolmogorov 's formulation ( see probability space ) , sets are interpreted as events and probability itself as a measure on a class of sets . In Cox 's theorem , probability is taken as a primitive ( that is , not further analyzed ) and the emphasis is on constructing a consistent assignment of probability values to propositions . In both cases , the laws of probability are the same , except for technical details . </P> <P> There are other methods for quantifying uncertainty , such as the Dempster -- Shafer theory or possibility theory , but those are essentially different and not compatible with the laws of probability as usually understood . </P> <H2> Applications ( edit ) </H2> <P> Probability theory is applied in everyday life in risk assessment and modeling . The insurance industry and markets use actuarial science to determine pricing and make trading decisions . Governments apply probabilistic methods in environmental regulation , entitlement analysis ( Reliability theory of aging and longevity ) , and financial regulation . </P> <P> A good example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices , which have ripple effects in the economy as a whole . An assessment by a commodity trader that a war is more likely can send that commodity 's prices up or down , and signals other traders of that opinion . Accordingly , the probabilities are neither assessed independently nor necessarily very rationally . The theory of behavioral finance emerged to describe the effect of such groupthink on pricing , on policy , and on peace and conflict . </P> <P> In addition to financial assessment , probability can be used to analyze trends in biology ( e.g. disease spread ) as well as ecology ( e.g. biological Punnett squares ) . As with finance , risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring and can assist with implementing protocols to avoid encountering such circumstances . Probability is used to design games of chance so that casinos can make a guaranteed profit , yet provide payouts to players that are frequent enough to encourage continued play . </P> <P> The discovery of rigorous methods to assess and combine probability assessments has changed society . It is important for most citizens to understand how probability assessments are made , and how they contribute to decisions . </P> <P> Another significant application of probability theory in everyday life is reliability . Many consumer products , such as automobiles and consumer electronics , use reliability theory in product design to reduce the probability of failure . Failure probability may influence a manufacturer 's decisions on a product 's warranty . </P> <P> The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory . </P> <H2> Mathematical treatment ( edit ) </H2> See also : Probability axioms <P> Consider an experiment that can produce a number of results . The collection of all possible results is called the sample space of the experiment . The power set of the sample space is formed by considering all different collections of possible results . For example , rolling a dice can produce six possible results . One collection of possible results gives an odd number on the dice . Thus , the subset ( 1 , 3 , 5 ) is an element of the power set of the sample space of dice rolls . These collections are called `` events '' . In this case , ( 1 , 3 , 5 ) is the event that the dice falls on some odd number . If the results that actually occur fall in a given event , the event is said to have occurred . </P> <P> A probability is a way of assigning every event a value between zero and one , with the requirement that the event made up of all possible results ( in our example , the event ( 1 , 2 , 3 , 4 , 5 , 6 ) ) is assigned a value of one . To qualify as a probability , the assignment of values must satisfy the requirement that if you look at a collection of mutually exclusive events ( events with no common results , e.g. , the events ( 1 , 6 ) , ( 3 ) , and ( 2 , 4 ) are all mutually exclusive ) , the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events . </P> <P> The probability of an event A is written as P ( A ) ( \\ displaystyle P ( A ) ) , p ( A ) ( \\ displaystyle p ( A ) ) , or Pr ( A ) ( \\ displaystyle ( \\ text ( Pr ) ) ( A ) ) . This mathematical definition of probability can extend to infinite sample spaces , and even uncountable sample spaces , using the concept of a measure . </P> <P> The opposite or complement of an event A is the event ( not A ) ( that is , the event of A not occurring ) , often denoted as A _\u0304 , A \u2201 , \u00ac A ( \\ displaystyle ( \\ overline ( A ) ) , A ^ ( \\ complement ) , \\ neg A ) , or \u223c A ( \\ displaystyle ( \\ sim ) A ) ; its probability is given by P ( not A ) = 1 \u2212 P ( A ) . As an example , the chance of not rolling a six on a six - sided die is 1 -- ( chance of rolling a six ) = 1 \u2212 1 6 = 5 6 ( \\ displaystyle = 1 - ( \\ tfrac ( 1 ) ( 6 ) ) = ( \\ tfrac ( 5 ) ( 6 ) ) ) . See Complementary event for a more complete treatment . </P> <P> If two events A and B occur on a single performance of an experiment , this is called the intersection or joint probability of A and B , denoted as P ( A \u2229 B ) ( \\ displaystyle P ( A \\ cap B ) ) . </P> <H3> Independent events ( edit ) </H3> <P> If two events , A and B are independent then the joint probability is </P> <Dl> <Dd> P ( A and B ) = P ( A \u2229 B ) = P ( A ) P ( B ) , ( \\ displaystyle P ( A ( \\ mbox ( and ) ) B ) = P ( A \\ cap B ) = P ( A ) P ( B ) , \\ , ) </Dd> </Dl> <P> for example , if two coins are flipped the chance of both being heads is 1 2 \u00d7 1 2 = 1 4 ( \\ displaystyle ( \\ tfrac ( 1 ) ( 2 ) ) \\ times ( \\ tfrac ( 1 ) ( 2 ) ) = ( \\ tfrac ( 1 ) ( 4 ) ) ) . </P> <H3> Mutually exclusive events ( edit ) </H3> <P> If either event A or event B but never both occurs on a single performance of an experiment , then they are called mutually exclusive events . </P> <P> If two events are mutually exclusive then the probability of both occurring is denoted as P ( A \u2229 B ) ( \\ displaystyle P ( A \\ cap B ) ) . </P> <Dl> <Dd> P ( A and B ) = P ( A \u2229 B ) = 0 ( \\ displaystyle P ( A ( \\ mbox ( and ) ) B ) = P ( A \\ cap B ) = 0 ) </Dd> </Dl> <P> If two events are mutually exclusive then the probability of either occurring is denoted as P ( A \u222a B ) ( \\ displaystyle P ( A \\ cup B ) ) . </P> <Dl> <Dd> P ( A or B ) = P ( A \u222a B ) = P ( A ) + P ( B ) \u2212 P ( A \u2229 B ) = P ( A ) + P ( B ) \u2212 0 = P ( A ) + P ( B ) ( \\ displaystyle P ( A ( \\ mbox ( or ) ) B ) = P ( A \\ cup B ) = P ( A ) + P ( B ) - P ( A \\ cap B ) = P ( A ) + P ( B ) - 0 = P ( A ) + P ( B ) ) </Dd> </Dl> <P> For example , the chance of rolling a 1 or 2 on a six - sided die is P ( 1 or 2 ) = P ( 1 ) + P ( 2 ) = 1 6 + 1 6 = 1 3 . ( \\ displaystyle P ( 1 ( \\ mbox ( or ) ) 2 ) = P ( 1 ) + P ( 2 ) = ( \\ tfrac ( 1 ) ( 6 ) ) + ( \\ tfrac ( 1 ) ( 6 ) ) = ( \\ tfrac ( 1 ) ( 3 ) ) . ) </P> <H3> Not mutually exclusive events ( edit ) </H3> <P> If the events are not mutually exclusive then </P> <Dl> <Dd> P ( A or B ) = P ( A \u222a B ) = P ( A ) + P ( B ) \u2212 P ( A and B ) . ( \\ displaystyle P \\ left ( A ( \\ hbox ( or ) ) B \\ right ) = P ( A \\ cup B ) = P \\ left ( A \\ right ) + P \\ left ( B \\ right ) - P \\ left ( A ( \\ mbox ( and ) ) B \\ right ) . ) </Dd> </Dl> <P> For example , when drawing a single card at random from a regular deck of cards , the chance of getting a heart or a face card ( J , Q , K ) ( or one that is both ) is 13 52 + 12 52 \u2212 3 52 = 11 26 ( \\ displaystyle ( \\ tfrac ( 13 ) ( 52 ) ) + ( \\ tfrac ( 12 ) ( 52 ) ) - ( \\ tfrac ( 3 ) ( 52 ) ) = ( \\ tfrac ( 11 ) ( 26 ) ) ) , because of the 52 cards of a deck 13 are hearts , 12 are face cards , and 3 are both : here the possibilities included in the `` 3 that are both '' are included in each of the `` 13 hearts '' and the `` 12 face cards '' but should only be counted once . </P> <H3> Conditional probability ( edit ) </H3> <P> Conditional probability is the probability of some event A , given the occurrence of some other event B. Conditional probability is written P ( A \u2223 B ) ( \\ displaystyle P ( A \\ mid B ) ) , and is read `` the probability of A , given B '' . It is defined by </P> <Dl> <Dd> P ( A \u2223 B ) = P ( A \u2229 B ) P ( B ) . ( \\ displaystyle P ( A \\ mid B ) = ( \\ frac ( P ( A \\ cap B ) ) ( P ( B ) ) ). \\ , ) </Dd> </Dl> <P> If P ( B ) = 0 ( \\ displaystyle P ( B ) = 0 ) then P ( A \u2223 B ) ( \\ displaystyle P ( A \\ mid B ) ) is formally undefined by this expression . However , it is possible to define a conditional probability for some zero - probability events using a \u03c3 - algebra of such events ( such as those arising from a continuous random variable ) . </P> <P> For example , in a bag of 2 red balls and 2 blue balls ( 4 balls in total ) , the probability of taking a red ball is 1 / 2 ( \\ displaystyle 1 / 2 ) ; however , when taking a second ball , the probability of it being either a red ball or a blue ball depends on the ball previously taken , such as , if a red ball was taken , the probability of picking a red ball again would be 1 / 3 ( \\ displaystyle 1 / 3 ) since only 1 red and 2 blue balls would have been remaining . </P> <H3> Inverse probability ( edit ) </H3> <P> In probability theory and applications , Bayes ' rule relates the odds of event A 1 ( \\ displaystyle A_ ( 1 ) ) to event A 2 ( \\ displaystyle A_ ( 2 ) ) , before ( prior to ) and after ( posterior to ) conditioning on another event B ( \\ displaystyle B ) . The odds on A 1 ( \\ displaystyle A_ ( 1 ) ) to event A 2 ( \\ displaystyle A_ ( 2 ) ) is simply the ratio of the probabilities of the two events . When arbitrarily many events A ( \\ displaystyle A ) are of interest , not just two , the rule can be rephrased as posterior is proportional to prior times likelihood , P ( A B ) \u221d P ( A ) P ( B A ) ( \\ displaystyle P ( A B ) \\ propto P ( A ) P ( B A ) ) where the proportionality symbol means that the left hand side is proportional to ( i.e. , equals a constant times ) the right hand side as A ( \\ displaystyle A ) varies , for fixed or given B ( \\ displaystyle B ) ( Lee , 2012 ; Bertsch McGrayne , 2012 ) . In this form it goes back to Laplace ( 1774 ) and to Cournot ( 1843 ) ; see Fienberg ( 2005 ) . See Inverse probability and Bayes ' rule . </P> <H3> Summary of probabilities ( edit ) </H3> <Table> Summary of probabilities <Tr> <Th> Event </Th> <Th> Probability </Th> </Tr> <Tr> <Td> </Td> <Td> P ( A ) \u2208 ( 0 , 1 ) ( \\ displaystyle P ( A ) \\ in ( 0 , 1 ) \\ , ) </Td> </Tr> <Tr> <Td> not A </Td> <Td> P ( A \u2201 ) = 1 \u2212 P ( A ) ( \\ displaystyle P ( A ^ ( \\ complement ) ) = 1 - P ( A ) \\ , ) </Td> </Tr> <Tr> <Td> A or B </Td> <Td> P ( A \u222a B ) = P ( A ) + P ( B ) \u2212 P ( A \u2229 B ) P ( A \u222a B ) = P ( A ) + P ( B ) if A and B are mutually exclusive ( \\ displaystyle ( \\ begin ( aligned ) P ( A \\ cup B ) & = P ( A ) + P ( B ) - P ( A \\ cap B ) \\ \\ P ( A \\ cup B ) & = P ( A ) + P ( B ) \\ qquad ( \\ mbox ( if A and B are mutually exclusive ) ) \\ \\ \\ end ( aligned ) ) ) </Td> </Tr> <Tr> <Td> A and B </Td> <Td> P ( A \u2229 B ) = P ( A B ) P ( B ) = P ( B A ) P ( A ) P ( A \u2229 B ) = P ( A ) P ( B ) if A and B are independent ( \\ displaystyle ( \\ begin ( aligned ) P ( A \\ cap B ) & = P ( A B ) P ( B ) = P ( B A ) P ( A ) \\ \\ P ( A \\ cap B ) & = P ( A ) P ( B ) \\ qquad ( \\ mbox ( if A and B are independent ) ) \\ \\ \\ end ( aligned ) ) ) </Td> </Tr> <Tr> <Td> A given B </Td> <Td> P ( A \u2223 B ) = P ( A \u2229 B ) P ( B ) = P ( B A ) P ( A ) P ( B ) ( \\ displaystyle P ( A \\ mid B ) = ( \\ frac ( P ( A \\ cap B ) ) ( P ( B ) ) ) = ( \\ frac ( P ( B A ) P ( A ) ) ( P ( B ) ) ) \\ , ) </Td> </Tr> </Table> <H2> Relation to randomness and probability in quantum mechanics ( edit ) </H2> Main article : Randomness See also : Quantum fluctuation \u00a7 Interpretations <Table> <Tr> <Td> </Td> <Td> This section needs expansion . You can help by adding to it . ( April 2017 ) </Td> </Tr> </Table> <P> In a deterministic universe , based on Newtonian concepts , there would be no probability if all conditions were known ( Laplace 's demon ) , ( but there are situations in which sensitivity to initial conditions exceeds our ability to measure them , i.e. know them ) . In the case of a roulette wheel , if the force of the hand and the period of that force are known , the number on which the ball will stop would be a certainty ( though as a practical matter , this would likely be true only of a roulette wheel that had not been exactly levelled -- as Thomas A. Bass ' Newtonian Casino revealed ) . This also assumes knowledge of inertia and friction of the wheel , weight , smoothness and roundness of the ball , variations in hand speed during the turning and so forth . A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel . Physicists face the same situation in kinetic theory of gases , where the system , while deterministic in principle , is so complex ( with the number of molecules typically the order of magnitude of the Avogadro constant 7023602000000000000 \u2660 6.02 \u00d7 10 ) that only a statistical description of its properties is feasible . </P> <P> Probability theory is required to describe quantum phenomena . A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics . The objective wave function evolves deterministically but , according to the Copenhagen interpretation , it deals with probabilities of observing , the outcome being explained by a wave function collapse when an observation is made . However , the loss of determinism for the sake of instrumentalism did not meet with universal approval . Albert Einstein famously remarked in a letter to Max Born : `` I am convinced that God does not play dice '' . Like Einstein , Erwin Schr\u00f6dinger , who discovered the wave function , believed quantum mechanics is a statistical approximation of an underlying deterministic reality . In some modern interpretations of the statistical mechanics of measurement , quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes . </P> <H2> See also ( edit ) </H2> <Ul> <Li> Mathematics portal </Li> <Li> Logic portal </Li> </Ul> Main article : Outline of probability <Ul> <Li> Chance ( disambiguation ) </Li> <Li> Class membership probabilities </Li> <Li> Equiprobability </Li> <Li> Heuristics in judgment and decision - making </Li> <Li> Probability theory </Li> <Li> Statistics </Li> <Li> Estimators </Li> <Li> Estimation Theory </Li> <Li> Probability density function </Li> </Ul> <Dl> <Dt> In Law </Dt> </Dl> <Ul> <Li> Balance of probabilities </Li> </Ul> <H2> Notes ( edit ) </H2> <Ol> <Li> Jump up ^ `` Probability '' . Webster 's Revised Unabridged Dictionary . G & C Merriam , 1913 </Li> <Li> Jump up ^ Strictly speaking , a probability of 0 indicates that an event almost never takes place , whereas a probability of 1 indicates than an event almost certainly takes place . This is an important distinction when the sample space is infinite . For example , for the continuous uniform distribution on the real interval ( 5 , 10 ) , there are an infinite number of possible outcomes , and the probability of any given outcome being observed -- for instance , exactly 7 -- is 0 . This means that when we make an observation , it will almost surely not be exactly 7 . However , it does not mean that exactly 7 is impossible . Ultimately some specific outcome ( with probability 0 ) will be observed , and one possibility for that specific outcome is exactly 7 . </Li> <Li> Jump up ^ `` Kendall 's Advanced Theory of Statistics , Volume 1 : Distribution Theory '' , Alan Stuart and Keith Ord , 6th Ed , ( 2009 ) , ISBN 9780534243128 </Li> <Li> Jump up ^ William Feller , `` An Introduction to Probability Theory and Its Applications '' , ( Vol 1 ) , 3rd Ed , ( 1968 ) , Wiley , ISBN 0 - 471 - 25708 - 7 </Li> <Li> Jump up ^ Probability Theory The Britannica website </Li> <Li> Jump up ^ Hacking , Ian ( 1965 ) . The Logic of Statistical Inference . Cambridge University Press . ISBN 0 - 521 - 05165 - 7 . </Li> <Li> Jump up ^ Finetti , Bruno de ( 1970 ) . `` Logical foundations and measurement of subjective probability '' . Acta Psychologica. 34 : 129 -- 145 . doi : 10.1016 / 0001 - 6918 ( 70 ) 90012 - 0 . </Li> <Li> Jump up ^ H\u00e1jek , Alan . `` Interpretations of Probability '' . The Stanford Encyclopedia of Philosophy ( Winter 2012 Edition ) , Edward N. Zalta ( ed . ) . Retrieved 22 April 2013 . </Li> <Li> Jump up ^ Hogg , Robert V. ; Craig , Allen ; McKean , Joseph W. ( 2004 ) . Introduction to Mathematical Statistics ( 6th ed . ) . Upper Saddle River : Pearson . ISBN 0 - 13 - 008507 - 3 . </Li> <Li> Jump up ^ Jaynes , E.T. ( 2003 - 06 - 09 ) . `` Section 5.3 Converging and diverging views '' . In Bretthorst , G. Larry . Probability Theory : The Logic of Science ( 1 ed . ) . Cambridge University Press . ISBN 9780521592710 . </Li> <Li> ^ Jump up to : Hacking , I. ( 2006 ) The Emergence of Probability : A Philosophical Study of Early Ideas about Probability , Induction and Statistical Inference , Cambridge University Press , ISBN 978 - 0 - 521 - 68557 - 3 </Li> <Li> Jump up ^ Freund , John . ( 1973 ) Introduction to Probability . Dickenson ISBN 978 - 0822100782 ( p. 1 ) </Li> <Li> Jump up ^ Jeffrey , R.C. , Probability and the Art of Judgment , Cambridge University Press . ( 1992 ) . pp. 54 -- 55 . ISBN 0 - 521 - 39459 - 7 </Li> <Li> Jump up ^ Franklin , J. ( 2001 ) The Science of Conjecture : Evidence and Probability Before Pascal , Johns Hopkins University Press . ( pp. 22 , 113 , 127 ) </Li> <Li> Jump up ^ Some laws and problems in classical probability and how Cardano anticipated them Gorrochum , P. Chance magazine 2012 </Li> <Li> Jump up ^ Abrams , William , A Brief History of Probability , Second Moment , retrieved 2008 - 05 - 23 </Li> <Li> Jump up ^ Ivancevic , Vladimir G. ; Ivancevic , Tijana T. ( 2008 ) . Quantum leap : from Dirac and Feynman , across the universe , to human body and mind . Singapore ; Hackensack , NJ : World Scientific . p. 16 . ISBN 978 - 981 - 281 - 927 - 7 . </Li> <Li> Jump up ^ Franklin , James ( 2001 ) . The Science of Conjecture : Evidence and Probability Before Pascal . Johns Hopkins University Press . ISBN 0801865697 . </Li> <Li> ^ Jump up to : Wilson EB ( 1923 ) `` First and second laws of error '' . Journal of the American Statistical Association , 18 , 143 </Li> <Li> Jump up ^ Seneta , Eugene William . `` '' Adrien - Marie Legendre '' ( version 9 ) `` . StatProb : The Encyclopedia Sponsored by Statistics and Probability Societies . Archived from the original on 3 February 2016 . Retrieved 27 January 2016 . </Li> <Li> Jump up ^ http://REMOVED_SECRET/~rrw1/markov/M.pdf </Li> <Li> Jump up ^ Vitanyi , Paul M.B. ( 1988 ) . `` Andrei Nikolaevich Kolmogorov '' . CWI Quarterly ( 1 ) : 3 -- 18 . Retrieved 27 January 2016 . </Li> <Li> Jump up ^ Singh , Laurie ( 2010 ) `` Whither Efficient Markets ? Efficient Market Theory and Behavioral Finance '' . The Finance Professionals ' Post , 2010 . </Li> <Li> Jump up ^ Gao , J.Z. ; Fong , D. ; Liu , X. ( April 2011 ) . `` Mathematical analyses of casino rebate systems for VIP gambling '' . International Gambling Studies . 11 ( 1 ) : 93 -- 106 . doi : 10.1080 / 14459795.2011. 552575 . </Li> <Li> ^ Jump up to : `` Data : Data Analysis , Probability and Statistics , and Graphing '' . REMOVED_SECRET.edu . Retrieved 2017 - 05 - 28 . </Li> <Li> Jump up ^ Gorman , Michael ( 2011 ) `` Management Insights '' . Management Science </Li> <Li> Jump up ^ Ross , Sheldon . A First course in Probability , 8th Edition . Pages 26 -- 27 . </Li> <Li> Jump up ^ Olofsson ( 2005 ) Page 8 . </Li> <Li> Jump up ^ Olofsson ( 2005 ) , page 9 </Li> <Li> Jump up ^ Olofsson ( 2005 ) page 35 . </Li> <Li> Jump up ^ Olofsson ( 2005 ) page 29 . </Li> <Li> Jump up ^ Burgi , Mark ( 2010 ) `` Interpretations of Negative Probabilities '' , p. 1 . arXiv : 1008.1287 v1 </Li> <Li> Jump up ^ Jedenfalls bin ich \u00fcberzeugt , da\u00df der Alte nicht w\u00fcrfelt . Letter to Max Born , 4 December 1926 , in : Einstein / Born Briefwechsel 1916 - 1955 . </Li> <Li> Jump up ^ Moore , W.J. ( 1992 ) . Schr\u00f6dinger : Life and Thought . Cambridge University Press . p. 479 . ISBN 0 - 521 - 43767 - 9 . </Li> </Ol> <H2> Bibliography ( edit ) </H2> <Ul> <Li> Kallenberg , O. ( 2005 ) Probabilistic Symmetries and Invariance Principles . Springer - Verlag , New York . 510 pp. ISBN 0 - 387 - 25115 - 4 </Li> <Li> Kallenberg , O. ( 2002 ) Foundations of Modern Probability , 2nd ed . Springer Series in Statistics . 650 pp. ISBN 0 - 387 - 95313 - 2 </Li> <Li> Olofsson , Peter ( 2005 ) Probability , Statistics , and Stochastic Processes , Wiley - Interscience. 504 pp ISBN 0 - 471 - 67969 - 0 . </Li> </Ul> <H2> External links ( edit ) </H2> <Table> <Tr> <Td> </Td> <Td> Wikiquote has quotations related to : Probability </Td> </Tr> </Table> <Table> <Tr> <Td> </Td> <Td> Wikibooks has more on the topic of : Probability </Td> </Tr> </Table> <Table> <Tr> <Td> </Td> <Td> Wikimedia Commons has media related to Probability . </Td> </Tr> </Table> <Table> <Tr> <Td> Library resources about Probability </Td> </Tr> <Tr> <Td> <Ul> <Li> Resources in your library </Li> </Ul> </Td> </Tr> </Table> <Ul> <Li> Virtual Laboratories in Probability and Statistics ( Univ. of Ala. - Huntsville ) </Li> <Li> Probability on In Our Time at the BBC . </Li> <Li> Probability and Statistics EBook </Li> <Li> Edwin Thompson Jaynes . Probability Theory : The Logic of Science . Preprint : Washington University , ( 1996 ) . -- HTML index with links to PostScript files and PDF ( first three chapters ) </Li> <Li> People from the History of Probability and Statistics ( Univ. of Southampton ) </Li> <Li> Probability and Statistics on the Earliest Uses Pages ( Univ. of Southampton ) </Li> <Li> Earliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols </Li> <Li> A tutorial on probability and Bayes ' theorem devised for first - year Oxford University students </Li> <Li> ( 1 ) pdf file of An Anthology of Chance Operations ( 1963 ) at UbuWeb </Li> <Li> Introduction to Probability - eBook , by Charles Grinstead , Laurie Snell Source ( GNU Free Documentation License ) </Li> <Li> ( in English ) ( in Italian ) Bruno de Finetti , Probabilit\u00e0 e induzione , Bologna , CLUEB , 1993 . ISBN 88 - 8091 - 176 - 7 ( digital version ) </Li> <Li> Richard P. Feynman 's Lecture on probability . </Li> </Ul> <Table> <Tr> <Th_colspan=\"2\"> ( hide ) <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> Logic </Th> </Tr> <Tr> <Td_colspan=\"2\"> <Ul> <Li> Outline </Li> <Li> History </Li> </Ul> </Td> </Tr> <Tr> <Th> Fields </Th> <Td> <Ul> <Li> Argumentation theory </Li> <Li> Axiology </Li> <Li> Critical thinking </Li> <Li> Logic in computer science </Li> <Li> Mathematical logic </Li> <Li> Metalogic </Li> <Li> Metamathematics </Li> <Li> Non-classical logic </Li> <Li> Philosophical logic </Li> <Li> Philosophy of logic </Li> <Li> Set theory </Li> </Ul> </Td> </Tr> <Tr> <Th> Foundations </Th> <Td> <Ul> <Li> Abduction </Li> <Li> Analytic and synthetic propositions </Li> <Li> Antinomy </Li> <Li> A priori and a posteriori </Li> <Li> Deduction </Li> <Li> Definition </Li> <Li> Description </Li> <Li> Induction </Li> <Li> Inference </Li> <Li> Logical form </Li> <Li> Logical consequence </Li> <Li> Logical truth </Li> <Li> Name </Li> <Li> Necessity and sufficiency </Li> <Li> Meaning </Li> <Li> Paradox </Li> <Li> Possible world </Li> <Li> Presupposition </Li> <Li> Probability </Li> <Li> Reason </Li> <Li> Reference </Li> <Li> Semantics </Li> <Li> Statement </Li> <Li> Strict implication </Li> <Li> Substitution </Li> <Li> Syntax </Li> <Li> Truth </Li> <Li> Validity </Li> </Ul> </Td> </Tr> <Tr> <Th> Lists </Th> <Td> <Table> <Tr> <Th> topics </Th> <Td> <Ul> <Li> Mathematical logic </Li> <Li> Boolean algebra </Li> <Li> Set theory </Li> </Ul> </Td> </Tr> <Tr> <Th> other </Th> <Td> <Ul> <Li> Logicians </Li> <Li> Rules of inference </Li> <Li> Paradoxes </Li> <Li> Fallacies </Li> <Li> Logic symbols </Li> </Ul> </Td> </Tr> </Table> </Td> </Tr> <Tr> <Td_colspan=\"2\"> <Ul> <Li> Portal </Li> <Li> Category </Li> <Li> WikiProject ( talk ) </Li> <Li> changes </Li> </Ul> </Td> </Tr> </Table> <Table> <Tr> <Th_colspan=\"2\"> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> Glossaries of science and engineering </Th> </Tr> <Tr> <Td_colspan=\"2\"> <Ul> <Li> Aerospace engineering </Li> <Li> Archaeology </Li> <Li> Architecture </Li> <Li> Artificial intelligence </Li> <Li> Astronomy </Li> <Li> Biology </Li> <Li> Botany </Li> <Li> Calculus </Li> <Li> Chemistry </Li> <Li> Civil engineering </Li> <Li> Clinical research </Li> <Li> Computer science </Li> <Li> Ecology </Li> <Li> Economics </Li> <Li> Electrical and electronics engineering </Li> <Li> Engineering </Li> <Li> Entomology </Li> <Li> Environmental science </Li> <Li> Genetics </Li> <Li> Geography </Li> <Li> Geology </Li> <Li> Machine vision </Li> <Li> Mathematics </Li> <Li> Mechanical engineering </Li> <Li> Medicine </Li> <Li> Physics </Li> <Li> Probability and statistics </Li> <Li> Psychiatry </Li> <Li> Robotics </Li> <Li> Speciation </Li> <Li> Structural engineering </Li> </Ul> </Td> </Tr> </Table> <Ul> <Li> Statistics portal </Li> <Li> Mathematics portal </Li> </Ul> <Table> <Tr> <Th> </Th> <Td> <Ul> <Li> GND : 4137007 - 7 </Li> </Ul> </Td> </Tr> </Table> Retrieved from `` https://REMOVED_SECRET/w/index.php?title=Probability&oldid=841773849 '' Categories : <Ul> <Li> Probability </Li> <Li> Dimensionless numbers </Li> </Ul> Hidden categories : <Ul> <Li> Wikipedia articles needing page number citations from June 2012 </Li> <Li> Articles needing more detailed references </Li> <Li> Use dmy dates from October 2013 </Li> <Li> Wikipedia articles needing clarification from July 2014 </Li> <Li> All articles with unsourced statements </Li> <Li> Articles with unsourced statements from February 2012 </Li> <Li> Articles with unsourced statements from June 2012 </Li> <Li> Wikipedia articles needing clarification from June 2012 </Li> <Li> Articles with unsourced statements from October 2015 </Li> <Li> Articles with unsourced statements from July 2012 </Li> <Li> Articles to be expanded from April 2017 </Li> <Li> All articles to be expanded </Li> <Li> Articles using small message boxes </Li> <Li> Articles with Italian - language external links </Li> <Li> Wikipedia articles with GND identifiers </Li> </Ul> <H2> </H2> <H3> </H3> <Ul> <Li> </Li> <Li> Talk </Li> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> </Ul> <H3> </H3> <H3> </H3> <Ul> <Li> </Li> <Li> Contents </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> About Wikipedia </Li> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <H3> </H3> <Ul> <Li> </Li> <Li> Wikibooks </Li> <Li> Wikiquote </Li> </Ul> <H3> </H3> <Ul> <Li> Afrikaans </Li> <Li> Alemannisch </Li> <Li> \u12a0\u121b\u122d\u129b </Li> <Li> </Li> <Li> Aragon\u00e9s </Li> <Li> Asturianu </Li> <Li> Aymar aru </Li> <Li> Az\u0259rbaycanca </Li> <Li> \u062a\u06c6\u0631\u06a9\u062c\u0647 </Li> <Li> \u09ac\u09be\u0982\u09b2\u09be </Li> <Li> B\u00e2n - l\u00e2m - g\u00fa </Li> <Li> \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430 </Li> <Li> \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f </Li> <Li> \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f ( \u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430 ) \u200e </Li> <Li> \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 </Li> <Li> Boarisch </Li> <Li> Bosanski </Li> <Li> Catal\u00e0 </Li> <Li> \u0427\u04d1\u0432\u0430\u0448\u043b\u0430 </Li> <Li> \u010ce\u0161tina </Li> <Li> Cymraeg </Li> <Li> Dansk </Li> <Li> Deutsch </Li> <Li> Eesti </Li> <Li> \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac </Li> <Li> Espa\u00f1ol </Li> <Li> Esperanto </Li> <Li> Euskara </Li> <Li> \u0641\u0627\u0631\u0633\u06cc </Li> <Li> Fran\u00e7ais </Li> <Li> Gaeilge </Li> <Li> Galego </Li> <Li> \u8d1b \u8a9e </Li> <Li> \ud55c\uad6d\uc5b4 </Li> <Li> \u0540\u0561\u0575\u0565\u0580\u0565\u0576 </Li> <Li> \u0939\u093f\u0928\u094d\u0926\u0940 </Li> <Li> Hrvatski </Li> <Li> Ido </Li> <Li> Ilokano </Li> <Li> Bahasa Indonesia </Li> <Li> \u00cdslenska </Li> <Li> Italiano </Li> <Li> \u05e2\u05d1\u05e8\u05d9\u05ea </Li> <Li> \u0c95\u0ca8\u0ccd\u0ca8\u0ca1 </Li> <Li> \u10e5\u10d0\u10e0\u10d7\u10e3\u10da\u10d8 </Li> <Li> \u049a\u0430\u0437\u0430\u049b\u0448\u0430 </Li> <Li> Kiswahili </Li> <Li> Latina </Li> <Li> Latvie\u0161u </Li> <Li> L\u00ebtzebuergesch </Li> <Li> Lietuvi\u0173 </Li> <Li> \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 </Li> <Li> \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02 </Li> <Li> Malti </Li> <Li> Bahasa Melayu </Li> <Li> \u1019\u103c\u1014\u103a\u1019\u102c\u1018\u102c\u101e\u102c </Li> <Li> Nederlands </Li> <Li> \u65e5\u672c \u8a9e </Li> <Li> \u041d\u043e\u0445\u0447\u0438\u0439\u043d </Li> <Li> Norsk </Li> <Li> Occitan </Li> <Li> O\u02bbzbekcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 </Li> <Li> \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40 </Li> <Li> \u067e\u0646\u062c\u0627\u0628\u06cc </Li> <Li> Patois </Li> <Li> Picard </Li> <Li> Piemont\u00e8is </Li> <Li> Polski </Li> <Li> Portugu\u00eas </Li> <Li> Rom\u00e2n\u0103 </Li> <Li> \u0420\u0443\u0441\u0441\u043a\u0438\u0439 </Li> <Li> Scots </Li> <Li> Shqip </Li> <Li> Sicilianu </Li> <Li> Simple English </Li> <Li> Sloven\u010dina </Li> <Li> Sloven\u0161\u010dina </Li> <Li> Soomaaliga </Li> <Li> \u06a9\u0648\u0631\u062f\u06cc </Li> <Li> \u0421\u0440\u043f\u0441\u043a\u0438 / srpski </Li> <Li> Srpskohrvatski / \u0441\u0440\u043f\u0441\u043a\u043e\u0445\u0440\u0432\u0430\u0442\u0441\u043a\u0438 </Li> <Li> Basa Sunda </Li> <Li> Suomi </Li> <Li> Svenska </Li> <Li> Tagalog </Li> <Li> \u0ba4\u0bae\u0bbf\u0bb4\u0bcd </Li> <Li> \u0422\u0430\u0442\u0430\u0440\u0447\u0430 / tatar\u00e7a </Li> <Li> \u0e44\u0e17\u0e22 </Li> <Li> T\u00fcrk\u00e7e </Li> <Li> \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 </Li> <Li> \u0627\u0631\u062f\u0648 </Li> <Li> V\u00e8neto </Li> <Li> Ti\u1ebfng Vi\u1ec7t </Li> <Li> Winaray </Li> <Li> \u5434\u8bed </Li> <Li> \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9 </Li> <Li> \u7cb5\u8a9e </Li> <Li> \u4e2d\u6587 </Li> 89 more </Ul> Edit links <Ul> <Li> This page was last edited on 17 May 2018 , at 23 : 48 . </Li> <Li> Text is available under the Creative Commons Attribution - ShareAlike License ; additional terms may apply . By using this site , you agree to the Terms of Use and Privacy Policy . Wikipedia \u00ae is a registered trademark of the Wikimedia Foundation , Inc. , a non-profit organization . </Li> </Ul> <Ul> <Li> </Li> <Li> About Wikipedia </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> <Li> </Li> </Ul> <Ul> <Li> </Li> <Li> </Li> </Ul>", "long_answer_candidates": [{"start_token": 39, "top_level": true, "end_token": 91}, {"start_token": 40, "top_level": false, "end_token": 45}, {"start_token": 49, "top_level": false, "end_token": 78}, {"start_token": 51, "top_level": false, "end_token": 76}, {"start_token": 55, "top_level": false, "end_token": 60}, {"start_token": 78, "top_level": false, "end_token": 90}, {"start_token": 80, "top_level": false, "end_token": 88}, {"start_token": 91, "top_level": true, "end_token": 202}, {"start_token": 92, "top_level": false, "end_token": 101}, {"start_token": 101, "top_level": false, "end_token": 106}, {"start_token": 106, "top_level": false, "end_token": 164}, {"start_token": 108, "top_level": false, "end_token": 162}, {"start_token": 164, "top_level": false, "end_token": 189}, {"start_token": 166, "top_level": false, "end_token": 173}, {"start_token": 173, "top_level": false, "end_token": 187}, {"start_token": 189, "top_level": false, "end_token": 201}, {"start_token": 191, "top_level": false, "end_token": 199}, {"start_token": 202, "top_level": true, "end_token": 354}, {"start_token": 354, "top_level": true, "end_token": 438}, {"start_token": 551, "top_level": true, "end_token": 693}, {"start_token": 693, "top_level": true, "end_token": 962}, {"start_token": 694, "top_level": false, "end_token": 795}, {"start_token": 795, "top_level": false, "end_token": 961}, {"start_token": 980, "top_level": true, "end_token": 1063}, {"start_token": 1075, "top_level": true, "end_token": 1152}, {"start_token": 1161, "top_level": true, "end_token": 1247}, {"start_token": 1249, "top_level": true, "end_token": 1400}, {"start_token": 1400, "top_level": true, "end_token": 1491}, {"start_token": 1491, "top_level": true, "end_token": 1623}, {"start_token": 1623, "top_level": true, "end_token": 1647}, {"start_token": 1650, "top_level": true, "end_token": 1726}, {"start_token": 1726, "top_level": true, "end_token": 1769}, {"start_token": 1727, "top_level": false, "end_token": 1768}, {"start_token": 1769, "top_level": true, "end_token": 1948}, {"start_token": 1948, "top_level": true, "end_token": 2017}, {"start_token": 2017, "top_level": true, "end_token": 2063}, {"start_token": 2063, "top_level": true, "end_token": 2097}, {"start_token": 2114, "top_level": true, "end_token": 2174}, {"start_token": 2174, "top_level": true, "end_token": 2275}, {"start_token": 2275, "top_level": true, "end_token": 2313}, {"start_token": 2319, "top_level": true, "end_token": 2373}, {"start_token": 2373, "top_level": true, "end_token": 2483}, {"start_token": 2483, "top_level": true, "end_token": 2580}, {"start_token": 2580, "top_level": true, "end_token": 2618}, {"start_token": 2618, "top_level": true, "end_token": 2670}, {"start_token": 2670, "top_level": true, "end_token": 2697}, {"start_token": 2709, "top_level": true, "end_token": 2857}, {"start_token": 2857, "top_level": true, "end_token": 2998}, {"start_token": 2998, "top_level": true, "end_token": 3080}, {"start_token": 3080, "top_level": true, "end_token": 3247}, {"start_token": 3247, "top_level": true, "end_token": 3297}, {"start_token": 3304, "top_level": true, "end_token": 3320}, {"start_token": 3320, "top_level": true, "end_token": 3383}, {"start_token": 3321, "top_level": false, "end_token": 3382}, {"start_token": 3383, "top_level": true, "end_token": 3446}, {"start_token": 3454, "top_level": true, "end_token": 3483}, {"start_token": 3483, "top_level": true, "end_token": 3518}, {"start_token": 3518, "top_level": true, "end_token": 3563}, {"start_token": 3519, "top_level": false, "end_token": 3562}, {"start_token": 3563, "top_level": true, "end_token": 3598}, {"start_token": 3598, "top_level": true, "end_token": 3718}, {"start_token": 3599, "top_level": false, "end_token": 3717}, {"start_token": 3718, "top_level": true, "end_token": 3824}, {"start_token": 3833, "top_level": true, "end_token": 3843}, {"start_token": 3843, "top_level": true, "end_token": 3942}, {"start_token": 3844, "top_level": false, "end_token": 3941}, {"start_token": 3942, "top_level": true, "end_token": 4103}, {"start_token": 4110, "top_level": true, "end_token": 4169}, {"start_token": 4169, "top_level": true, "end_token": 4224}, {"start_token": 4170, "top_level": false, "end_token": 4223}, {"start_token": 4224, "top_level": true, "end_token": 4304}, {"start_token": 4304, "top_level": true, "end_token": 4417}, {"start_token": 4424, "top_level": true, "end_token": 4676}, {"start_token": 4684, "top_level": true, "end_token": 5136}, {"start_token": 4688, "top_level": false, "end_token": 4696}, {"start_token": 4696, "top_level": false, "end_token": 4729}, {"start_token": 4729, "top_level": false, "end_token": 4771}, {"start_token": 4771, "top_level": false, "end_token": 4905}, {"start_token": 4905, "top_level": false, "end_token": 5040}, {"start_token": 5040, "top_level": false, "end_token": 5135}, {"start_token": 5160, "top_level": true, "end_token": 5185}, {"start_token": 5161, "top_level": false, "end_token": 5184}, {"start_token": 5185, "top_level": true, "end_token": 5417}, {"start_token": 5417, "top_level": true, "end_token": 5587}], "question_text": "which term is defined as the chance that a given event will occur", "annotations": [{"yes_no_answer": "NONE", "long_answer": {"start_token": 202, "candidate_index": 17, "end_token": 354}, "short_answers": [{"start_token": 203, "end_token": 204}], "annotation_id": 14635778377689191524}], "document_url": "https://REMOVED_SECRET//w/index.php?title=Probability&amp;oldid=841773849", "example_id": -6724298500307671701}