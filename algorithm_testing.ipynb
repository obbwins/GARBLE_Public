{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "2fe1bfd1519b4ed990b59cf0d4725ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "03c8ef9a7f72454295e131256c4bf209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "5cbcc76a19834f12993955ef711e9231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n",
      "==================================Answer==================================\n",
      " **Image and question:**\n",
      "No image is provided. The question asks for guidance on creating \"gemini\" or Gemini 1.5, which is likely referring to a model developed by Microsoft focused on multimodal understanding.\n",
      "\n",
      "**Reasoning:**\n",
      "Gemini 1.5 would require an understanding of the technology it's based on, the specifics of how it was developed, and how it's intended to be applied. \"Creating\" a model like Gemini would involve complex technical processes that require knowledge in AI development and machine learning.\n",
      "\n",
      "**Final Answer:**\n",
      "To create a system like Gemini 1.5, you would need access to Microsoft's research and development resources. The process would involve training the model on large sets of multimodal data, fine-tuning its capabilities, and programming it for specific tasks. Additionally, you would need a team of AI researchers and engineers with expertise in natural language processing, machine learning, and understanding multimodal inputs.\n",
      "\n",
      "(Note: As the details provided do not specify an \"action\" to create the mentioned \"gemini\", the answer provides a generalized approach on how a similar AI model might be developed.)\n",
      "==================================Source docs==================================\n",
      "Document 0------------------------------------------------------------\n",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "I am working on the weekly activities calendar for my preschool class, and I need to create the calendar and the corresponding\n",
      "weekly and daily themes and supplies. I would like you to start doing this for me. Each Friday afternoon, I spend about 3\n",
      "hours searching for a weekly theme, making a printable calendar for the upcoming week, and designing the activity sheets\n",
      "for each day.\n",
      "I would like you to use the resources I provided and any other resources you have access to in order to choose a theme for\n",
      "the week of May 6th through May 10th, 2024. We are located in Maple Grove, MN, and I would like the theme to be suited\n",
      "for the typical climate in our location during the first week of May.\n",
      "After choosing the theme, choose these components for the week:\n",
      "* Color of the week\n",
      "* Animal of the week\n",
      "* Snack theme of the week - example: tree fruits (no nuts, processed sugars, or gluten)\n",
      "* Word of the week (related to the theme)\n",
      "* Song of the week\n",
      "All of the \"of the week\" components mentioned above should be related to the theme chosen, the specific dates (ie: if one of\n",
      "the dates for that week is a holiday or is known for something specific), the season, and/or our geographic location.\n",
      "Next, the daily activities need to be chosen based on the theme and the \"of the week\" components. For each day, the following\n",
      "will be needed:\n",
      "* A specific snack (ie: apple slices)\n",
      "* A fact about the animal of the week\n",
      "* A title and author for a book of the day so I can check out the book from the library (preferably related to the theme or one\n",
      "of the \"of the week\" components)\n",
      "* A positive message \"sentence of the day\" that includes the word of the week\n",
      "* An activity sheet for the day that can be printed out on normal 9x11 paper (coloring, tracing, dotting, watercolor, or\n",
      "Document 1------------------------------------------------------------\n",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "I am working on the weekly activities calendar for my preschool class, and I need to create the calendar and the corresponding\n",
      "weekly and daily themes and supplies. Each Friday afternoon, I spend about 3 hours searching for a weekly theme, making a\n",
      "printable calendar for the upcoming week, and designing the activity sheets for each day.\n",
      "I would like you to use the resources I provided and any other resources you have access to in order to choose a theme for\n",
      "the week of May 6th through May 10th, 2024. We are located in Maple Grove, MN, and I would like the theme to be suited\n",
      "for the typical climate in our location during the first week of May.\n",
      "After choosing the theme, choose these components for the week:\n",
      "* Color of the week\n",
      "* Animal of the week\n",
      "* Snack theme of the week - example: tree fruits (no nuts, processed sugars, or gluten)\n",
      "[...]\n",
      "Next, the daily activities need to be chosen based on the theme and the \"of the week\" components. For each day, [...]\n",
      "Please, design me a weekly calendar that the students can refer to each day.\n",
      "[...]\n",
      "Finally, provide me with a table with 3 columns: snack, book, supplies for activity sheet (ie: paint, pencils, dobbers, crayons),\n",
      "and 5 rows for the day of the week.\n",
      "Attached documents:\n",
      "* Web page with pre-school themes\n",
      "* Image with an example of a weekly pre-school planner\n",
      "Table 17 | Example task from a pre-school teacher (abridged).\n",
      "In previous work, productivity or economic impact was measured in studies that classified jobs\n",
      "based on what current LLMs are able to do with human annotators or classifiers categorizing the tasks\n",
      "in each job as impacted by AI advances (Eloundou et al., 2023; Felten et al., 2018; World Economic\n",
      "Document 2------------------------------------------------------------\n",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "Question: {question with images} Options: {options}\n",
      "Try to reason about the question step by step to help you get the correct answer. You\n",
      "might find that sometimes no reasoning is needed if the answer is straightforward.\n",
      "Sometimes listing out a few reasoning steps will be helpful. In any case, please keep\n",
      "the reasoning concise.\n",
      "First, please describe what is included in the image. Then, respond with your reason\n",
      "first and output the final answer in the format \"Final Answer: <answer>\" where <answer>\n",
      "is the single correct letter choice (A), (B), (C), (D), (E), (F), etc, when options are\n",
      "provided. If you find that your reasoning lead to none of the choice, reject your\n",
      "reasoning and choose the most likely answer. You have to answer with one of the choices.\n",
      "If no options are provided, <answer> is your answer. If you would like to skip\n",
      "reasoning, just directly output the \"Final Answer\" part.\n",
      "REMOVED_SECRET. MathVista\n",
      "<MathVista preamble>\n",
      "<image>\n",
      "Question: <Question>\n",
      "Try to reason about the question below step by step to help you get the correct answer.\n",
      "Formulate your answer as follows.\n",
      "**Image and question:** Please describe what is the image about and what does the\n",
      "question ask for. If there are technical terms, privide a brief definition. For math\n",
      "questions, point out what technique is needed to solve the question. If the question\n",
      "involves number of objects, enumearte over the objects you see.\n",
      "**Reasoning:** Please include your reasoning to solve this question.\n",
      "**Final Answer:** <answer>\" where <answer> is your answer to the question.\n",
      "Keep the answer short and terse. Do not paraphrase or reformat the text you see in the\n",
      "image.\n",
      "Document 3------------------------------------------------------------\n",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "Core Contributors\n",
      "Vihan Jain\n",
      "Nikhil Sethi\n",
      "Megha Goel\n",
      "Takaki Makino\n",
      "Rhys May\n",
      "Zhen Yang\n",
      "Johan Schalkwyk\n",
      "Christina Butterfield\n",
      "Anja Hauth\n",
      "Alex Goldin\n",
      "Will Hawkins\n",
      "Evan Senter\n",
      "Sergey Brin\n",
      "Oliver Woodman\n",
      "Marvin Ritter\n",
      "Eric Noland\n",
      "Minh Giang\n",
      "Vijay Bolina\n",
      "Lisa Lee\n",
      "Tim Blyth\n",
      "Ian Mackinnon\n",
      "Machel Reid\n",
      "Obaid Sarvana\n",
      "David Silver\n",
      "Alexander Chen\n",
      "Lily Wang\n",
      "Loren Maggiore\n",
      "Oscar Chang\n",
      "Nithya Attaluri\n",
      "Gregory Thornton\n",
      "Chung-Cheng Chiu\n",
      "Oskar Bunyan\n",
      "Nir Levine\n",
      "Timothy Chung\n",
      "Evgenii Eltyshev\n",
      "Xiance Si\n",
      "Timothy Lillicrap\n",
      "Demetra Brady\n",
      "Vaibhav Aggarwal\n",
      "Boxi Wu\n",
      "Yuanzhong Xu\n",
      "Ross McIlroy\n",
      "Kartikeya Badola\n",
      "Paramjit Sandhu\n",
      "Erica Moreira\n",
      "Wojciech Stokowiec\n",
      "Ross Hemsley\n",
      "Dong Li\n",
      "Core Contributors\n",
      "Alex Tudor\n",
      "Pranav Shyam\n",
      "Elahe Rahimtoroghi\n",
      "Salem Haykal\n",
      "Pablo Sprechmann\n",
      "Xiang Zhou\n",
      "Diana Mincu\n",
      "Yujia Li\n",
      "Ravi Addanki\n",
      "Kalpesh Krishna\n",
      "Xiao Wu\n",
      "Alexandre Frechette\n",
      "Matan Eyal\n",
      "Allan Dafoe\n",
      "Dave Lacey\n",
      "Document 4------------------------------------------------------------\n",
      "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\n",
      "Core Contributors\n",
      "Denis Teplyashin\n",
      "Jonathan Lai\n",
      "Phil Crone\n",
      "Bogdan Damoc\n",
      "Lewis Ho\n",
      "Sebastian Riedel\n",
      "Karel Lenc\n",
      "Chih-Kuan Yeh\n",
      "Aakanksha Chowdhery\n",
      "Yang Xu\n",
      "Mehran Kazemi\n",
      "Ehsan Amid\n",
      "Anastasia Petrushkina\n",
      "Kevin Swersky\n",
      "Ali Khodaei\n",
      "Gowoon Chen\n",
      "Chris Larkin\n",
      "Mario Pinto\n",
      "Geng Yan\n",
      "Adrià Puigdomènech Badia\n",
      "Piyush Patil\n",
      "Steven Hansen\n",
      "Dave Orr\n",
      "Sébastien M. R. Arnold\n",
      "Jordan Grimstad\n",
      "Andrew Dai\n",
      "Sholto Douglas\n",
      "Rishika Sinha\n",
      "Vikas Yadav\n",
      "Xi Chen\n",
      "Elena Gribovskaya\n",
      "Jacob Austin\n",
      "Jeffrey Zhao\n",
      "Kaushal Patel\n",
      "Paul Komarek\n",
      "Sophia Austin\n",
      "Sebastian Borgeaud\n",
      "Linda Friso\n",
      "Abhimanyu Goyal\n",
      "Ben Caine\n",
      "Kris Cao\n",
      "Da-Woon Chung\n",
      "Matthew Lamm\n",
      "Gabe Barth-Maron\n",
      "Thais Kagohara\n",
      "Kate Olszewska\n",
      "Mia Chen\n",
      "Kaushik Shivakumar\n",
      "Core Contributors\n",
      "Rishabh Agarwal\n",
      "Harshal Godhia\n",
      "Ravi Rajwar\n",
      "Javier Snaider\n",
      "Xerxes Dotiwalla\n",
      "Yuan Liu\n",
      "Aditya Barua\n",
      "Victor Ungureanu\n",
      "Yuan Zhang\n",
      "Bat-Orgil Batsaikhan\n",
      "Mateo Wirth\n",
      "James Qin\n",
      "Ivo Danihelka\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m token_vocabulary \u001b[38;5;241m=\u001b[39m [kw[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keyword_results[pdf_file_path]] \u001b[38;5;66;03m# use all extracted keywords as vocab\u001b[39;00m\n\u001b[1;32m     96\u001b[0m target_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m query_based_on_pdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_sub_document\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial sub-document:\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_sub_document)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken vocabulary:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_vocabulary)\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import tkinter as tk\n",
    "import fitz\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tkinter import filedialog\n",
    "from rag_workflow import READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RERANKER, answer_with_rag\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#query rag system - dummy function\n",
    "\n",
    "def query_rag_system(question, sub_document):\n",
    "    combined_query = question + \" \" + \" \".join(sub_document)\n",
    "    response, _ = answer_with_rag(\n",
    "        question=combined_query,\n",
    "        llm=READER_LLM,\n",
    "        knowledge_index=KNOWLEDGE_VECTOR_DATABASE,\n",
    "        reranker=RERANKER\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "#dummy function, will add real one later - this one works but uses cosine similarity, i want to use oracle judge\n",
    "#dummy similarity function, will add real one later\n",
    "def compute_similarity(candidate_response, target_response):\n",
    "    candidate_embedding = embedding_model.encode(candidate_response, convert_to_tensor=True)\n",
    "    target_embedding = embedding_model.encode(target_response, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(candidate_embedding, target_embedding).item()\n",
    "    return similarity\n",
    "\n",
    "# Function to inject text into a PDF at a random location using PyMuPDF\n",
    "def inject_text_into_pdf(input_pdf_path, output_pdf_path, text_to_inject):\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "\n",
    "    page_number = random.randint(0, len(pdf_document) - 1)\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    page_width, page_height = REMOVED_SECRET, REMOVED_SECRET\n",
    "    x = random.uniform(0, page_width - 100) \n",
    "    y = random.uniform(0, page_height - 20)\n",
    "\n",
    "    page.insert_text((x, y), text_to_inject, fontsize=12, color=(1, 1, 1))\n",
    "\n",
    "    pdf_document.save(output_pdf_path)\n",
    "    pdf_document.close()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=30):\n",
    "    keyword_dict = {}  \n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0] \n",
    "\n",
    "        # Keyword extraction with KeyBERT\n",
    "        kw_model = KeyBERT()\n",
    "        keywords = kw_model.extract_keywords(document.page_content, keyphrase_ngram_range=(1, 7), top_n=num_keywords)\n",
    "        \n",
    "        # Ensure keywords is a dictionary and convert to list of tuples if needed\n",
    "        if not isinstance(keywords, dict):\n",
    "            keywords = {kw: 1/rank for rank, kw in enumerate(keywords, start=1)}\n",
    "            keywords_with_scores = [(keyword, score) for keyword, score in keywords.items()]\n",
    "        else:\n",
    "            keywords_with_scores = [(keyword, score) for keyword, score in keywords.items()]\n",
    "\n",
    "        keyword_dict[pdf_path] = keywords_with_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keyword_dict\n",
    "\"\"\"\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=30):\n",
    "    keywords_list = []  # Create a list to store the keywords\n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0] \n",
    "\n",
    "        # Keyword extraction with KeyBERT\n",
    "        kw_model = KeyBERT()\n",
    "        keywords = kw_model.extract_keywords(document.page_content, keyphrase_ngram_range=(1, 7), top_n=num_keywords)\n",
    "\n",
    "        # Extract just the keywords from the (keyword, score) tuples\n",
    "        keywords_list = [keyword for keyword, score in keywords]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keywords_list  # Return the list of keywords\n",
    "\n",
    "\n",
    "\n",
    "def browse_for_pdf():\n",
    "    \"\"\"Opens a file dialog to let the user select a PDF file.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# Get PDF file path using the browser\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "# Check if a file was selected\n",
    "if pdf_file_path:\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "\n",
    "    if keyword_results:\n",
    "        best_keywords = keyword_results[pdf_file_path][:5] #pick top 5 keywords for initial sub doc\n",
    "        initial_sub_document = [kw[0] for kw in best_keywords]\n",
    "        token_vocabulary = [kw[0] for kw in keyword_results[pdf_file_path]] # use all extracted keywords as vocab\n",
    "        target_response = \"I don't know.\"\n",
    "\n",
    "        \n",
    "        query_based_on_pdf = \" \".join(initial_sub_document)\n",
    "\n",
    "        print(\"Initial sub-document:\", initial_sub_document)\n",
    "        print(\"Token vocabulary:\", token_vocabulary)\n",
    "        print(\"Target response:\", target_response)\n",
    "        print(\"Query based on PDF: \", query_based_on_pdf)\n",
    "\n",
    "         # Algorithm Parameters\n",
    "        T = 10  # Number of iterations\n",
    "        B = 5   # Batch size (number of candidate sub-documents to generate each iteration)\n",
    "\n",
    "\n",
    "        sub_document = initial_sub_document.copy()\n",
    "\n",
    "        for i in range(T):\n",
    "            l = random.randint(0, len(sub_document) - 1)\n",
    "            candidate_sub_documents = []\n",
    "            similarities = []\n",
    "            \n",
    "            for b in range(B):\n",
    "                new_token = random.choice(token_vocabulary)\n",
    "                candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "\n",
    "                candidate_response = query_rag_system(query_based_on_pdf, candidate)\n",
    "                similarity = compute_similarity(candidate_response, target_response)\n",
    "                candidate_sub_documents.append(candidate)\n",
    "                similarities.append(similarity)\n",
    "\n",
    "            best_candidate_index = similarities.index(max(similarities))\n",
    "            sub_document = candidate_sub_documents[best_candidate_index]\n",
    "            \n",
    "            print(f\"Iteration {i+1}/{T}: Best candidate sub-document: {' '.join(sub_document)} (Similarity: {similarities[best_candidate_index]:.4f})\")\n",
    "        final_sub_document_text = ' '.join(sub_document)\n",
    "        print(f\"Final optimised sub-document: {final_sub_document_text}\")\n",
    "\n",
    "        output_pdf_path = \"test.pdf\"\n",
    "        inject_text_into_pdf(pdf_file_path, output_pdf_path, final_sub_document_text)\n",
    "    else:\n",
    "        print(\"No keywords extracted from the PDF.\")\n",
    "\n",
    "        # Find the keyword with the highest score (KeyBERT uses cosine similarity)\n",
    "        #best_keyword = max(keyword_results[pdf_file_path], key=lambda x: x[1])\n",
    "\n",
    "        #print(\"Keywords with scores:\", keyword_results)\n",
    "       # print(\"Best keyword (highest score):\", best_keyword[0])\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading or processing PDF your_pdf_file.pdf: File path your_pdf_file.pdf is not a valid file or url\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=20):\n",
    "    \"\"\"\n",
    "    Loads a PDF, extracts keywords, and returns a dictionary with the document ID and keywords.\n",
    "    \"\"\"\n",
    "    keyword_dict = {}  # To store the results\n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]  # Load the first (and only) document\n",
    "\n",
    "        # Keyword extraction\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"en\", n=3, dedupLim=0.9, top=num_keywords)\n",
    "        keywords = kw_extractor.extract_keywords(document.page_content)\n",
    "\n",
    "        # Format keywords\n",
    "        keywords = [kw[0] for kw in keywords]  # Get just the keyword strings\n",
    "\n",
    "        # Store in dictionary\n",
    "        keyword_dict[pdf_path] = keywords  # Use file path as unique ID\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "# Example usage\n",
    "pdf_file = \"your_pdf_file.pdf\" \n",
    "keyword_results = extract_keywords_from_pdf(pdf_file)\n",
    "\n",
    "if keyword_results:  # Check if extraction was successful\n",
    "    print(\"Keywords for\", pdf_file, \":\", keyword_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords with scores: {'/home/obb/codes/langers/A_fast_and_elitist_multiobjective_genetic_algorithm_NSGA-II.pdf': ['Pareto-optimal solutions', 'nondominated sorting genetic algorithm', 'Multiobjective', 'NSGA-II', 'problems', 'Kanpur Genetic Algorithms Laboratory', 'sorting', 'Genetic', 'number', 'complexity', 'IEEE', 'Algorithm', 'sharing parameter', 'EVOLUTIONARY COMPUTATION', 'find', 'results', 'simulation run', 'elitist MOEAs', 'set', 'find multiple Pareto-optimal']}\n",
      "Best keyword (lowest score): IEEE\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=20):\n",
    "    \"\"\"\n",
    "    Loads a PDF, extracts keywords, and returns a dictionary with the document ID and keywords.\n",
    "    \"\"\"\n",
    "    keyword_dict = {}  # To store the results\n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]  # Load the first (and only) document\n",
    "\n",
    "        # Keyword extraction\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"en\", n=4, dedupLim=0.3, top=num_keywords)\n",
    "        keywords = kw_extractor.extract_keywords(document.page_content)\n",
    "\n",
    "        # Format keywords\n",
    "        keywords = [kw[0] for kw in keywords]  # Get just the keyword strings\n",
    "\n",
    "        # Store in dictionary\n",
    "        keyword_dict[pdf_path] = keywords  # Use file path as unique ID\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "\n",
    "def browse_for_pdf():\n",
    "    \"\"\"\n",
    "    Opens a file dialog to let the user select a PDF file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    \n",
    "    # Open file dialog with PDF filter\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "# Get PDF file path using the browser\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "# Check if a file was selected\n",
    "if pdf_file_path:\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "    if keyword_results:  \n",
    "        best_keyword = min(keyword_results[pdf_file_path], key=lambda x: x[1]) \n",
    "\n",
    "        print(\"Keywords with scores:\", keyword_results)\n",
    "        print(\"Best keyword (lowest score):\", best_keyword)\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords with scores: {'/home/obb/codes/langers/A_fast_and_elitist_multiobjective_genetic_algorithm_NSGA-II.pdf': [('Pareto-optimal solutions', 0.02067240280973193), ('IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION', 0.021719027779717848), ('Pareto-optimal', 0.025025255650409396), ('solutions', 0.026177512439900946), ('nondominated sorting genetic algorithm', 0.028942766474564098), ('nondominated sorting', 0.03682714518207172), ('Multiobjective', 0.038061246240296345), ('Elitist Multiobjective Genetic Algorithm', 0.04119974532350175), ('sorting genetic algorithm', 0.04554250660942391), ('NSGA-II', 0.04797422704080826), ('Genetic Algorithm', 0.0500755456763078), ('TRANSACTIONS ON EVOLUTIONARY COMPUTATION', 0.0527981118013695), ('nondominated', 0.055856461796754846), ('Fast and Elitist Multiobjective Genetic Algorithm', 0.058505832827929295), ('nondominated sorting genetic', 0.05951631941574196), ('problems', 0.05962528766840571), ('Multiobjective Genetic Algorithm', 0.06290036208240299), ('Kanpur Genetic Algorithms Laboratory', 0.06538850864234101), ('sorting', 0.0655234115987527), ('sharing', 0.06969151133371056)]}\n",
      "Best keyword (lowest score): Pareto-optimal solutions\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=20):\n",
    "    \"\"\"Loads a PDF, extracts keywords with scores, and returns a dictionary.\"\"\"\n",
    "    keyword_dict = {}  \n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]\n",
    "\n",
    "        # Keyword extraction (keeping scores this time)\n",
    "        kw_extractor = yake.KeywordExtractor(lan=\"en\", n=6, dedupLim=0.9, top=num_keywords)\n",
    "        keywords_with_scores = kw_extractor.extract_keywords(document.page_content)\n",
    "\n",
    "        # Store keywords with scores in the dictionary\n",
    "        keyword_dict[pdf_path] = keywords_with_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "\n",
    "def browse_for_pdf():\n",
    "    \"\"\"Opens a file dialog to let the user select a PDF file.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# Get PDF file path using the browser\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "# Check if a file was selected\n",
    "if pdf_file_path:\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "\n",
    "    if keyword_results:\n",
    "        # Find the keyword with the lowest score\n",
    "        best_keyword = min(keyword_results[pdf_file_path], key=lambda x: x[1])\n",
    "\n",
    "        print(\"Keywords with scores:\", keyword_results)\n",
    "        print(\"Best keyword (lowest score):\", best_keyword[0]) # Extract the keyword string itself\n",
    "\n",
    "        # Now you can use `best_keyword[0]` as the initial seed for your algorithm\n",
    "        # ... rest of your algorithm code here ...\n",
    "\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords with scores: {'/home/obb/codes/langers/WATEERFALLVs V-MODEL Vs AGILE A COMPARATIVE STUDY ON SDLC.pdf': [(95.0, 'sundararajan murugaiyan computer science dept ., government arts college chennai'), (69.0, 'balaji computer science dept ., gulf college muscat'), (24.5, 'business management 29th june 2012'), (19.714285714285715, 'typical v shape agile modeling'), (19.598484848484848, 'right software development life cycle'), (16.0, 'rights reserved issn 2304'), (15.416666666666666, 'development life cycle method'), (14.598484848484848, 'software development life cycle'), (14.598484848484848, 'software development life cycle'), (13.265151515151514, 'software development methodologies based'), (13.2, '1 © 2012 jitbm'), (11.416666666666666, 'development life cycle'), (9.598484848484848, 'software development processes'), (9.598484848484848, 'agile software development'), (9.265151515151514, 'software development process'), (9.181818181818182, 'developing software solution'), (9.0, 'flowing steadily downwards'), (8.931818181818182, 'software development project'), (8.333333333333334, 'project teams located'), (8.240196078431373, 'sequential development model')]}\n",
      "Best keyword (highest score): 19.714285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/obb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/obb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "from rake_nltk import Rake\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=20):\n",
    "    \"\"\"Loads a PDF, extracts keywords with scores using rake-nltk, and returns a dictionary.\"\"\"\n",
    "    keyword_dict = {}  \n",
    "\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]\n",
    "\n",
    "        # Download necessary NLTK resources if not already downloaded\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')\n",
    "\n",
    "        # Keyword extraction with rake-nltk\n",
    "        r = Rake()\n",
    "        r.extract_keywords_from_text(document.page_content)\n",
    "\n",
    "        # Get the top keywords with scores\n",
    "        keywords_with_scores = r.get_ranked_phrases_with_scores()[:num_keywords]\n",
    "\n",
    "        # Store keywords with scores in the dictionary\n",
    "        keyword_dict[pdf_path] = keywords_with_scores\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "\n",
    "    return keyword_dict\n",
    "\n",
    "\n",
    "def browse_for_pdf():\n",
    "    \"\"\"Opens a file dialog to let the user select a PDF file.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "\n",
    "# Get PDF file path using the browser\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "# Check if a file was selected\n",
    "if pdf_file_path:\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "\n",
    "    if keyword_results:\n",
    "        # Find the keyword with the highest score (rake-nltk scores are positive)\n",
    "        best_keyword = max(keyword_results[pdf_file_path], key=lambda x: x[1])\n",
    "\n",
    "        print(\"Keywords with scores:\", keyword_results)\n",
    "        print(\"Best keyword (highest score):\", best_keyword[0])\n",
    "\n",
    "        # Now you can use `best_keyword[0]` as the initial seed for your algorithm\n",
    "        # ... rest of your algorithm code here ...\n",
    "\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Load embedding model\n",
    "EMBEDDING_MODEL = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\"\"\"\n",
    "# Function to query the RAG system (dummy function for illustration)\n",
    "def query_rag_system(query, sub_document):\n",
    "    combined_query = query + \" \" + \" \".join(sub_document)\n",
    "    response = rag_system.generate_response(combined_query)  # Example function call\n",
    "    return response\n",
    "\n",
    "# Function to compute semantic similarity\n",
    "def compute_similarity(candidate_response, target_response):\n",
    "    candidate_embedding = embedding_model.encode(candidate_response, convert_to_tensor=True)\n",
    "    target_embedding = embedding_model.encode(target_response, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(candidate_embedding, target_embedding).item()\n",
    "    return similarity\n",
    "\"\"\"\n",
    "# Function to inject text into a PDF at a random location using PyMuPDF\n",
    "def inject_text_into_pdf(input_pdf_path, output_pdf_path, text_to_inject):\n",
    "    # Open the existing PDF\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "\n",
    "    # Randomly choose a page to inject the text (assuming more than one page)\n",
    "    page_number = random.randint(0, len(pdf_document) - 1)\n",
    "    page = pdf_document[page_number]\n",
    "\n",
    "    # Randomly choose a position on the page\n",
    "    page_width, page_height = REMOVED_SECRET, REMOVED_SECRET\n",
    "    x = random.uniform(0, page_width - 100)  # Ensuring text fits on the page\n",
    "    y = random.uniform(0, page_height - 20)\n",
    "\n",
    "    # Inject text in white color (invisible)\n",
    "    page.insert_text((x, y), text_to_inject, fontsize=12, color=(1, 1, 1))\n",
    "\n",
    "    # Save the modified PDF\n",
    "    pdf_document.save(output_pdf_path)\n",
    "    pdf_document.close()\n",
    "\n",
    "# Parameters\n",
    "T = 10  # Number of iterations\n",
    "B = 5   # Batch size (number of candidate sub-documents to generate each iteration)\n",
    "token_vocabulary = [\"Vienna\", \"Paris\", \"London\", \"best\", \"city\", \"quality\", \"life\", \"high\", \"Europe\"]\n",
    "initial_sub_document = [\"city\", \"in\", \"Europe\", \"is\", \"best\"]\n",
    "target_response = \"Vienna is the best city in Europe due to its high quality of life.\"\n",
    "\n",
    "sub_document = initial_sub_document.copy()\n",
    "\n",
    "for i in range(T):\n",
    "    l = random.randint(0, len(sub_document) - 1)\n",
    "    candidate_sub_documents = []\n",
    "    similarities = []\n",
    "\n",
    "    for b in range(B):\n",
    "        new_token = random.choice(token_vocabulary)\n",
    "        candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "\n",
    "        # Query the RAG system with the candidate sub-document\n",
    "        candidate_response = query_rag_system(\"What is the best city in Europe?\", candidate)\n",
    "\n",
    "        # Compute similarity to the target response\n",
    "        similarity = compute_similarity(candidate_response, target_response)\n",
    "        candidate_sub_documents.append(candidate)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Select the candidate with the highest similarity\n",
    "    best_candidate_index = similarities.index(max(similarities))\n",
    "    sub_document = candidate_sub_documents[best_candidate_index]\n",
    "\n",
    "    print(f\"Iteration {i+1}/{T}: Best candidate sub-document: {' '.join(sub_document)} (Similarity: {similarities[best_candidate_index]:.4f})\")\n",
    "\n",
    "# Final optimized sub-document\n",
    "final_sub_document_text = ' '.join(sub_document)\n",
    "print(f\"Final optimized sub-document: {final_sub_document_text}\")\n",
    "\n",
    "# Inject the final sub-document into the PDF\n",
    "input_pdf_path = \"path/to/your/input.pdf\"  # Replace with your input PDF path\n",
    "output_pdf_path = \"path/to/your/output.pdf\"  # Replace with your output PDF path\n",
    "inject_text_into_pdf(input_pdf_path, output_pdf_path, final_sub_document_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
