{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredPDFLoader\n",
    "from REMOVED_SECRET import Document as LangchainDocument\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from rag_for_notebook_sunday import CustomTextGenerationPipeline\n",
    "import REMOVED_SECRET as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "efc1735fb4704b11b1cfcb3cf10b5476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "custom_pipeline = CustomTextGenerationPipeline(model_id=\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How was your day? Assistant: I'm doing wellâ€”\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "Token_Logits tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Not all logits are -inf\n",
      "All_Logits: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Logits requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "generated_sequence, all_logits = custom_pipeline(prompt)\n",
    "\n",
    "\n",
    "#check if logits require gradients\n",
    "print(f\"Logits requires_grad: {all_logits.requires_grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_logits shape: torch.Size([1, 448896])\n",
      "target shape: torch.Size([1])\n",
      "Loss: tensor(inf, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Gradients computed for logits: True\n",
      "Grads: tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "print(f\"all_logits shape: {all_logits.shape}\")\n",
    "# Assuming all_logits is your logits tensor\n",
    "#batch_size, sequence_length, vocab_size = all_logits.size()\n",
    "sequence_length, vocab_size = all_logits.size()\n",
    "# Example target tensor: must be of shape (batch_size * sequence_length)\n",
    "# Replace this with your actual target IDs. Here, we're simulating with random targets\n",
    "target = torch.randint(0, vocab_size, (sequence_length,)).to(all_logits.device)\n",
    "# Flatten the logits for loss computation\n",
    "#logits_for_loss = all_logits.view(-1, vocab_size)  # shape: (batch_size * sequence_length, vocab_size)\n",
    "#print(f\"logits_for_loss shape: {logits_for_loss.shape}\")\n",
    "print(f\"target shape: {target.shape}\")\n",
    "# Calculate the cross-entropy loss\n",
    "loss = F.cross_entropy(all_logits, target)\n",
    "\n",
    "# Backpropagation to compute gradients\n",
    "loss.backward()\n",
    "print(\"Loss:\", loss)\n",
    "print(f\"Gradients computed for logits: {all_logits.grad is not None}\")\n",
    "print(\"Grads:\", all_logits.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"  # Or any other model you want to test\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                              torch_dtype=torch.float16, \n",
    "                                              device_map='auto', \n",
    "                                              attn_implementation=\"flash_attention_2\", \n",
    "\n",
    "                                              trust_remote_code=True\n",
    "                                             )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Create the text generation pipeline\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer, \n",
    "                     return_full_text=False, # To avoid including the prompt in the output\n",
    "                     output_scores=True,    # To get the logits\n",
    "                     max_new_tokens=50      # Adjust as needed\n",
    "                    ) \n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Generate text and extract logits\n",
    "with torch.no_grad():  # No need for gradients during inference\n",
    "    outputs = generator(prompt)\n",
    "\n",
    "generated_text = outputs[0]['generated_text'] \n",
    "logits = outputs[0]['scores']  # Access the logits\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(\"Logits Shape:\", logits.shape)  # Should be (sequence_length, vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
