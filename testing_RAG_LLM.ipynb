{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredPDFLoader\n",
    "from REMOVED_SECRET import Document as LangchainDocument\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from rag_for_notebook_sunday import CustomTextGenerationPipeline\n",
    "import REMOVED_SECRET as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "4da0056dc73a4ba8896cd43bf73dcc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "custom_pipeline = CustomTextGenerationPipeline(model_id=\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Hey there, I feel like a real adventurer! I\n",
      "Logits requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "generated_sequence, all_logits = custom_pipeline(prompt)\n",
    "\n",
    "\n",
    "#check if logits require gradients\n",
    "print(f\"Logits requires_grad: {all_logits.requires_grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_logits shape: torch.Size([1, 448896])\n",
      "target shape: torch.Size([1])\n",
      "Gradients computed for logits: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18384/2690611828.py:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(f\"Gradients computed for logits: {all_logits.grad is not None}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "print(f\"all_logits shape: {all_logits.shape}\")\n",
    "# Assuming all_logits is your logits tensor\n",
    "#batch_size, sequence_length, vocab_size = all_logits.size()\n",
    "sequence_length, vocab_size = all_logits.size()\n",
    "# Example target tensor: must be of shape (batch_size * sequence_length)\n",
    "# Replace this with your actual target IDs. Here, we're simulating with random targets\n",
    "target = torch.randint(0, vocab_size, (sequence_length,)).to(all_logits.device)\n",
    "# Flatten the logits for loss computation\n",
    "#logits_for_loss = all_logits.view(-1, vocab_size)  # shape: (batch_size * sequence_length, vocab_size)\n",
    "#print(f\"logits_for_loss shape: {logits_for_loss.shape}\")\n",
    "print(f\"target shape: {target.shape}\")\n",
    "# Calculate the cross-entropy loss\n",
    "loss = F.cross_entropy(all_logits, target)\n",
    "\n",
    "# Backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradients computed for logits: {all_logits.grad is not None}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
