{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "\n",
    "def create_animated_umap(embeddings, labels, n_neighbors=15, min_dist=0.1, n_components=3, metric='cosine', n_frames=100):\n",
    "    # Normalize the embeddings\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Create UMAP reducer\n",
    "    reducer = UMAP(n_neighbors=n_neighbors, min_dist=min_dist, n_components=n_components, metric=metric)\n",
    "\n",
    "    # Fit UMAP\n",
    "    umap_embeddings = reducer.fit_transform(scaled_embeddings)\n",
    "\n",
    "    # Create interpolation between initial state and final UMAP embedding\n",
    "    initial_state = scaled_embeddings[:, :n_components]  # Use first n_components of scaled embeddings\n",
    "    embedding_list = [\n",
    "        initial_state + (umap_embeddings - initial_state) * (i / (n_frames - 1))\n",
    "        for i in range(n_frames)\n",
    "    ]\n",
    "\n",
    "    # Set up the figure and 3D axis\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Set style and color palette\n",
    "    REMOVED_SECRET(\"dark_background\")\n",
    "    color_palette = sns.color_palette(\"plasma\", n_colors=len(np.unique(labels)))\n",
    "\n",
    "    # Create scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        embedding_list[0][:, 0],\n",
    "        embedding_list[0][:, 1],\n",
    "        embedding_list[0][:, 2],\n",
    "        c=labels,\n",
    "        cmap=REMOVED_SECRET(\"plasma\"),\n",
    "        s=20\n",
    "    )\n",
    "    \n",
    "    # Add title and labels\n",
    "    title = ax.set_title(\"UMAP Embedding Progress (Frame 0)\", fontsize=16, color='cyan')\n",
    "    ax.set_xlabel(\"UMAP1\", fontsize=12, color='magenta')\n",
    "    ax.set_ylabel(\"UMAP2\", fontsize=12, color='magenta')\n",
    "    ax.set_zlabel(\"UMAP3\", fontsize=12, color='magenta')\n",
    "\n",
    "    # Add a color bar\n",
    "    cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
    "    cbar.set_label(\"Document Clusters\", fontsize=12, color='yellow')\n",
    "\n",
    "    # Set consistent axis limits\n",
    "    all_embeddings = np.vstack(embedding_list)\n",
    "    ax.set_xlim(all_embeddings[:, 0].min(), all_embeddings[:, 0].max())\n",
    "    ax.set_ylim(all_embeddings[:, 1].min(), all_embeddings[:, 1].max())\n",
    "    ax.set_zlim(all_embeddings[:, 2].min(), all_embeddings[:, 2].max())\n",
    "\n",
    "    # Animation update function\n",
    "    def update(frame):\n",
    "        title.set_text(f\"UMAP Embedding Progress (Frame {frame})\")\n",
    "        scatter._offsets3d = (embedding_list[frame][:, 0], \n",
    "                              embedding_list[frame][:, 1], \n",
    "                              embedding_list[frame][:, 2])\n",
    "        return scatter, title\n",
    "\n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, update, frames=n_frames, interval=50, blit=False)\n",
    "\n",
    "    # Save animation\n",
    "    anim.save('umap_progress_animation.gif', writer='pillow', fps=30)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"UMAP progress animation saved as 'umap_progress_animation.gif'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <REMOVED_SECRET object at 0x7fd5c2b838b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/obb/codes/langers/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "01f153011b7e474188f8a0e62a59214b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Loading PDFs: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: [Document(page_content='974 623 Gr 698 finding\\n\\nWhat is the capital of France? Paris!', metadata={'source': 'local_database/injected_output.pdf'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 382.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text splitting Sankey diagram saved as 'text_splitting_sankey.html'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|██████████| 1/1 [00:00<00:00, 190.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: [Document(page_content='974 623 Gr 698 finding\\n\\nWhat is the capital of France? Paris!', metadata={'source': 'local_database/injected_output.pdf'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 1835.58it/s]\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning:\n",
      "\n",
      "The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "\n",
      "/tmp/ipykernel_1452684/4146345483.py:41: MatplotlibDeprecationWarning:\n",
      "\n",
      "The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``REMOVED_SECRET()`` or ``pyplot.get_cmap()`` instead.\n",
      "\n",
      "/tmp/ipykernel_1452684/4146345483.py:57: UserWarning:\n",
      "\n",
      "Attempting to set identical low and high xlims makes transformation singular; automatically expanding.\n",
      "\n",
      "/tmp/ipykernel_1452684/4146345483.py:58: UserWarning:\n",
      "\n",
      "Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "\n",
      "/tmp/ipykernel_1452684/4146345483.py:59: UserWarning:\n",
      "\n",
      "Attempting to set identical low and high zlims makes transformation singular; automatically expanding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()  # Adjust criteria as needed\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract question, context, and answer from a PDF file using PyMuPDF.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Regex patterns to extract information\n",
    "    question_pattern = r\"Question: (.*?)\\n\"\n",
    "    context_pattern = r\"Context:(.*?)(?=Question:|Answer:|$)\"\n",
    "    answer_pattern = r\"Answer: (.*?)(?=Context:|$)\"\n",
    "\n",
    "    questions = re.findall(question_pattern, text)\n",
    "    contexts = re.findall(context_pattern, text, re.DOTALL)\n",
    "    answers = re.findall(answer_pattern, text, re.DOTALL)\n",
    "\n",
    "    # Clean and pair the extracted data\n",
    "    evaluation_data = []\n",
    "    for q, c, a in zip(questions, contexts, answers):\n",
    "        evaluation_data.append({\n",
    "            \"question\": q.strip(),\n",
    "            \"context\": c.strip(),\n",
    "            \"ground_truth\": a.strip()\n",
    "        })\n",
    "\n",
    "    return evaluation_data\n",
    "\n",
    "def calculate_perplexity(logits, input_ids):\n",
    "    # Ensure logits and input_ids have the same sequence length\n",
    "    seq_len = min(logits.size(1), input_ids.size(1))\n",
    "    logits = logits[:, :seq_len, :]\n",
    "    input_ids = input_ids[:, :seq_len]\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss_fct = REMOVED_SECRET(ignore_index=-100, reduction='none')\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    return torch.exp(loss.mean())\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        # Check if doc is a string or an object with page_content attribute\n",
    "        if isinstance(doc, str):\n",
    "            doc_text = preprocess_text(doc)\n",
    "        else:\n",
    "            doc_text = preprocess_text(doc.page_content)\n",
    "        \n",
    "        # Check for significant overlap\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:  \n",
    "            relevant_docs += 1\n",
    "            break  # Stop after finding the first relevant document\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"\n",
    "    Normalize answer text while preserving important punctuation and structure.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace multiple whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Preserve common punctuation that might be important for context\n",
    "    text = re.sub(r'[^a-z0-9\\s.,;:()\"-]', '', text)\n",
    "    \n",
    "    # Normalize some common variations\n",
    "    text = text.replace(' , ', ', ').replace(' . ', '. ')\n",
    "    text = text.replace('( ', '(').replace(' )', ')')\n",
    "    \n",
    "    # Remove spaces before punctuation\n",
    "    text = re.sub(r'\\s([.,;:])', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def evaluate_rag_system(rag_system, evaluation_data, pdf_folder_path):\n",
    "    results = []\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Load and process documents\n",
    "    raw_documents = REMOVED_SECRET(pdf_folder_path)\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Build vector database\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    for sample in tqdm(evaluation_data, desc=\"Evaluating samples\"):\n",
    "        question = sample['question']\n",
    "        ground_truth = normalize_answer(sample['ground_truth'])\n",
    "        context = sample['context']\n",
    "        \n",
    "        print(f\"\\n\\nQuestion: {question}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Original Context: {context[:200]}...\")  # Print first 200 characters of context\n",
    "        \n",
    "        if not is_valid_answer(ground_truth):\n",
    "            print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "            continue\n",
    "        \n",
    "        # Get RAG system's answer and relevant documents\n",
    "        answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "        answer = normalize_answer(answer)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        question_embedding = REMOVED_SECRET(question)\n",
    "        doc_embeddings = REMOVED_SECRET([doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in relevant_docs])\n",
    "        similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "        \n",
    "        print(f\"Generated Answer: {answer}\")\n",
    "        print(\"Retrieved Documents:\")\n",
    "        for i, doc in enumerate(relevant_docs[:3], 1):  # Print top 3 retrieved documents\n",
    "            print(f\"Doc {i}: {doc[:200]}...\")  # Print first 200 characters of each document\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu_score = calculate_bleu_score(ground_truth, answer)\n",
    "        rouge_scores = rouge_scorer_instance.score(ground_truth, answer)\n",
    "        retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "        \n",
    "        # BERT Score\n",
    "        _, _, bert_f1 = bert_scorer.score([answer], [ground_truth])\n",
    "        \n",
    "        # Perplexity calculation\n",
    "        input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "        perplexity = calculate_perplexity(logits, input_ids)\n",
    "        \n",
    "        # Additional metrics\n",
    "        exact_match = exact_match_score(answer, ground_truth)\n",
    "        f1 = f1_score(answer, ground_truth)\n",
    "        \n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "        print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "        print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "        print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "        print(f\"Exact Match: {exact_match}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'ground_truth': ground_truth,\n",
    "            'generated_answer': answer,\n",
    "            'bleu_score': bleu_score,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "            'retrieval_accuracy': retrieval_accuracy,\n",
    "            'bert_score': bert_f1.item(),\n",
    "            'perplexity': perplexity.item(),\n",
    "            'exact_match': exact_match,\n",
    "            'f1_score': f1,\n",
    "            'relevant_docs': relevant_docs,\n",
    "            'similarity_scores': similarity_scores.tolist()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "# Function to plot distribution of scores\n",
    "def plot_score_distribution(data, score_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data[score_name], kde=True)\n",
    "    plt.title(f'Distribution of {score_name}')\n",
    "    plt.xlabel(score_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Function for error analysis\n",
    "def error_analysis(results):\n",
    "    # Calculate absolute difference between BLEU score and 1\n",
    "    results['bleu_error'] = 1 - results['bleu_score']\n",
    "    \n",
    "    # Sort by error and get top 10 worst predictions\n",
    "    worst_predictions = results.sort_values('bleu_error', ascending=False).head(10)\n",
    "    \n",
    "    print(\"Top 10 Worst Predictions:\")\n",
    "    for _, row in worst_predictions.iterrows():\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"Ground Truth: {row['ground_truth']}\")\n",
    "        print(f\"Generated Answer: {row['generated_answer']}\")\n",
    "        print(f\"BLEU Score: {row['bleu_score']}\")\n",
    "        print(f\"F1 Score: {row['f1_score']}\")\n",
    "        print(f\"Exact Match: {row['exact_match']}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "    # Analyze error patterns\n",
    "    error_patterns = {\n",
    "        'short_answer': (results['generated_answer'].str.split().str.len() < 5).sum(),\n",
    "        'long_answer': (results['generated_answer'].str.split().str.len() > 50).sum(),\n",
    "        'low_bleu': (results['bleu_score'] < 0.1).sum(),\n",
    "        'low_f1': (results['f1_score'] < 0.5).sum(),\n",
    "        'no_exact_match': (results['exact_match'] == 0).sum()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nError Patterns:\")\n",
    "    for pattern, count in error_patterns.items():\n",
    "        print(f\"{pattern}: {count}\")\n",
    "\n",
    "\n",
    "def prepare_umap_data(rag_system, pdf_directory):\n",
    "    # Load and process documents\n",
    "    raw_documents = REMOVED_SECRET(pdf_directory)\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = REMOVED_SECRET([doc.page_content for doc in processed_documents])\n",
    "\n",
    "    # Convert embeddings to numpy array\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    # Create labels based on document sources\n",
    "    source_to_label = defaultdict(lambda: len(source_to_label))\n",
    "    labels = np.array([source_to_label[REMOVED_SECRET('source', 'unknown')] for doc in processed_documents])\n",
    "\n",
    "\n",
    "    return embeddings_array, labels, processed_documents\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    # Extract data from all PDFs in a directory\n",
    "    pdf_directory = \"local_database\"  \n",
    "\n",
    "    # Load and process documents\n",
    "    raw_documents = REMOVED_SECRET(pdf_directory)\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    " \n",
    "    # Prepare data for UMAP visualization\n",
    "    embeddings, labels, processed_documents = prepare_umap_data(rag_system, pdf_directory)\n",
    "    \n",
    "\n",
    "    # Create UMAP visualization\n",
    "    create_animated_umap(embeddings, labels)\n",
    " \n",
    "    all_evaluation_data = []\n",
    "    for filename in os.listdir(pdf_directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = REMOVED_SECRET(pdf_directory, filename)\n",
    "            all_evaluation_data.extend(extract_data_from_pdf(pdf_path))\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluation_results = evaluate_rag_system(rag_system, all_evaluation_data, pdf_directory)\n",
    "\n",
    "    # After evaluation, create visualizations for each result\n",
    "    for i, result in enumerate(evaluation_results.itertuples()):\n",
    "        question = result.question\n",
    "        answer = result.generated_answer\n",
    "        relevant_docs = result.relevant_docs  \n",
    "        similarity_scores = result.similarity_scores \n",
    "\n",
    "\n",
    "\n",
    "    # Display some information about the UMAP visualization\n",
    "    print(f\"UMAP visualization created with {len(embeddings)} document chunks.\")\n",
    "    print(f\"Number of unique labels: {len(np.unique(labels))}\")\n",
    "    print(\"Label distribution:\")\n",
    "    for label, count in zip(*np.unique(labels, return_counts=True)):\n",
    "        print(f\"  Label {label}: {count} chunks\")\n",
    "        \n",
    "    # Display results\n",
    "    print(evaluation_results.describe())\n",
    "\n",
    "    # Calculate average scores for numeric columns only\n",
    "    numeric_columns = evaluation_results.select_dtypes(include=[np.number]).columns\n",
    "    average_scores = evaluation_results[numeric_columns].mean()\n",
    "    print(\"\\nAverage Scores:\")\n",
    "    print(average_scores)\n",
    "\n",
    "    # Plot distributions for numeric columns\n",
    "    for metric in numeric_columns:\n",
    "        plot_score_distribution(evaluation_results, metric)\n",
    "\n",
    "    # Correlation heatmap for numeric columns\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(evaluation_results[numeric_columns].corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap of Evaluation Metrics')\n",
    "    plt.show()\n",
    "\n",
    "    # Error analysis\n",
    "    error_analysis(evaluation_results)\n",
    "\n",
    "    # Display generated answers\n",
    "    print(\"\\nGenerated Answers:\")\n",
    "    for _, row in evaluation_results.iterrows():\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"Ground Truth: {row['ground_truth']}\")\n",
    "        print(f\"Generated Answer: {row['generated_answer']}\")\n",
    "        print(f\"BLEU Score: {row['bleu_score']:.4f}\")\n",
    "        print(f\"F1 Score: {row['f1_score']:.4f}\")\n",
    "        print(f\"Exact Match: {row['exact_match']}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "    # Clear memory\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "8035e2577ca04bc5a89a888002883935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 612.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: paris\n",
      "Original Context: What is the capital of France? Paris! \n",
      "\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: paris context: document 2: the eiffel tower, located in paris, is one of the\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!\n",
      "BLEU Score: 0.0176\n",
      "ROUGE-L F1: 0.1333\n",
      "Retrieval Accuracy: 1.0000\n",
      "BERT Score: 0.1273\n",
      "Exact Match: 0\n",
      "F1 Score: 0.1333\n",
      "\n",
      "Evaluation Results:\n",
      "question: What is the capital of France?\n",
      "ground_truth: paris\n",
      "generated_answer: paris context: document 2: the eiffel tower, located in paris, is one of the\n",
      "bleu_score: 0.01758542189440898\n",
      "rouge1: 0.13333333333333333\n",
      "rouge2: 0.0\n",
      "rougeL: 0.13333333333333333\n",
      "retrieval_accuracy: 1.0\n",
      "bert_score: 0.1272696852684021\n",
      "perplexity: 1419106.25\n",
      "exact_match: 0\n",
      "f1_score: 0.13333333333333333\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "from langchain.schema import Document\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_perplexity(logits, input_ids):\n",
    "    seq_len = min(logits.size(1), input_ids.size(1))\n",
    "    logits = logits[:, :seq_len, :]\n",
    "    input_ids = input_ids[:, :seq_len]\n",
    "    \n",
    "    loss_fct = REMOVED_SECRET(ignore_index=-100, reduction='none')\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "    \n",
    "    return torch.exp(loss.mean())\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = preprocess_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def normalize_answer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s.,;:()\"-]', '', text)\n",
    "    text = text.replace(' , ', ', ').replace(' . ', '. ')\n",
    "    text = text.replace('( ', '(').replace(' )', ')')\n",
    "    text = re.sub(r'\\s([.,;:])', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Create a Document object from the raw text\n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    \n",
    "    # Process documents\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Build vector database\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context}\")\n",
    "    \n",
    "    if not is_valid_answer(ground_truth):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    # Get RAG system's answer and relevant documents\n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    answer = normalize_answer(answer)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    doc_embeddings = REMOVED_SECRET([doc.page_content if hasattr(doc, 'page_content') else str(doc) for doc in relevant_docs])\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):  # Print top 3 retrieved documents\n",
    "        print(f\"Doc {i}: {doc}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_score = calculate_bleu_score(ground_truth, answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    # BERT Score\n",
    "    _, _, bert_f1 = bert_scorer.score([answer], [ground_truth])\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    perplexity = calculate_perplexity(logits, input_ids)\n",
    "    \n",
    "    # Additional metrics\n",
    "    exact_match = exact_match_score(answer, ground_truth)\n",
    "    f1 = f1_score(answer, ground_truth)\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth,\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'perplexity': perplexity.item(),\n",
    "        'exact_match': exact_match,\n",
    "        'f1_score': f1,\n",
    "        'relevant_docs': relevant_docs,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    # Specify the path to your single PDF\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "\n",
    "    # Extract data from the single PDF\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        # Run evaluation\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    # Clear memory\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "8fe548e105b34893a9d33f715e940d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 613.38it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalize_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 188\u001b[0m\n\u001b[1;32m    184\u001b[0m sample \u001b[38;5;241m=\u001b[39m extract_data_from_pdf(pdf_path)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m, in \u001b[0;36mevaluate_rag_system\u001b[0;34m(rag_system, sample, pdf_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m knowledge_index \u001b[38;5;241m=\u001b[39m rag_system\u001b[38;5;241m.\u001b[39mbuild_vector_database(processed_documents)\n\u001b[1;32m     87\u001b[0m question \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 88\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_answer\u001b[49m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     89\u001b[0m context \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize_answer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_perplexity(logits, input_ids):\n",
    "    seq_len = min(logits.size(1), input_ids.size(1))\n",
    "    logits = logits[:, :seq_len, :]\n",
    "    input_ids = input_ids[:, :seq_len]\n",
    "    \n",
    "    loss_fct = REMOVED_SECRET(ignore_index=-100, reduction='none')\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "    \n",
    "    # Debug: Print the loss before exponentiating\n",
    "    print(f\"Mean loss before exp: {loss.mean().item()}\")\n",
    "    \n",
    "    # Debug: Print individual loss values\n",
    "    print(f\"Individual loss values: {loss.tolist()}\")\n",
    "    \n",
    "    perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "def simple_perplexity(logits, input_ids):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_probs = probs.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return torch.exp(-torch.log(target_probs).mean())\n",
    "\n",
    "def normalize_logits(logits):\n",
    "    return (logits - logits.mean()) / logits.std()\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Create a Document object from the raw text\n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    \n",
    "    # Process documents\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Build vector database\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context}\")\n",
    "    \n",
    "    if not is_valid_answer(ground_truth):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    # Get RAG system's answer and relevant documents\n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    answer = normalize_answer(answer)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    # Handle the case where relevant_docs are strings\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):  # Print top 3 retrieved documents\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")  # Print first 200 characters of each document\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_score = calculate_bleu_score(ground_truth, answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    # BERT Score\n",
    "    _, _, bert_f1 = bert_scorer.score([answer], [ground_truth])\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    # Debug: Print shapes and ranges\n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    perplexity = calculate_perplexity(logits, input_ids)\n",
    "    simple_ppl = simple_perplexity(logits, input_ids)\n",
    "    normalized_logits = normalize_logits(logits)\n",
    "    normalized_ppl = calculate_perplexity(normalized_logits, input_ids)\n",
    "    \n",
    "    print(f\"Original Perplexity: {perplexity.item()}\")\n",
    "    print(f\"Simple Perplexity: {simple_ppl.item()}\")\n",
    "    print(f\"Perplexity with normalized logits: {normalized_ppl.item()}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    exact_match = exact_match_score(answer, ground_truth)\n",
    "    f1 = f1_score(answer, ground_truth)\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth,\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'perplexity': perplexity.item(),\n",
    "        'simple_perplexity': simple_ppl.item(),\n",
    "        'normalized_perplexity': normalized_ppl.item(),\n",
    "        'exact_match': exact_match,\n",
    "        'f1_score': f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    # Specify the path to your single PDF\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "\n",
    "    # Extract data from the single PDF\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        # Run evaluation\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    # Clear memory\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "79a66814a45f4ed59c75ceebed04a889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 239.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: paris\n",
      "Original Context: What is the capital of France? Paris! \n",
      "\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: paris. document 2: context: the eiffel tower, located in paris, france\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!...\n",
      "Logits shape: torch.Size([1, 118, 32064]), Input IDs shape: torch.Size([1, 29])\n",
      "Logits min: -22.0625, max: 58.09375\n",
      "Input IDs min: 275, max: 29973\n",
      "Mean loss before exp: 13.365091323852539\n",
      "Individual loss values: [10.752317428588867, 8.864455223083496, 3.5212817192077637, 17.61346435546875, 5.729909896850586, 10.233869552612305, 12.044573783874512, 16.271759033203125, 15.42618465423584, 15.754791259765625, 14.414917945861816, 9.427671432495117, 11.575547218322754, 10.462754249572754, 12.07187557220459, 7.998154640197754, 15.473542213439941, 15.969281196594238, 22.405630111694336, 18.194625854492188, 16.787139892578125, 10.553323745727539, 9.440649032592773, 11.923979759216309, 17.618492126464844, 21.9570369720459, 4.29144287109375, 16.961393356323242, 23.84755516052246]\n",
      "Mean loss before exp: 9.699338912963867\n",
      "Individual loss values: [9.823966979980469, 9.538629531860352, 8.819564819335938, 10.35969352722168, 9.014229774475098, 9.71714973449707, 9.808382034301758, 10.348637580871582, 10.249993324279785, 10.227964401245117, 9.54702377319336, 9.094802856445312, 9.29523754119873, 9.516357421875, 9.663728713989258, 9.195745468139648, 9.908231735229492, 9.84984302520752, 10.747182846069336, 9.897655487060547, 10.228910446166992, 9.097067832946777, 8.986297607421875, 9.305456161499023, 9.603963851928711, 10.38054370880127, 8.300013542175293, 10.08077621459961, 10.673791885375977]\n",
      "Original Perplexity: 637360.875\n",
      "Simple Perplexity: 637360.875\n",
      "Perplexity with normalized logits: 16306.8232421875\n",
      "BLEU Score: 0.0000\n",
      "ROUGE-L F1: 0.1667\n",
      "Retrieval Accuracy: 1.0000\n",
      "BERT Score: 0.0551\n",
      "Exact Match: 0\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Evaluation Results:\n",
      "question: What is the capital of France?\n",
      "ground_truth: paris\n",
      "generated_answer: paris. document 2: context: the eiffel tower, located in paris, france\n",
      "bleu_score: 0\n",
      "rouge1: 0.16666666666666669\n",
      "rouge2: 0.0\n",
      "rougeL: 0.16666666666666669\n",
      "retrieval_accuracy: 1.0\n",
      "bert_score: 0.05505441874265671\n",
      "perplexity: 637360.875\n",
      "simple_perplexity: 637360.875\n",
      "normalized_perplexity: 16306.8232421875\n",
      "exact_match: 0\n",
      "f1_score: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"\n",
    "    Normalize answer text while preserving important punctuation and structure.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace multiple whitespace characters with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Preserve common punctuation that might be important for context\n",
    "    text = re.sub(r'[^a-z0-9\\s.,;:()\"-]', '', text)\n",
    "    \n",
    "    # Normalize some common variations\n",
    "    text = text.replace(' , ', ', ').replace(' . ', '. ')\n",
    "    text = text.replace('( ', '(').replace(' )', ')')\n",
    "    \n",
    "    # Remove spaces before punctuation\n",
    "    text = re.sub(r'\\s([.,;:])', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_perplexity(logits, input_ids):\n",
    "    seq_len = min(logits.size(1), input_ids.size(1))\n",
    "    logits = logits[:, :seq_len, :]\n",
    "    input_ids = input_ids[:, :seq_len]\n",
    "    \n",
    "    loss_fct = REMOVED_SECRET(ignore_index=-100, reduction='none')\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "    \n",
    "    # Debug: Print the loss before exponentiating\n",
    "    print(f\"Mean loss before exp: {loss.mean().item()}\")\n",
    "    \n",
    "    # Debug: Print individual loss values\n",
    "    print(f\"Individual loss values: {loss.tolist()}\")\n",
    "    \n",
    "    perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "def simple_perplexity(logits, input_ids):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_probs = probs.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return torch.exp(-torch.log(target_probs).mean())\n",
    "\n",
    "def normalize_logits(logits):\n",
    "    return (logits - logits.mean()) / logits.std()\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.25, 0.25, 0.25, 0.25), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = preprocess_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return int(normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Create a Document object from the raw text\n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    \n",
    "    # Process documents\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Build vector database\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context}\")\n",
    "    \n",
    "    if not is_valid_answer(ground_truth):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    # Get RAG system's answer and relevant documents\n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    answer = normalize_answer(answer)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    # Handle the case where relevant_docs are strings\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):  # Print top 3 retrieved documents\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")  # Print first 200 characters of each document\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_score = calculate_bleu_score(ground_truth, answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    # BERT Score\n",
    "    _, _, bert_f1 = bert_scorer.score([answer], [ground_truth])\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    # Debug: Print shapes and ranges\n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    perplexity = calculate_perplexity(logits, input_ids)\n",
    "    simple_ppl = simple_perplexity(logits, input_ids)\n",
    "    normalized_logits = normalize_logits(logits)\n",
    "    normalized_ppl = calculate_perplexity(normalized_logits, input_ids)\n",
    "    \n",
    "    print(f\"Original Perplexity: {perplexity.item()}\")\n",
    "    print(f\"Simple Perplexity: {simple_ppl.item()}\")\n",
    "    print(f\"Perplexity with normalized logits: {normalized_ppl.item()}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    exact_match = exact_match_score(answer, ground_truth)\n",
    "    f1 = f1_score(answer, ground_truth)\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': ground_truth,\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'perplexity': perplexity.item(),\n",
    "        'simple_perplexity': simple_ppl.item(),\n",
    "        'normalized_perplexity': normalized_ppl.item(),\n",
    "        'exact_match': exact_match,\n",
    "        'f1_score': f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    # Specify the path to your single PDF\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "\n",
    "    # Extract data from the single PDF\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        # Run evaluation\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    # Clear memory\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "0f29b65dc8b242d0a3475b3fe1c6419f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 465.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: paris\n",
      "Original Context: What is the capital of France? Paris! \n",
      "\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: Paris.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "Context: The Eiffel Tower, located on the Champ de\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!...\n",
      "Logits shape: torch.Size([1, 118, 32064]), Input IDs shape: torch.Size([1, 31])\n",
      "Logits min: -25.765625, max: 58.09375\n",
      "Input IDs min: 13, max: 29973\n",
      "Mean loss before exp: 11.982036590576172\n",
      "Individual loss values: [10.752317428588867, 8.864455223083496, 3.5212817192077637, 17.61346435546875, 5.729909896850586, 10.233869552612305, 12.044573783874512, 15.271758079528809, 15.42618465423584, 15.754791259765625, 10.539917945861816, 11.240171432495117, 6.575547218322754, 7.431504249572754, 3.728126049041748, 10.404404640197754, 13.645417213439941, 5.62553071975708, 11.077505111694336, 0.022750791162252426, 4.818389415740967, 13.584573745727539, 24.159399032592773, 21.814605712890625, 19.962242126464844, 13.972662925720215, 8.85394287109375, 14.430143356323242, 14.738181114196777, 23.107873916625977, 16.497636795043945]\n",
      "Mean loss before exp: 9.5849609375\n",
      "Individual loss values: [9.874715805053711, 9.61799430847168, 8.973942756652832, 10.355015754699707, 9.148002624511719, 9.776611328125, 9.860648155212402, 10.229402542114258, 10.247343063354492, 10.224477767944336, 9.196003913879395, 9.412505149841309, 8.852810859680176, 9.26658821105957, 8.817560195922852, 9.57364559173584, 9.74768352508545, 8.764019966125488, 9.457448959350586, 7.947098255157471, 8.926140785217285, 9.54741096496582, 10.731271743774414, 10.482540130615234, 9.926980972290039, 9.493433952331543, 9.007352828979492, 9.82576847076416, 9.63434886932373, 10.567801475524902, 9.647244453430176]\n",
      "Original Perplexity: 159857.265625\n",
      "Simple Perplexity: 159857.265625\n",
      "Perplexity with normalized logits: 14544.39453125\n",
      "Improved Perplexity: 159857.265625\n",
      "BLEU Score: 0.0848\n",
      "ROUGE-L F1: 0.1538\n",
      "Retrieval Accuracy: 1.0000\n",
      "BERT Score: 0.2074\n",
      "Exact Match: False\n",
      "Word F1 Score: 0.1538\n",
      "Logits mean: 26.58221435546875, std: 9.142765045166016\n",
      "Top 5 most likely tokens:\n",
      "  1. 'ual' (probability: 0.2108)\n",
      "  2. ':' (probability: 0.1566)\n",
      "  3. 'of' (probability: 0.0695)\n",
      "  4. '**' (probability: 0.0318)\n",
      "  5. 'and' (probability: 0.0276)\n",
      "\n",
      "Evaluation Results:\n",
      "question: What is the capital of France?\n",
      "ground_truth: Paris!\n",
      "generated_answer: Paris.\n",
      "\n",
      "\n",
      "Document 2:\n",
      "Context: The Eiffel Tower, located on the Champ de\n",
      "bleu_score: 0.0847962147646323\n",
      "rouge1: 0.15384615384615385\n",
      "rouge2: 0.0\n",
      "rougeL: 0.15384615384615385\n",
      "retrieval_accuracy: 1.0\n",
      "bert_score: 0.20736894011497498\n",
      "perplexity: 159857.265625\n",
      "simple_perplexity: 159857.265625\n",
      "normalized_perplexity: 14544.39453125\n",
      "improved_perplexity: 159857.265625\n",
      "exact_match: 0\n",
      "word_f1_score: 0.15384615384615385\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"Normalize answer for more lenient comparison.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_perplexity(logits, input_ids):\n",
    "    seq_len = min(logits.size(1), input_ids.size(1))\n",
    "    logits = logits[:, :seq_len, :]\n",
    "    input_ids = input_ids[:, :seq_len]\n",
    "    \n",
    "    loss_fct = REMOVED_SECRET(ignore_index=-100, reduction='none')\n",
    "    loss = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "    \n",
    "    # Debug: Print the loss before exponentiating\n",
    "    print(f\"Mean loss before exp: {loss.mean().item()}\")\n",
    "    \n",
    "    # Debug: Print individual loss values\n",
    "    print(f\"Individual loss values: {loss.tolist()}\")\n",
    "    \n",
    "    perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def improved_perplexity(logits, input_ids, ignore_index=-100):\n",
    "    # Flatten the tensors\n",
    "    logits = logits.view(-1, logits.size(-1))\n",
    "    input_ids = input_ids.view(-1)\n",
    "    \n",
    "    # Create a mask for non-ignored indices\n",
    "    mask = (input_ids != ignore_index).float()\n",
    "    \n",
    "    # Calculate log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Gather the log probabilities of the correct tokens\n",
    "    target_log_probs = log_probs.gather(1, input_ids.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # Apply the mask and calculate the mean negative log likelihood\n",
    "    nll = -(target_log_probs * mask).sum() / mask.sum()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(nll)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def simple_perplexity(logits, input_ids):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    target_probs = probs.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return torch.exp(-torch.log(target_probs).mean())\n",
    "\n",
    "def normalize_logits(logits):\n",
    "    return (logits - logits.mean()) / logits.std()\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.5, 0.3, 0.2), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = preprocess_text(doc.page_content if hasattr(doc, 'page_content') else str(doc))\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def word_f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    # Create a Document object from the raw text\n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    \n",
    "    # Process documents\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "    # Build vector database\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context}\")\n",
    "    \n",
    "    if not is_valid_answer(sample['ground_truth']):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    # Get RAG system's answer and relevant documents\n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    normalized_answer = normalize_answer(answer)\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    # Handle the case where relevant_docs are strings\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):  # Print top 3 retrieved documents\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")  # Print first 200 characters of each document\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bleu_score = calculate_bleu_score(ground_truth, normalized_answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, normalized_answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    # BERT Score\n",
    "    _, _, bert_f1 = bert_scorer.score([normalized_answer], [ground_truth])\n",
    "    \n",
    "    # Perplexity calculation\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    # Debug: Print shapes and ranges\n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    perplexity = calculate_perplexity(logits, input_ids)\n",
    "    improved_ppl = improved_perplexity(logits, input_ids)\n",
    "    \n",
    "    simple_ppl = simple_perplexity(logits, input_ids)\n",
    "    normalized_logits = normalize_logits(logits)\n",
    "    normalized_ppl = calculate_perplexity(normalized_logits, input_ids)\n",
    "    \n",
    "    print(f\"Original Perplexity: {perplexity.item()}\")\n",
    "    print(f\"Simple Perplexity: {simple_ppl.item()}\")\n",
    "    print(f\"Perplexity with normalized logits: {normalized_ppl.item()}\")\n",
    "    print(f\"Improved Perplexity: {improved_ppl.item()}\")\n",
    "    # Additional metrics\n",
    "    exact_match = ground_truth == normalized_answer\n",
    "    word_f1 = word_f1_score(answer, sample['ground_truth'])\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"Word F1 Score: {word_f1:.4f}\")\n",
    "    print(f\"Logits mean: {logits.mean().item()}, std: {logits.std().item()}\")\n",
    "    print(f\"Top 5 most likely tokens:\")\n",
    "    top_tokens = torch.topk(F.softmax(logits.view(-1, logits.size(-1)), dim=-1), k=5, dim=-1)\n",
    "    for i, (prob, idx) in enumerate(zip(top_tokens.values[0], top_tokens.indices[0])):\n",
    "        token = REMOVED_SECRET.decode([idx])\n",
    "        print(f\"  {i+1}. '{token}' (probability: {prob.item():.4f})\")\n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': sample['ground_truth'],\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'perplexity': perplexity.item(),\n",
    "        'simple_perplexity': simple_ppl.item(),\n",
    "        'normalized_perplexity': normalized_ppl.item(),\n",
    "        'improved_perplexity': improved_ppl.item(),\n",
    "        'exact_match': int(exact_match),\n",
    "        'word_f1_score': word_f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize RAG system\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    # Specify the path to single PDF\n",
    "    pdf_path = \"test_one.pdf\"  \n",
    "\n",
    "    # Extract data from the single PDF\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        # Run evaluation\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    # Clear memory\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "79c04b806e8a49ceb627f4de793f1200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 374.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: paris\n",
      "Original Context: What is the capital of France? Paris! \n",
      "...\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: Paris\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!...\n",
      "Logits shape: torch.Size([1, 118, 32064]), Input IDs shape: torch.Size([1, 9])\n",
      "Logits min: -23.75, max: 67.1875\n",
      "Input IDs min: 275, max: 29973\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'answer_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 282\u001b[0m\n\u001b[1;32m    279\u001b[0m sample \u001b[38;5;241m=\u001b[39m extract_data_from_pdf(pdf_path)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample:\n\u001b[0;32m--> 282\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_system\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluation Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m, in \u001b[0;36mevaluate_rag_system\u001b[0;34m(rag_system, sample, pdf_path)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput IDs min: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 215\u001b[0m single_token_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity_for_single_token(logits, \u001b[43manswer_tokens\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;241m10.0\u001b[39m]:\n\u001b[1;32m    218\u001b[0m     scaled_ppl \u001b[38;5;241m=\u001b[39m calculate_perplexity_with_scaling(logits, input_ids, temperature\u001b[38;5;241m=\u001b[39mtemp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"Normalize answer for more lenient comparison.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def scale_logits(logits, temperature=1.0):\n",
    "    return logits / temperature\n",
    "\n",
    "def calculate_perplexity_with_scaling(logits, input_ids, temperature=1.0):\n",
    "    scaled_logits = scale_logits(logits, temperature)\n",
    "    log_probs = F.log_softmax(scaled_logits, dim=-1)\n",
    "    target_log_probs = log_probs.gather(-1, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    return torch.exp(-target_log_probs.mean())\n",
    "\n",
    "def calculate_perplexity_for_single_token(logits, correct_token_id):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a single-token answer.\n",
    "    \"\"\"\n",
    "    # Get the logits for the last token (the answer token)\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Calculate softmax probabilities\n",
    "    probs = F.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get the probability of the correct token\n",
    "    correct_prob = probs[correct_token_id].item()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = 1 / correct_prob if correct_prob > 0 else float('inf')\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.5, 0.3, 0.2), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = preprocess_text(doc if isinstance(doc, str) else doc.page_content)\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def word_f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def perplexity_sanity_check():\n",
    "    vocab_size = 1000\n",
    "    sequence_length = 10\n",
    "    batch_size = 1\n",
    "    \n",
    "    logits = torch.full((batch_size, sequence_length, vocab_size), -100.0)\n",
    "    correct_indices = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "    logits.scatter_(-1, correct_indices.unsqueeze(-1), 100.0)\n",
    "    \n",
    "    input_ids = correct_indices\n",
    "    \n",
    "    #ppl = improved_perplexity(logits, input_ids)\n",
    "    #print(f\"Sanity check perplexity: {ppl.item()}\")\n",
    "\n",
    "def post_process_answer(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up the generated answer by removing specific phrases and unnecessary information.\n",
    "    \"\"\"\n",
    "    # Remove \"Answer:\" prefix if present\n",
    "    answer = re.sub(r'^Answer:\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove phrases like \"Based on the context\" or \"According to the document\"\n",
    "    answer = re.sub(r'(Based on|According to) (the|this) (context|document|passage|text)[,:]?\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove phrases that reference the context or documents\n",
    "    answer = re.sub(r'(The context|The document|The passage) (states|mentions|says|indicates) that\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove any remaining mentions of \"context\" or \"document\" at the end of the answer\n",
    "    answer = re.sub(r'\\s+(Context:|Document:).*$', '', answer, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Trim any trailing whitespace\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def analyze_answer_logits(logits, tokenizer, answer):\n",
    "    answer_token_id = tokenizer.encode(answer)[0]\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    answer_logit = last_token_logits[answer_token_id].item()\n",
    "    \n",
    "    # Get top 5 logits\n",
    "    top_logits, top_indices = torch.topk(last_token_logits, 5)\n",
    "    \n",
    "    print(f\"Logit for '{answer}': {answer_logit:.4f}\")\n",
    "    print(\"Top 5 logits and tokens:\")\n",
    "    for logit, idx in zip(top_logits, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  {token}: {logit.item():.4f}\")\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context[:200]}...\")  # Print first 200 characters of context\n",
    "    \n",
    "    if not is_valid_answer(sample['ground_truth']):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    \n",
    "    # Apply post-processing to the answer\n",
    "    answer = post_process_answer(answer)\n",
    "    \n",
    "    normalized_answer = normalize_answer(answer)\n",
    "\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")\n",
    "    \n",
    "    bleu_score = calculate_bleu_score(ground_truth, normalized_answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, normalized_answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    _, _, bert_f1 = bert_scorer.score([normalized_answer], [ground_truth])\n",
    "    \n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    single_token_perplexity = calculate_perplexity_for_single_token(logits, answer_tokens[0])\n",
    "    \n",
    "    for temp in [1.0, 2.0, 5.0, 10.0]:\n",
    "        scaled_ppl = calculate_perplexity_with_scaling(logits, input_ids, temperature=temp)\n",
    "        print(f\"Perplexity with temperature {temp}: {scaled_ppl.item()}\")\n",
    "    \n",
    "    print(\"Tokenizer check:\")\n",
    "    print(\"Question tokens:\", REMOVED_SECRET.encode(question))\n",
    "    print(\"Answer tokens:\", REMOVED_SECRET.encode(answer))\n",
    "    print(\"Tokenized question:\", REMOVED_SECRET.tokenize(question))\n",
    "    print(\"Tokenized answer:\", REMOVED_SECRET.tokenize(answer))\n",
    "    \n",
    "    print(\"Model output check:\")\n",
    "    print(\"Model output type:\", type(logits))\n",
    "    print(\"Model output shape:\", logits.shape)\n",
    "    print(\"Sample of model output:\", logits[0, 0, :10])  # First 10 values of the first token\n",
    "\n",
    "    print(\"Top 5 most likely tokens:\")\n",
    "    top_tokens = torch.topk(F.softmax(logits.view(-1, logits.size(-1)), dim=-1), k=5, dim=-1)\n",
    "    for i, (prob, idx) in enumerate(zip(top_tokens.values[0], top_tokens.indices[0])):\n",
    "        token = REMOVED_SECRET.decode([idx])\n",
    "        print(f\"  {i+1}. '{token}' (probability: {prob.item():.4f})\")\n",
    "    \n",
    "    exact_match = ground_truth == normalized_answer\n",
    "    word_f1 = word_f1_score(answer, sample['ground_truth'])\n",
    "    \n",
    "    analyze_answer_logits(logits, REMOVED_SECRET, answer)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"Word F1 Score: {word_f1:.4f}\")\n",
    "    print(f\"Single-token Perplexity: {single_token_perplexity:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': sample['ground_truth'],\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'single_token_perplexity': single_token_perplexity.item(),\n",
    "        'exact_match': int(exact_match),\n",
    "        'word_f1_score': word_f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    perplexity_sanity_check()\n",
    "\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "61637fd07083467b9ae0edb77916d85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 452.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: paris\n",
      "Original Context: What is the capital of France? Paris! \n",
      "...\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: Paris\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!...\n",
      "Logits shape: torch.Size([1, 118, 32064]), Input IDs shape: torch.Size([1, 9])\n",
      "Logits min: -26.421875, max: 67.1875\n",
      "Input IDs min: 275, max: 29973\n",
      "Single-token Perplexity: 733752722283196.2500\n",
      "Tokenizer check:\n",
      "Question tokens: [1724, 338, 278, 7483, 310, 3444, 29973]\n",
      "Answer tokens: [3681]\n",
      "Tokenized question: ['▁What', '▁is', '▁the', '▁capital', '▁of', '▁France', '?']\n",
      "Tokenized answer: ['▁Paris']\n",
      "Model output check:\n",
      "Model output type: <class 'torch.Tensor'>\n",
      "Model output shape: torch.Size([1, 118, 32064])\n",
      "Sample of model output: tensor([ 2.6836,  5.5234,  9.4297,  7.9688, 13.4531, 14.7656, 17.4062, 11.8047,\n",
      "        10.0859, 12.8438], device='cuda:0')\n",
      "Logit for 'Paris': 20.3594\n",
      "Top 5 logits and tokens:\n",
      "  into: 54.5625\n",
      "  ,: 50.5312\n",
      "  to: 49.6250\n",
      "  from: 47.8125\n",
      "  captured: 45.9062\n",
      "BLEU Score: 0.7071\n",
      "ROUGE-L F1: 1.0000\n",
      "Retrieval Accuracy: 1.0000\n",
      "BERT Score: 1.0000\n",
      "Exact Match: True\n",
      "Word F1 Score: 1.0000\n",
      "\n",
      "Evaluation Results:\n",
      "question: What is the capital of France?\n",
      "ground_truth: Paris!\n",
      "generated_answer: Paris\n",
      "bleu_score: 0.7071067811865476\n",
      "rouge1: 1.0\n",
      "rouge2: 0.0\n",
      "rougeL: 1.0\n",
      "retrieval_accuracy: 1.0\n",
      "bert_score: 1.0\n",
      "single_token_perplexity: 733752722283196.2\n",
      "exact_match: 1\n",
      "word_f1_score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_valid_answer(answer: str) -> bool:\n",
    "    \"\"\"Check if an answer is valid.\"\"\"\n",
    "    cleaned = clean_text(answer)\n",
    "    return len(cleaned) > 1 and not cleaned.isdigit()\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"Normalize answer for more lenient comparison.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def post_process_answer(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up the generated answer by removing specific phrases and unnecessary information.\n",
    "    \"\"\"\n",
    "    # Remove \"Answer:\" prefix if present\n",
    "    answer = re.sub(r'^Answer:\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove phrases like \"Based on the context\" or \"According to the document\"\n",
    "    answer = re.sub(r'(Based on|According to) (the|this) (context|document|passage|text)[,:]?\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove phrases that reference the context or documents\n",
    "    answer = re.sub(r'(The context|The document|The passage) (states|mentions|says|indicates) that\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove any remaining mentions of \"context\" or \"document\" at the end of the answer\n",
    "    answer = re.sub(r'\\s+(Context:|Document:).*$', '', answer, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Trim any trailing whitespace\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def calculate_perplexity_for_single_token(logits, correct_token_id):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a single-token answer.\n",
    "    \"\"\"\n",
    "    # Get the logits for the last token (the answer token)\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Calculate softmax probabilities\n",
    "    probs = F.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get the probability of the correct token\n",
    "    correct_prob = probs[correct_token_id].item()\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity = 1 / correct_prob if correct_prob > 0 else float('inf')\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.5, 0.3, 0.2), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    def preprocess_text(text):\n",
    "        return ' '.join(text.lower().split())\n",
    "\n",
    "    ground_truth_context = preprocess_text(ground_truth_context)\n",
    "    \n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = preprocess_text(doc if isinstance(doc, str) else doc.page_content)\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "\n",
    "    return relevant_docs / k\n",
    "\n",
    "def word_f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def analyze_answer_logits(logits, tokenizer, answer):\n",
    "    answer_token_id = tokenizer.encode(answer)[0]\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    answer_logit = last_token_logits[answer_token_id].item()\n",
    "    \n",
    "    # Get top 5 logits\n",
    "    top_logits, top_indices = torch.topk(last_token_logits, 5)\n",
    "    \n",
    "    print(f\"Logit for '{answer}': {answer_logit:.4f}\")\n",
    "    print(\"Top 5 logits and tokens:\")\n",
    "    for logit, idx in zip(top_logits, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  {token}: {logit.item():.4f}\")\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = normalize_answer(sample['ground_truth'])\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context[:200]}...\")  # Print first 200 characters of context\n",
    "    \n",
    "    if not is_valid_answer(sample['ground_truth']):\n",
    "        print(f\"Warning: Invalid ground truth for question: {question}\")\n",
    "        return None\n",
    "    \n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    answer = post_process_answer(answer)\n",
    "    normalized_answer = normalize_answer(answer)\n",
    "\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")\n",
    "    \n",
    "    bleu_score = calculate_bleu_score(ground_truth, normalized_answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, normalized_answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    _, _, bert_f1 = bert_scorer.score([normalized_answer], [ground_truth])\n",
    "    \n",
    "    answer_token_ids = REMOVED_SECRET.encode(answer)\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    # Calculate perplexity for the single-token answer\n",
    "    single_token_perplexity = calculate_perplexity_for_single_token(logits, answer_token_ids[0])\n",
    "    print(f\"Single-token Perplexity: {single_token_perplexity:.4f}\")\n",
    "    \n",
    "    print(\"Tokenizer check:\")\n",
    "    print(\"Question tokens:\", REMOVED_SECRET.encode(question))\n",
    "    print(\"Answer tokens:\", answer_token_ids)\n",
    "    print(\"Tokenized question:\", REMOVED_SECRET.tokenize(question))\n",
    "    print(\"Tokenized answer:\", REMOVED_SECRET.tokenize(answer))\n",
    "    \n",
    "    print(\"Model output check:\")\n",
    "    print(\"Model output type:\", type(logits))\n",
    "    print(\"Model output shape:\", logits.shape)\n",
    "    print(\"Sample of model output:\", logits[0, 0, :10])  # First 10 values of the first token\n",
    "\n",
    "    analyze_answer_logits(logits, REMOVED_SECRET, answer)\n",
    "    \n",
    "    exact_match = ground_truth == normalized_answer\n",
    "    word_f1 = word_f1_score(answer, sample['ground_truth'])\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"Word F1 Score: {word_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': sample['ground_truth'],\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'single_token_perplexity': single_token_perplexity,\n",
    "        'exact_match': int(exact_match),\n",
    "        'word_f1_score': word_f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    rag_system.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "d851ffed6aa74df4a940ffe3625b6d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.REMOVED_SECRET4\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['REMOVED_SECRET.bias', 'REMOVED_SECRET.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 330.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Ground Truth: Paris!\n",
      "Original Context: What is the capital of France? Paris! \n",
      "...\n",
      "=> Retrieving documents...\n",
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Answer: Paris\n",
      "\n",
      "        Context: Document 2:\n",
      "During the late 19th century, the Industrial Revolution\n",
      "Retrieved Documents:\n",
      "Doc 1: What is the capital of France? Paris!...\n",
      "Logits shape: torch.Size([1, 118, 32064]), Input IDs shape: torch.Size([1, 33])\n",
      "Logits min: -23.75, max: 67.1875\n",
      "Input IDs min: 13, max: 29973\n",
      "Single-token Perplexity: 3154604143.8474\n",
      "Tokenizer check:\n",
      "Question tokens: [1724, 338, 278, 7483, 310, 3444, 29973]\n",
      "Answer tokens: [3681, 13, 13, 4706, 15228, 29901, 10854, 29871, 29906, 29901, 13, 29928, 3864, 278, 5683, 29871, 29896, 29929, 386, 6462, 29892, 278, 12157, 9315, 14595]\n",
      "Tokenized question: ['▁What', '▁is', '▁the', '▁capital', '▁of', '▁France', '?']\n",
      "Tokenized answer: ['▁Paris', '<0x0A>', '<0x0A>', '▁▁▁▁▁▁▁', '▁Context', ':', '▁Document', '▁', '2', ':', '<0x0A>', 'D', 'uring', '▁the', '▁late', '▁', '1', '9', 'th', '▁century', ',', '▁the', '▁Indust', 'rial', '▁Revolution']\n",
      "Model output check:\n",
      "Model output type: <class 'torch.Tensor'>\n",
      "Model output shape: torch.Size([1, 118, 32064])\n",
      "Sample of model output: tensor([ 2.6836,  5.5234,  9.4297,  7.9688, 13.4531, 14.7656, 17.4062, 11.8047,\n",
      "        10.0859, 12.8438], device='cuda:0')\n",
      "Logit for 'Paris\n",
      "\n",
      "        Context: Document 2:\n",
      "During the late 19th century, the Industrial Revolution': 40.2500\n",
      "Top 5 logits and tokens:\n",
      "  brought: 60.7812\n",
      "  led: 60.5000\n",
      "  had: 60.2812\n",
      "  transformed: 59.6875\n",
      "  significantly: 59.0625\n",
      "BLEU Score: 0.0000\n",
      "ROUGE-L F1: 0.1538\n",
      "Retrieval Accuracy: 1.0000\n",
      "BERT Score: -0.1211\n",
      "Exact Match: False\n",
      "Word F1 Score: 0.0000\n",
      "\n",
      "Evaluation Results:\n",
      "question: What is the capital of France?\n",
      "ground_truth: Paris!\n",
      "generated_answer: Paris\n",
      "\n",
      "        Context: Document 2:\n",
      "During the late 19th century, the Industrial Revolution\n",
      "bleu_score: 0\n",
      "rouge1: 0.15384615384615385\n",
      "rouge2: 0.0\n",
      "rougeL: 0.15384615384615385\n",
      "retrieval_accuracy: 1.0\n",
      "bert_score: -0.12110665440559387\n",
      "single_token_perplexity: 3154604143.8473945\n",
      "exact_match: 0\n",
      "word_f1_score: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from REMOVED_SECRET import sentence_bleu, SmoothingFunction\n",
    "from bert_score import BERTScorer\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "from collections import Counter\n",
    "from difflib import SequenceMatcher\n",
    "from REMOVED_SECRET import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "# Import your RAG system\n",
    "from RAG_UTILS import RAGSystem, EMBEDDING_MODEL_NAME, MODEL_ID, RERANKER_MODEL\n",
    "\n",
    "MODEL_ID = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "RERANKER_MODEL = None\n",
    "NUM_RETRIEVED_DOCS = 5\n",
    "\n",
    "def extract_data_from_pdf(pdf_path):\n",
    "    \"\"\"Extract question and answer from a PDF file using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    # Split the text into question and answer\n",
    "    parts = text.split('?')\n",
    "    if len(parts) >= 2:\n",
    "        question = parts[0].strip() + '?'\n",
    "        answer = parts[1].strip()\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": answer,\n",
    "            \"context\": text  # Use the full text as context\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def calculate_perplexity_for_single_token(logits, correct_token_id):\n",
    "    \"\"\"\n",
    "    Calculate perplexity for a single-token answer.\n",
    "    \"\"\"\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    probs = F.softmax(last_token_logits, dim=-1)\n",
    "    correct_prob = probs[correct_token_id].item()\n",
    "    perplexity = 1 / correct_prob if correct_prob > 0 else float('inf')\n",
    "    return perplexity\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), \n",
    "                         weights=(0.5, 0.3, 0.2), \n",
    "                         smoothing_function=smoothie)\n",
    "\n",
    "def calculate_retrieval_accuracy(retrieved_docs, ground_truth_context, k=1):\n",
    "    relevant_docs = 0\n",
    "    for doc in retrieved_docs[:k]:\n",
    "        doc_text = doc if isinstance(doc, str) else doc.page_content\n",
    "        similarity = SequenceMatcher(None, ground_truth_context, doc_text).ratio()\n",
    "        if similarity > 0.5:\n",
    "            relevant_docs += 1\n",
    "            break\n",
    "    return relevant_docs / k\n",
    "\n",
    "def word_f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = prediction.split()\n",
    "    ground_truth_tokens = ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "def analyze_answer_logits(logits, tokenizer, answer):\n",
    "    answer_token_id = tokenizer.encode(answer)[0]\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    answer_logit = last_token_logits[answer_token_id].item()\n",
    "    \n",
    "    top_logits, top_indices = torch.topk(last_token_logits, 5)\n",
    "    \n",
    "    print(f\"Logit for '{answer}': {answer_logit:.4f}\")\n",
    "    print(\"Top 5 logits and tokens:\")\n",
    "    for logit, idx in zip(top_logits, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"  {token}: {logit.item():.4f}\")\n",
    "\n",
    "def evaluate_rag_system(rag_system, sample, pdf_path):\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bert_scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    raw_documents = [Document(page_content=sample['context'], metadata={\"source\": pdf_path})]\n",
    "    processed_documents = REMOVED_SECRET(raw_documents)\n",
    "    knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "    \n",
    "    question = sample['question']\n",
    "    ground_truth = sample['ground_truth']\n",
    "    context = sample['context']\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Ground Truth: {ground_truth}\")\n",
    "    print(f\"Original Context: {context[:200]}...\")  # Print first 200 characters of context\n",
    "    \n",
    "    answer, relevant_docs, logits = rag_system.answer_with_rag(question, knowledge_index)\n",
    "\n",
    "    question_embedding = REMOVED_SECRET(question)\n",
    "    doc_contents = [doc if isinstance(doc, str) else doc.page_content for doc in relevant_docs]\n",
    "    doc_embeddings = REMOVED_SECRET(doc_contents)\n",
    "    similarity_scores = cosine_similarity([question_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    print(f\"Generated Answer: {answer}\")\n",
    "    print(\"Retrieved Documents:\")\n",
    "    for i, doc in enumerate(relevant_docs[:3], 1):\n",
    "        doc_content = doc if isinstance(doc, str) else doc.page_content\n",
    "        print(f\"Doc {i}: {doc_content[:200]}...\")\n",
    "    \n",
    "    bleu_score = calculate_bleu_score(ground_truth, answer)\n",
    "    rouge_scores = rouge_scorer_instance.score(ground_truth, answer)\n",
    "    retrieval_accuracy = calculate_retrieval_accuracy(relevant_docs, context)\n",
    "    \n",
    "    _, _, bert_f1 = bert_scorer.score([answer], [ground_truth])\n",
    "    \n",
    "    answer_token_ids = REMOVED_SECRET.encode(answer)\n",
    "    input_ids = REMOVED_SECRET.encode(question + answer, return_tensors=\"pt\").to(logits.device)\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}, Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Logits min: {logits.min().item()}, max: {logits.max().item()}\")\n",
    "    print(f\"Input IDs min: {input_ids.min().item()}, max: {input_ids.max().item()}\")\n",
    "    \n",
    "    single_token_perplexity = calculate_perplexity_for_single_token(logits, answer_token_ids[0])\n",
    "    print(f\"Single-token Perplexity: {single_token_perplexity:.4f}\")\n",
    "    \n",
    "    print(\"Tokenizer check:\")\n",
    "    print(\"Question tokens:\", REMOVED_SECRET.encode(question))\n",
    "    print(\"Answer tokens:\", answer_token_ids)\n",
    "    print(\"Tokenized question:\", REMOVED_SECRET.tokenize(question))\n",
    "    print(\"Tokenized answer:\", REMOVED_SECRET.tokenize(answer))\n",
    "    \n",
    "    print(\"Model output check:\")\n",
    "    print(\"Model output type:\", type(logits))\n",
    "    print(\"Model output shape:\", logits.shape)\n",
    "    print(\"Sample of model output:\", logits[0, 0, :10])  # First 10 values of the first token\n",
    "\n",
    "    analyze_answer_logits(logits, REMOVED_SECRET, answer)\n",
    "    \n",
    "    exact_match = ground_truth == answer\n",
    "    word_f1 = word_f1_score(answer, sample['ground_truth'])\n",
    "    \n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {rouge_scores['rougeL'].fmeasure:.4f}\")\n",
    "    print(f\"Retrieval Accuracy: {retrieval_accuracy:.4f}\")\n",
    "    print(f\"BERT Score: {bert_f1.item():.4f}\")\n",
    "    print(f\"Exact Match: {exact_match}\")\n",
    "    print(f\"Word F1 Score: {word_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'ground_truth': sample['ground_truth'],\n",
    "        'generated_answer': answer,\n",
    "        'bleu_score': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'retrieval_accuracy': retrieval_accuracy,\n",
    "        'bert_score': bert_f1.item(),\n",
    "        'single_token_perplexity': single_token_perplexity,\n",
    "        'exact_match': int(exact_match),\n",
    "        'word_f1_score': word_f1,\n",
    "        'relevant_docs': doc_contents,\n",
    "        'similarity_scores': similarity_scores.tolist()\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    pdf_path = \"test_one.pdf\"  # Replace with your PDF file path if different\n",
    "    sample = extract_data_from_pdf(pdf_path)\n",
    "\n",
    "    if sample:\n",
    "        result = evaluate_rag_system(rag_system, sample, pdf_path)\n",
    "\n",
    "        if result:\n",
    "            print(\"\\nEvaluation Results:\")\n",
    "            for key, value in result.items():\n",
    "                if key not in ['relevant_docs', 'similarity_scores']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(\"Evaluation failed.\")\n",
    "    else:\n",
    "        print(\"Failed to extract data from the PDF.\")\n",
    "\n",
    "    rag_system.clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
