{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obb/algo_folder/garble/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL REMOVED_SECRET+, currently the 'ssl' module is compiled with 'LibreSSL REMOVED_SECRET'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import fitz\n",
    "import tiktoken\n",
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from memory_profiler import profile\n",
    "from RAG_UTILS import RERANKER_MODEL, MODEL_ID, EMBEDDING_MODEL_NAME, CustomTextGenerationPipeline, RAGSystem, main, DocumentProcessor, generate_vocab_list\n",
    "#from rag_for_notebook_sunday import READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RERANKER, CustomTextGenerationPipeline, answer_with_rag, docs_processed\n",
    "from loss_functions import weighted_loss\n",
    "\n",
    "# Ensure that we are using the correct device\n",
    "device = torch.device(\"cuda\" if REMOVED_SECRET() else \"mps\" if REMOVED_SECRET.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFKeywordExtractor:\n",
    "    def __init__(self, num_keywords=50):\n",
    "        self.num_keywords = num_keywords\n",
    "        self.kw_model = KeyBERT()\n",
    "    \n",
    "    def extract_keywords(self, pdf_path):\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            document = loader.load()[0]\n",
    "            keywords = REMOVED_SECRET(document.page_content, keyphrase_ngram_range=(1, 3), top_n=self.num_keywords)\n",
    "            keywords_list = [keyword for keyword, score in keywords]\n",
    "            return keywords_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFInjector:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model fix this !!!!\n",
    "\n",
    "    def inject_text(self, input_pdf_path, output_pdf_path, text_to_inject, keywords_list, docs_processed):\n",
    "        pdf_document = fitz.open(input_pdf_path)\n",
    "        zero_width_inject_word = \"\\u200B\".join(list(text_to_inject))\n",
    "\n",
    "        for doc in docs_processed:\n",
    "            page_num = 0\n",
    "            page = pdf_document[page_num]\n",
    "            original_text = page.get_text(\"text\")\n",
    "            chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "\n",
    "            if chunk_keywords:\n",
    "                strongest_keyword = self._find_strongest_keyword(chunk_keywords, doc.page_content)\n",
    "                new_text = original_text.replace(strongest_keyword, f\"{zero_width_inject_word}{strongest_keyword}\")\n",
    "                page.clean_contents()\n",
    "                page.insert_text((0, 0), new_text, fontsize=12)\n",
    "\n",
    "        pdf_document.save(output_pdf_path)\n",
    "        pdf_document.close()\n",
    "\n",
    "    def _find_strongest_keyword(self, keywords, chunk_text):\n",
    "        chunk_embedding = REMOVED_SECRET(chunk_text, convert_to_tensor=True)\n",
    "        keyword_embeddings = [REMOVED_SECRET(kw, convert_to_tensor=True) for kw in keywords]\n",
    "        keyword_similarities = {kw: 0 for kw in keywords}\n",
    "        for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "            if kw in chunk_text:\n",
    "                similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "                keyword_similarities[kw] = similarity\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get, default=None)\n",
    "        return strongest_keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SequenceMutator:\n",
    "    def __init__(self, model, tokenizer, weight=0.8, k=32, learning_rate=0.1):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.weight = weight\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_loss(self, logits, target_response_tokens, crucial_indices):\n",
    "        # Compute the weighted loss, just as in previous functions\n",
    "        loss = weighted_loss(logits, target_response_tokens, crucial_indices, self.weight)\n",
    "        return loss\n",
    "\n",
    "    def mutate_sequence(self, seq_tokens, logits, target_response_tokens, crucial_indices):\n",
    "        # Obtain embeddings for the input sequence\n",
    "        seq_embeddings = REMOVED_SECRET()(seq_tokens.long())\n",
    "\n",
    "        # Calculate loss and backpropagate to get gradients with respect to embeddings\n",
    "        loss = self.calculate_loss(logits, target_response_tokens, crucial_indices)\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Check if gradients are available\n",
    "        if seq_embeddings.grad is None:\n",
    "            raise RuntimeError(\"Gradient computation failed; grad is None\")\n",
    "\n",
    "        new_seqs = []\n",
    "        for _ in range(self.k):\n",
    "            mutate_index = torch.randint(0, seq_embeddings.shape[1], (1,)).item()\n",
    "\n",
    "            # Mutate the token's embedding based on the gradient\n",
    "            mutated_embedding = seq_embeddings[0][mutate_index] - self.learning_rate * seq_embeddings.grad[0][mutate_index]\n",
    "\n",
    "            # Find the closest token in the vocabulary to the mutated embedding\n",
    "            distances = torch.norm(REMOVED_SECRET().weight.data - mutated_embedding, dim=1)\n",
    "            closest_token_id = torch.argmin(distances).item()\n",
    "\n",
    "            # Create a new sequence with the mutated token\n",
    "            new_seq = seq_tokens.clone()\n",
    "            new_seq[0][mutate_index] = closest_token_id\n",
    "\n",
    "            # Append mutated sequence\n",
    "            new_seqs.append(new_seq)\n",
    "\n",
    "        # Zero out the gradients for the next iteration\n",
    "        REMOVED_SECRET()\n",
    "\n",
    "        return new_seqs\n",
    "\n",
    "    def choose_best_sequence(self, new_seqs, losses):\n",
    "        # Choose the sequence with the lowest loss\n",
    "        best_index = np.argmin(losses)\n",
    "        return new_seqs[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowManager:\n",
    "    def __init__(self):\n",
    "        self.rag_system = RAGSystem(\n",
    "            embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_id=MODEL_ID,\n",
    "            reranker_model=RERANKER_MODEL\n",
    "        )\n",
    "        self.pdf_extractor = PDFKeywordExtractor()\n",
    "        self.pdf_injector = PDFInjector(REMOVED_SECRET)\n",
    "        self.mutator = SequenceMutator(REMOVED_SECRET.model, REMOVED_SECRET.tokenizer)\n",
    "        self.docs_processed = None\n",
    "\n",
    "    def run(self):\n",
    "        pdf_file_path = self._browse_for_pdf()\n",
    "\n",
    "        if not pdf_file_path:\n",
    "            print(\"No file selected.\")\n",
    "            return\n",
    "\n",
    "        keyword_results = REMOVED_SECRET(pdf_file_path)\n",
    "        if not keyword_results:\n",
    "            print(\"No keywords extracted from the PDF.\")\n",
    "            return\n",
    "        \n",
    "        self.docs_processed = REMOVED_SECRET.split_documents(\n",
    "            REMOVED_SECRET.load_pdfs_from_folder(REMOVED_SECRET(pdf_file_path))\n",
    "        )\n",
    "\n",
    "        vocab_list = generate_vocab_list(50257)\n",
    "        best_keywords = keyword_results[:5]\n",
    "        initial_sub_document = best_keywords\n",
    "        token_vocabulary = vocab_list + keyword_results\n",
    "        target_response = \"I'm sorry, I can't help with that.\"\n",
    "        query_based_on_pdf = \"What is the capital of France?\"\n",
    "        T = 5  \n",
    "        B = 3  \n",
    "        target_response_tokens = REMOVED_SECRET(target_response, return_tensors='pt').squeeze().to(device)\n",
    "        crucial_phrase = \"I'm sorry\"\n",
    "        crucial_indices = [i for i, token in enumerate(target_response_tokens) if REMOVED_SECRET([token]) in crucial_phrase.split()]\n",
    "\n",
    "        sub_document = initial_sub_document.copy()\n",
    "        rag_database_folder = \"local_database\"\n",
    "        final_candidate = self._optimize_sub_document(\n",
    "            pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, \n",
    "            crucial_indices, query_based_on_pdf, T, B, rag_database_folder\n",
    "        )\n",
    "\n",
    "        final_response_file = \"final_response.txt\"\n",
    "        with open(final_response_file, \"w\") as f:\n",
    "            f.write(final_candidate)\n",
    "        print(f\"Final response saved to {final_response_file}\")\n",
    "\n",
    "        REMOVED_SECRET()\n",
    "    \n",
    "    def _optimize_sub_document(self, pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, \n",
    "                               crucial_indices, query_based_on_pdf, T, B, rag_database_folder):\n",
    "        for i in range(T):\n",
    "            l = random.randint(0, len(sub_document) - 1)\n",
    "            candidate_sub_documents = []\n",
    "            losses = []\n",
    "            \n",
    "\n",
    "            for b in range(B):\n",
    "                new_token = random.choice(token_vocabulary)\n",
    "                candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "                \n",
    "                output_pdf_path = REMOVED_SECRET(rag_database_folder, f\"updated_pdf_{i}_{b}.pdf\")\n",
    "                REMOVED_SECRET(pdf_file_path, output_pdf_path, ' '.join(candidate), keyword_results, self.docs_processed)\n",
    "\n",
    "                pdf_file_path = output_pdf_path\n",
    "                keyword_results = REMOVED_SECRET(pdf_file_path)\n",
    "                token_vocabulary = REMOVED_SECRET + keyword_results\n",
    "\n",
    "                answer, relevant_docs, logits = REMOVED_SECRET(query_based_on_pdf, REMOVED_SECRET(self.docs_processed))\n",
    "                loss = weighted_loss(logits, target_response_tokens, crucial_indices)\n",
    "                print(f\"Iteration {i+1}/{T}, Candidate {b+1}/{B}, Loss: {loss.item()}\")\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                candidate_sub_documents.append(candidate)\n",
    "\n",
    "            best_candidate = candidate_sub_documents[np.argmin(losses)]\n",
    "            sub_document = best_candidate\n",
    "        \n",
    "        return ' '.join(sub_document)\n",
    "    \n",
    "    def _browse_for_pdf(self):\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
    "        return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "1c1eac9ae2e8418d962e5bc90740b042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aug 26, 16:52:30] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obb/algo_folder/garble/.venv/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: REMOVED_SECRET.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "Loading PDFs: 100%|██████████| 1/1 [00:00<00:00, 698.70it/s]\n",
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 2032.12it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFaceEmbeddings' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     workflow_manager \u001b[38;5;241m=\u001b[39m WorkflowManager()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mworkflow_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 43\u001b[0m, in \u001b[0;36mWorkflowManager.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m sub_document \u001b[38;5;241m=\u001b[39m initial_sub_document\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     42\u001b[0m rag_database_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_database\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 43\u001b[0m final_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_sub_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_vocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_response_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrucial_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_based_on_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_database_folder\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m final_response_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_response.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(final_response_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[5], line 68\u001b[0m, in \u001b[0;36mWorkflowManager._optimize_sub_document\u001b[0;34m(self, pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, crucial_indices, query_based_on_pdf, T, B, rag_database_folder)\u001b[0m\n\u001b[1;32m     65\u001b[0m candidate \u001b[38;5;241m=\u001b[39m sub_document[:l] \u001b[38;5;241m+\u001b[39m [new_token] \u001b[38;5;241m+\u001b[39m sub_document[l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     67\u001b[0m output_pdf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(rag_database_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdated_pdf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf_injector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_pdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocs_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m pdf_file_path \u001b[38;5;241m=\u001b[39m output_pdf_path\n\u001b[1;32m     71\u001b[0m keyword_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf_extractor\u001b[38;5;241m.\u001b[39mextract_keywords(pdf_file_path)\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mPDFInjector.inject_text\u001b[0;34m(self, input_pdf_path, output_pdf_path, text_to_inject, keywords_list, docs_processed)\u001b[0m\n\u001b[1;32m     13\u001b[0m chunk_keywords \u001b[38;5;241m=\u001b[39m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keywords_list \u001b[38;5;28;01mif\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mpage_content]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk_keywords:\n\u001b[0;32m---> 16\u001b[0m     strongest_keyword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_strongest_keyword\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     new_text \u001b[38;5;241m=\u001b[39m original_text\u001b[38;5;241m.\u001b[39mreplace(strongest_keyword, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzero_width_inject_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstrongest_keyword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     page\u001b[38;5;241m.\u001b[39mclean_contents()\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mPDFInjector._find_strongest_keyword\u001b[0;34m(self, keywords, chunk_text)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_strongest_keyword\u001b[39m(\u001b[38;5;28mself\u001b[39m, keywords, chunk_text):\n\u001b[0;32m---> 25\u001b[0m     chunk_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(chunk_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m     keyword_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode(kw, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keywords]\n\u001b[1;32m     27\u001b[0m     keyword_similarities \u001b[38;5;241m=\u001b[39m {kw: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m keywords}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFaceEmbeddings' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workflow_manager = WorkflowManager()\n",
    "    workflow_manager.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
