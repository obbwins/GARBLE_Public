{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import fitz\n",
    "import tiktoken\n",
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "import shutil\n",
    "import queue\n",
    "import uuid\n",
    "from keybert import KeyBERT\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from memory_profiler import profile\n",
    "from RAG_UTILS import RERANKER_MODEL, MODEL_ID, EMBEDDING_MODEL_NAME, CustomTextGenerationPipeline, RAGSystem, main, DocumentProcessor, generate_vocab_list\n",
    "from loss_functions import weighted_loss, label_smoothed_nll_loss\n",
    "# \"mps\" if REMOVED_SECRET.is_available() else\n",
    "# Ensure that we are using the correct device\n",
    "device = torch.device(\"cuda\" if REMOVED_SECRET() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFKeywordExtractor:\n",
    "    def __init__(self, num_keywords=5):\n",
    "        self.num_keywords = num_keywords\n",
    "        self.kw_model = KeyBERT()\n",
    "    \n",
    "    def extract_keywords(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Function to extract top keywords from selected PDF.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            document = loader.load()[0]\n",
    "            keywords = REMOVED_SECRET(document.page_content, keyphrase_ngram_range=(1, 3), top_n=self.num_keywords)\n",
    "            print(\"Keywords extracted:\", keywords)\n",
    "            keywords_list = [keyword for keyword, score in keywords]\n",
    "            return keywords_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "#class PDFInjector:\n",
    "class PDFInjector:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def inject_text(self, source_pdf_path, destination_pdf_path, text_to_inject, keywords_list, docs_processed):\n",
    "        \"\"\"\n",
    "        Function to inject text into PDF at appropriate location.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Open the source PDF\n",
    "        src_doc = fitz.open(source_pdf_path)\n",
    "        \n",
    "        # Create a new PDF document\n",
    "        dst_doc = fitz.open()\n",
    "        \n",
    "        # Copy all pages from source to destination\n",
    "        for page in src_doc:\n",
    "            dst_doc.insert_pdf(src_doc, from_page=page.number, to_page=page.number)\n",
    "\n",
    "        # Inject the text\n",
    "        for doc in docs_processed:\n",
    "            page_num = 0  # Assuming we're always injecting on the first page\n",
    "            page = dst_doc[page_num]\n",
    "\n",
    "            chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "\n",
    "            if chunk_keywords:\n",
    "                strongest_keyword = self._find_strongest_keyword(chunk_keywords, doc.page_content)\n",
    "                print(\"Strongest keyword:\", strongest_keyword)\n",
    "\n",
    "                for text_instance in page.search_for(strongest_keyword):\n",
    "                    rect = text_instance  # rectangle where keyword is found\n",
    "                    page.insert_text(rect.tl, text_to_inject, fontsize=1, color=(1,1,1))\n",
    "\n",
    "        # Save the new PDF\n",
    "        dst_doc.save(destination_pdf_path)\n",
    "        dst_doc.close()\n",
    "        src_doc.close()\n",
    "\n",
    "    def _find_strongest_keyword(self, keywords, chunk_text):\n",
    "        \"\"\"\n",
    "        \n",
    "        Find strongest keyword in chunk\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        chunk_embedding = REMOVED_SECRET.encode(chunk_text, convert_to_tensor=True)\n",
    "        keyword_embeddings = [REMOVED_SECRET.encode(kw, convert_to_tensor=True) for kw in keywords]\n",
    "        keyword_similarities = {kw: 0 for kw in keywords}\n",
    "        for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "            if kw in chunk_text:\n",
    "                similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "                keyword_similarities[kw] = similarity\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get, default=None)\n",
    "        return strongest_keyword\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import queue\n",
    "\n",
    "class DebugUI: #this class creates a debug UI for easy access to the PSO algorithm\n",
    "    def __init__(self):\n",
    "        self.root = tk.Toplevel()\n",
    "        REMOVED_SECRET(\"PSO Debug UI\")\n",
    "        REMOVED_SECRET(\"800x600\")\n",
    "\n",
    "        self.iteration_var = tk.StringVar(value=\"Iteration: 0\")\n",
    "        self.particle_var = tk.StringVar(value=\"Particle: 0\")\n",
    "        self.memory_var = tk.StringVar(value=\"Memory Usage: 0 MB\")\n",
    "        self.best_fitness_var = tk.StringVar(value=\"Best Fitness: N/A\")\n",
    "\n",
    "        ttk.Label(self.root, textvariable=self.iteration_var).pack(pady=5)\n",
    "        ttk.Label(self.root, textvariable=self.particle_var).pack(pady=5)\n",
    "        ttk.Label(self.root, textvariable=self.memory_var).pack(pady=5)\n",
    "        ttk.Label(self.root, textvariable=self.best_fitness_var).pack(pady=5)\n",
    "\n",
    "        ttk.Label(self.root, text=\"Current LLM Output:\").pack(pady=5)\n",
    "        self.llm_output_text = tk.Text(self.root, height=5, width=80, wrap=tk.WORD)\n",
    "        REMOVED_SECRET(pady=5)\n",
    "\n",
    "        ttk.Label(self.root, text=\"Log:\").pack(pady=5)\n",
    "        self.log_text = tk.Text(self.root, height=15, width=80)\n",
    "        REMOVED_SECRET(pady=5)\n",
    "\n",
    "        self.update_queue = queue.Queue()\n",
    "        self.running = True\n",
    "\n",
    "    def update_stats(self, iteration, particle, best_fitness):\n",
    "        REMOVED_SECRET((\"stats\", iteration, particle, best_fitness))\n",
    "\n",
    "    def update_llm_output(self, output):\n",
    "        REMOVED_SECRET((\"llm_output\", output))\n",
    "\n",
    "    def log(self, message):\n",
    "        REMOVED_SECRET((\"log\", message))\n",
    "\n",
    "    def update_memory(self, memory):\n",
    "        REMOVED_SECRET((\"memory\", memory))\n",
    "\n",
    "    def process_queue(self):\n",
    "        try:\n",
    "            while True:\n",
    "                item = REMOVED_SECRET()\n",
    "                if item[0] == \"stats\":\n",
    "                    _, iteration, particle, best_fitness = item\n",
    "                    REMOVED_SECRET(f\"Iteration: {iteration}\")\n",
    "                    REMOVED_SECRET(f\"Particle: {particle}\")\n",
    "                    REMOVED_SECRET(f\"Best Fitness: {best_fitness}\")\n",
    "                elif item[0] == \"llm_output\":\n",
    "                    _, output = item\n",
    "                    REMOVED_SECRET('1.0', tk.END)\n",
    "                    REMOVED_SECRET(tk.END, output)\n",
    "                elif item[0] == \"log\":\n",
    "                    _, message = item\n",
    "                    REMOVED_SECRET(tk.END, message + \"\\n\")\n",
    "                    REMOVED_SECRET(tk.END)\n",
    "                elif item[0] == \"memory\":\n",
    "                    _, memory = item\n",
    "                    REMOVED_SECRET(f\"Memory Usage: {memory} MB\")\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "        if self.running:\n",
    "            REMOVED_SECRET(100, self.process_queue)\n",
    "\n",
    "    def start(self):\n",
    "        self.process_queue()\n",
    "        REMOVED_SECRET()\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        REMOVED_SECRET()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from REMOVED_SECRET import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tkinter as tk\n",
    "from functools import lru_cache\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import concurrent.futures\n",
    "class PSOSequenceOptimizer:\n",
    "    def __init__(self, embedding_model, n_particles=5, w=0.5, c1=1, c2=1, temperature=0.1):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.n_particles = n_particles\n",
    "        self.w = w  # Inertia weight\n",
    "        self.c1 = c1  # Cognitive weight\n",
    "        self.c2 = c2  # Social weight\n",
    "        self.fitness_cache = {}\n",
    "        self.generation_data = []\n",
    "        self.debug_ui = None\n",
    "        self.temperature = temperature\n",
    "\n",
    "       \n",
    "    @lru_cache(maxsize=1000)\n",
    "    def get_embedding(self, text):\n",
    "        return REMOVED_SECRET(text)\n",
    "    \n",
    "    def contrastive_loss(self, anchor, positive, negative):\n",
    "\n",
    "        anchor = torch.tensor(anchor) if not isinstance(anchor, torch.Tensor) else anchor\n",
    "        positive = torch.tensor(positive) if not isinstance(positive, torch.Tensor) else positive\n",
    "        negative = torch.tensor(negative) if not isinstance(negative, torch.Tensor) else negative\n",
    "        #compute similarities \n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=0)\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=0)\n",
    "\n",
    "        #compute contrastive loss\n",
    "        #we want to maximize the distance between the anchor and the positive.\n",
    "        #and minimize the distance between the positive and negative.\n",
    "\n",
    "        loss = torch.log(torch.exp(pos_sim / self.temperature) / (torch.exp(pos_sim / self.temperature) + torch.exp(neg_sim / self.temperature)))\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def evaluate_sequence(self, sequence, rag_system, pdf_injector, pdf_manager, temp_pdf_path, keyword_results, \n",
    "                        query_based_on_pdf, docs_processed):\n",
    "        sequence_str = ' '.join(sequence)\n",
    "        print(f\"Evaluating sequence: {sequence_str}\")\n",
    "        if sequence_str in self.fitness_cache:\n",
    "            print(f\"Returning cached fitness: {self.fitness_cache[sequence_str]}\")\n",
    "            return self.fitness_cache[sequence_str]\n",
    "\n",
    "        # Create a fresh copy of the original PDF\n",
    "        pdf_manager.create_fresh_copy(temp_pdf_path)\n",
    "        print(f\"Created fresh copy of PDF at {temp_pdf_path}\")\n",
    "\n",
    "        # Inject the sequence into the fresh copy\n",
    "        pdf_injector.inject_text(temp_pdf_path, temp_pdf_path, sequence_str, keyword_results, docs_processed)\n",
    "        print(\"Injected text into PDF\")\n",
    "\n",
    "        # Query RAG and LLM\n",
    "        temp_docs_processed = REMOVED_SECRET(\n",
    "            REMOVED_SECRET(REMOVED_SECRET(temp_pdf_path)))\n",
    "        temp_vector_db = rag_system.build_vector_database(temp_docs_processed)\n",
    "        llm_output, relevant_docs, logits = rag_system.query_rag_system(query_based_on_pdf, temp_vector_db)\n",
    "        print(f\"RAG system query result: answer='{llm_output}', relevant_docs={relevant_docs[:100]}...\")\n",
    "\n",
    "        if self.debug_ui:\n",
    "            REMOVED_SECRET(llm_output)\n",
    "\n",
    "\n",
    "        # Compute embeddings\n",
    "        original_embedding = torch.tensor(REMOVED_SECRET(' '.join([doc.page_content for doc in docs_processed])))\n",
    "        injected_embedding = torch.tensor(REMOVED_SECRET(sequence_str))\n",
    "        llm_output_embedding = torch.tensor(REMOVED_SECRET(llm_output))\n",
    "\n",
    "        contrastive_loss = self.contrastive_loss(original_embedding, injected_embedding, llm_output_embedding)\n",
    "\n",
    "        # Compute a simple coherence measure (we still want the output to be somewhat readable if possible)\n",
    "        coherence = self.calculate_coherence(llm_output)\n",
    "\n",
    "        # Combine metrics\n",
    "        # We want to maximize semantic distance while maintaining a minimal level of coherence\n",
    "\n",
    "\n",
    "        fitness = float(contrastive_loss - 0.1 * coherence)\n",
    "\n",
    "        self.fitness_cache[sequence_str] = fitness\n",
    "        return (fitness)\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_coherence(self, text):\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            print(\"Warning: Less than two words found. Coherence set to 0.\")\n",
    "            return 0.0\n",
    "\n",
    "        # Batch process embeddings\n",
    "        embeddings = REMOVED_SECRET(words)\n",
    "\n",
    "        # Calculate coherence using cosine similarity between adjacent word embeddings\n",
    "        coherence_scores = [1 - cosine(embeddings[i], embeddings[i+1]) for i in range(len(embeddings)-1)]\n",
    "\n",
    "        avg_coherence = np.mean(coherence_scores)\n",
    "        normalized_coherence = float((avg_coherence + 1) / 2)\n",
    "\n",
    "        print(f\"Number of words: {len(words)}\")\n",
    "        print(f\"Normalized coherence: {normalized_coherence}\")\n",
    "\n",
    "        return normalized_coherence\n",
    "\n",
    "    def optimize(self, rag_system, pdf_injector, pdf_manager, temp_pdf_path, initial_sequence, keyword_results, \n",
    "                 token_vocabulary, query_based_on_pdf, docs_processed,\n",
    "                 num_iterations):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.debug_ui = DebugUI()\n",
    "        REMOVED_SECRET()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        # Initialize particles\n",
    "        particles = [Particle(token_vocabulary, len(initial_sequence)) for _ in range(self.n_particles)]\n",
    "\n",
    "        # Set first particle's position to the initial_sequence, randomize others\n",
    "        particles[0].position = initial_sequence[:]\n",
    "        for particle in particles[1:]:\n",
    "            particle.position = random.choices(token_vocabulary, k=len(initial_sequence))\n",
    "            \n",
    "        global_best_position = initial_sequence\n",
    "        global_best_fitness = float('-inf')\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            for particle_index, particle in enumerate(particles):\n",
    "                # Evaluate current position\n",
    "                fitness = self.evaluate_sequence(particle.position, rag_system, pdf_injector, pdf_manager, \n",
    "                                                 temp_pdf_path, keyword_results, \n",
    "                                                 query_based_on_pdf, docs_processed)\n",
    "                \n",
    "                REMOVED_SECRET(iteration, particle_index, global_best_fitness)\n",
    "                REMOVED_SECRET(f\"Particle {particle_index} fitness: {fitness}\")\n",
    "                REMOVED_SECRET()\n",
    "                \n",
    "\n",
    "                # Update personal best\n",
    "                if fitness == 0 or fitness > particle.best_fitness:  # Comparing loss\n",
    "                    particle.best_position = particle.position[:]\n",
    "                    particle.best_fitness = fitness\n",
    "\n",
    "                # Update global best\n",
    "                if fitness > global_best_fitness:  # Comparing loss\n",
    "                    global_best_position = particle.position[:]\n",
    "                    global_best_fitness = fitness\n",
    "            if iteration < num_iterations -1:\n",
    "                self.update_particles(particles, global_best_position, token_vocabulary)\n",
    "\n",
    "\n",
    "            print(f\"Iteration {iteration}: Best Fitness = {global_best_fitness}\")\n",
    "            REMOVED_SECRET(f\"Iteration {iteration}: Best Loss = {global_best_fitness}, Best Coherence = {-global_best_fitness}\")\n",
    "            REMOVED_SECRET()\n",
    "            REMOVED_SECRET({\n",
    "                'iteration': iteration,\n",
    "                'particles': [p.position[:] for p in particles],\n",
    "                'global_best': global_best_position[:],\n",
    "                'best_fitness': global_best_fitness,\n",
    "            })\n",
    "\n",
    "        self.visualize_optimization_progress()\n",
    "        self.animate_pso(token_vocabulary)\n",
    "        REMOVED_SECRET(\"Optimization Complete.\")\n",
    "        REMOVED_SECRET()\n",
    "        return ' '.join(global_best_position), global_best_fitness\n",
    "\n",
    "\n",
    "    def update_particles(self, particles, global_best, token_vocabulary):\n",
    "        for particle in particles:\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            for i in range(len(particle.position)):\n",
    "                cognitive = self.c1 * r1 * (token_vocabulary.index(particle.best_position[i]) - \n",
    "                                            token_vocabulary.index(particle.position[i]))\n",
    "                social = self.c2 * r2 * (token_vocabulary.index(global_best[i]) - \n",
    "                                         token_vocabulary.index(particle.position[i]))\n",
    "                \n",
    "                particle.velocity[i] = self.w * particle.velocity[i] + cognitive + social\n",
    "                \n",
    "                # Update position\n",
    "                new_index = (token_vocabulary.index(particle.position[i]) + \n",
    "                             int(round(particle.velocity[i]))) % len(token_vocabulary)\n",
    "                particle.position[i] = token_vocabulary[new_index]\n",
    "\n",
    "\n",
    "    def visualize_optimization_progress(self):\n",
    "        iterations = [data['iteration'] for data in self.generation_data]\n",
    "        best_losses = [data['best_fitness'] for data in self.generation_data]\n",
    "        best_coherences = [-data['best_fitness'] for data in self.generation_data]\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "        ax1.plot(iterations, best_losses, marker='o')\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Best Loss')\n",
    "        ax1.set_title('Loss Progression')\n",
    "\n",
    "        ax2.plot(iterations, best_coherences, marker='o', color='orange')\n",
    "        ax2.set_xlabel('Iteration')\n",
    "        ax2.set_ylabel('Best Coherence')\n",
    "        ax2.set_title('Coherence Progression')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('pso_optimization_progress.png')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"Optimization progress visualization saved as 'pso_optimization_progress.png'\")\n",
    "\n",
    "    def animate_pso(self, token_vocabulary):\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "            data = self.generation_data[frame]\n",
    "            positions = np.array([[token_vocabulary.index(token) for token in p[:3]] for p in data['particles']])\n",
    "            \n",
    "            print(f\"Frame {frame}:\")\n",
    "            print(f\"  Particle positions: {positions}\")\n",
    "            \n",
    "            # Plot particles\n",
    "            scatter = ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], c='blue', alpha=0.5)\n",
    "            \n",
    "            # Plot global best\n",
    "            best_position = np.array([token_vocabulary.index(token) for token in data['global_best'][:3]])\n",
    "            ax.scatter(best_position[0], best_position[1], best_position[2], c='red', s=100, marker='*')\n",
    "\n",
    "            print(f\"  Global best position: {best_position}\")\n",
    "\n",
    "            ax.set_xlabel('Token 1')\n",
    "            ax.set_ylabel('Token 2')\n",
    "            ax.set_zlabel('Token 3')\n",
    "            ax.set_title(f'PSO Iteration {data[\"iteration\"]}')\n",
    "            ax.set_xlim(0, len(token_vocabulary))\n",
    "            ax.set_ylim(0, len(token_vocabulary))\n",
    "            ax.set_zlim(0, len(token_vocabulary))\n",
    "\n",
    "            return scatter,\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=len(self.generation_data), interval=200, repeat=True, blit=True)\n",
    "        ani.save('pso_animation.gif', writer='pillow')\n",
    "        plt.close()\n",
    "\n",
    "        print(\"PSO animation saved as 'pso_animation.gif'\")\n",
    "class Particle:\n",
    "    def __init__(self, token_vocabulary, sequence_length):\n",
    "        self.position = random.choices(token_vocabulary, k=sequence_length)\n",
    "        self.velocity = [0] * sequence_length\n",
    "        self.best_position = self.position[:]\n",
    "        self.best_fitness = float('-inf') # (loss, coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFManager:\n",
    "    \"\"\"This class simply stores the original PDF when selected and creates temp versions for each injected sequence\"\"\"\n",
    "    def __init__(self):\n",
    "        self.original_pdf_content = None\n",
    "\n",
    "    def store_original_pdf(self, pdf_path):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        self.original_pdf_content = doc.tobytes()\n",
    "        doc.close()\n",
    "\n",
    "    def create_fresh_copy(self, output_path):\n",
    "        doc = fitz.open(\"pdf\", self.original_pdf_content)\n",
    "        doc.save(output_path)\n",
    "        doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowManager:\n",
    "    def __init__(self):\n",
    "        self.rag_system = RAGSystem(\n",
    "            embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_id=MODEL_ID,\n",
    "            reranker_model=RERANKER_MODEL\n",
    "        )\n",
    "        self.pdf_extractor = PDFKeywordExtractor()\n",
    "        self.pdf_injector = PDFInjector(REMOVED_SECRET)\n",
    "        self.docs_processed = None\n",
    "        self.local_database_path = \"local_database\" #for simplicity just a folder\n",
    "        self.optimizer = PSOSequenceOptimizer(\n",
    "            embedding_model=REMOVED_SECRET,\n",
    "            n_particles=30,\n",
    "            w=0.5,\n",
    "            c1=1,\n",
    "            c2=1,\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "        #initialise pdf manager\n",
    "        self.pdf_manager = PDFManager()\n",
    "\n",
    "    def run(self):\n",
    "        original_pdf_path = self._browse_for_pdf()\n",
    "\n",
    "        if not original_pdf_path:\n",
    "            print(\"No file selected.\")\n",
    "            return\n",
    "        \n",
    "        # Create a unique identifier for this run\n",
    "        run_id = str(uuid.uuid4())\n",
    "        \n",
    "        #store original pdf content\n",
    "        REMOVED_SECRET(original_pdf_path)\n",
    "\n",
    "        # set up temporary pdf path in local database\n",
    "        temp_pdf_path = REMOVED_SECRET(self.local_database_path, f\"temp_{run_id}.pdf\")   \n",
    "\n",
    "        #create initial temp pdf\n",
    "        REMOVED_SECRET(temp_pdf_path)\n",
    "        \n",
    "        keyword_results = REMOVED_SECRET(temp_pdf_path)\n",
    "        if not keyword_results:\n",
    "            print(\"No keywords extracted from the PDF.\")\n",
    "            os.remove(temp_pdf_path)\n",
    "            return\n",
    "        \n",
    "        self.docs_processed = REMOVED_SECRET.split_documents(\n",
    "            REMOVED_SECRET.load_pdfs_from_folder(self.local_database_path)\n",
    "        )\n",
    "\n",
    "        self.vocab_list = generate_vocab_list(50257)\n",
    "        print(\"Vocab list:\", self.vocab_list)\n",
    "        best_keywords = keyword_results[:5]\n",
    "        initial_spam = [\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\",\"!\"]\n",
    "        initial_sub_document = best_keywords #+ initial_spam\n",
    "        token_vocabulary = self.vocab_list + keyword_results\n",
    "       \n",
    "        query_based_on_pdf = \"100 acres is equal to how many hectares?\" #for this project just a simplified manual question - in future implementation, this would be automated using a language model\n",
    "\n",
    "       \n",
    "        final_candidate, best_fitness = REMOVED_SECRET(\n",
    "            rag_system=self.rag_system,\n",
    "            pdf_injector=self.pdf_injector,\n",
    "            pdf_manager=self.pdf_manager,\n",
    "            temp_pdf_path=temp_pdf_path,\n",
    "            initial_sequence=initial_sub_document,\n",
    "            keyword_results=keyword_results,\n",
    "            token_vocabulary=token_vocabulary,\n",
    "            query_based_on_pdf=query_based_on_pdf,\n",
    "            docs_processed=self.docs_processed,\n",
    "            num_iterations=10 \n",
    "        )\n",
    "        print(f\"Final candidate: {final_candidate}\")\n",
    "        print(f\"Best fitness: Loss = {best_fitness}\")\n",
    "        \n",
    "        # Create the final output PDF\n",
    "        final_output_pdf_path = REMOVED_SECRET(self.local_database_path, f\"output_{run_id}.pdf\")\n",
    "        REMOVED_SECRET(final_output_pdf_path)\n",
    "        REMOVED_SECRET(final_output_pdf_path, final_output_pdf_path, final_candidate, keyword_results, self.docs_processed)\n",
    "\n",
    "        # Remove the temporary PDF\n",
    "        os.remove(temp_pdf_path)\n",
    "\n",
    "\n",
    "\n",
    "        # Get the LLM output for the final candidate\n",
    "        final_docs_processed = REMOVED_SECRET.split_documents(\n",
    "            REMOVED_SECRET.load_pdfs_from_folder(REMOVED_SECRET(final_output_pdf_path)))\n",
    "        final_vector_db = REMOVED_SECRET(final_docs_processed)\n",
    "        final_llm_output, _, _ = REMOVED_SECRET(query_based_on_pdf, final_vector_db)\n",
    "\n",
    "        final_response_file = REMOVED_SECRET(self.local_database_path, f\"final_response_{run_id}.txt\")\n",
    "        with open(final_response_file, \"w\") as f:\n",
    "            f.write(final_candidate)\n",
    "            f.write(f\"LLM Output:\\n {final_llm_output}\")\n",
    "        print(f\"Final response saved to {final_response_file}\")\n",
    "        print(f\"Final output PDF saved to {final_output_pdf_path}\")\n",
    "\n",
    "        #cleanup\n",
    "        REMOVED_SECRET()\n",
    "    \n",
    "    \n",
    "    def _browse_for_pdf(self):\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
    "        return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workflow_manager = WorkflowManager()\n",
    "\n",
    "    workflow_manager.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
