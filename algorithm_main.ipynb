{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `REMOVED_SECRET` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import fitz\n",
    "import tiktoken\n",
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from memory_profiler import profile\n",
    "from RAG_UTILS import RERANKER_MODEL, MODEL_ID, EMBEDDING_MODEL_NAME, CustomTextGenerationPipeline, RAGSystem, main, DocumentProcessor, generate_vocab_list\n",
    "#from rag_for_notebook_sunday import READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RERANKER, CustomTextGenerationPipeline, answer_with_rag, docs_processed\n",
    "from loss_functions import weighted_loss, label_smoothed_nll_loss\n",
    "# \"mps\" if REMOVED_SECRET.is_available() else\n",
    "# Ensure that we are using the correct device\n",
    "device = torch.device(\"cuda\" if REMOVED_SECRET() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFKeywordExtractor:\n",
    "    def __init__(self, num_keywords=50):\n",
    "        self.num_keywords = num_keywords\n",
    "        self.kw_model = KeyBERT()\n",
    "    \n",
    "    def extract_keywords(self, pdf_path):\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            document = loader.load()[0]\n",
    "            keywords = REMOVED_SECRET(document.page_content, keyphrase_ngram_range=(1, 3), top_n=self.num_keywords)\n",
    "            keywords_list = [keyword for keyword, score in keywords]\n",
    "            return keywords_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFInjector:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def inject_text(self, input_pdf_path, output_pdf_path, text_to_inject, keywords_list, docs_processed, incremental_save=True):\n",
    "        pdf_document = fitz.open(input_pdf_path)\n",
    "        zero_width_inject_word = \"\\u200B\".join(list(text_to_inject))\n",
    "\n",
    "        for doc in docs_processed:\n",
    "            page_num = 0\n",
    "            page = pdf_document[page_num]\n",
    "            original_text = page.get_text(\"text\")\n",
    "            chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "\n",
    "            if chunk_keywords:\n",
    "                strongest_keyword = self._find_strongest_keyword(chunk_keywords, doc.page_content)\n",
    "                new_text = original_text.replace(strongest_keyword, f\"{zero_width_inject_word}{strongest_keyword}\")\n",
    "                page.clean_contents()\n",
    "                page.insert_text((0, 0), new_text, fontsize=12)\n",
    "\n",
    "        pdf_document.save(output_pdf_path, incremental=True,encryption=fitz.PDF_ENCRYPT_KEEP)\n",
    "        pdf_document.close()\n",
    "\n",
    "    def _find_strongest_keyword(self, keywords, chunk_text):\n",
    "\n",
    "       \n",
    "\n",
    "        chunk_embedding = REMOVED_SECRET.encode(chunk_text, convert_to_tensor=True)\n",
    "        keyword_embeddings = [REMOVED_SECRET.encode(kw, convert_to_tensor=True) for kw in keywords]\n",
    "        keyword_similarities = {kw: 0 for kw in keywords}\n",
    "        for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "            if kw in chunk_text:\n",
    "                similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "                keyword_similarities[kw] = similarity\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get, default=None)\n",
    "        return strongest_keyword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class SequenceMutator:\n",
    "    def __init__(self, model, tokenizer, weight=0.8, k=32, learning_rate=0.1):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.weight = weight\n",
    "        self.k = k\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def calculate_loss(self, logits, target_response_tokens, crucial_indices):\n",
    "        # Compute the weighted loss, just as in previous functions\n",
    "        loss = weighted_loss(logits, target_response_tokens, crucial_indices, self.weight)\n",
    "        return loss\n",
    "\n",
    "    def mutate_sequence(self, seq_tokens, target_response_tokens, crucial_indices):\n",
    "        seq_tokens = seq_tokens.long().to(REMOVED_SECRET)\n",
    "        target_response_tokens = target_response_tokens.long().to(REMOVED_SECRET)\n",
    "            \n",
    "            # Get embeddings\n",
    "        embeddings = REMOVED_SECRET()(seq_tokens)\n",
    "        embeddings.requires_grad_(True)\n",
    "\n",
    "        print(\"embeddings dtype:\", embeddings.dtype)\n",
    "        print(\"target_response_tokens dtype:\", target_response_tokens.dtype)\n",
    "        # Ensure model parameters require gradients\n",
    "        #for param in REMOVED_SECRET():\n",
    "           #param.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(inputs_embeds=embeddings)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.calculate_loss(logits, target_response_tokens, crucial_indices)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = REMOVED_SECRET(loss, embeddings, retain_graph=True)[0]\n",
    "        print(\"Gradients:\", gradients)\n",
    "        if gradients is None:\n",
    "            raise RuntimeError(\"Gradient computation failed; grad is None\")\n",
    "        \n",
    "        new_seqs = []\n",
    "        for _ in range(self.k):\n",
    "            mutate_index = torch.randint(0, seq_tokens.shape[1], (1,)).item()\n",
    "            \n",
    "            # Mutate the token's embedding based on the gradient\n",
    "            mutated_embedding = embeddings[0][mutate_index] - self.learning_rate * gradients[0][mutate_index]\n",
    "            \n",
    "            # Find the closest token in the vocabulary to the mutated embedding\n",
    "            distances = torch.norm(REMOVED_SECRET().weight.data - mutated_embedding, dim=1)\n",
    "            closest_token_id = torch.argmin(distances).item()\n",
    "            \n",
    "            # Create a new sequence with the mutated token\n",
    "            new_seq = seq_tokens.clone()\n",
    "            new_seq[0][mutate_index] = closest_token_id\n",
    "            \n",
    "            # Append mutated sequence\n",
    "            new_seqs.append(new_seq)\n",
    "        \n",
    "        # Zero out the gradients for the next iteration\n",
    "        REMOVED_SECRET()\n",
    "        \n",
    "        return new_seqs\n",
    "\n",
    "    def choose_best_sequence(self, new_seqs, losses):\n",
    "        # Choose the sequence with the lowest loss\n",
    "        best_index = np.argmin(losses)\n",
    "        return new_seqs[best_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkflowManager:\n",
    "    def __init__(self):\n",
    "        self.rag_system = RAGSystem(\n",
    "            embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "            model_id=MODEL_ID,\n",
    "            reranker_model=RERANKER_MODEL\n",
    "        )\n",
    "        self.pdf_extractor = PDFKeywordExtractor()\n",
    "        self.pdf_injector = PDFInjector(REMOVED_SECRET)\n",
    "        self.mutator = SequenceMutator(REMOVED_SECRET.model, REMOVED_SECRET.tokenizer)\n",
    "        self.docs_processed = None\n",
    "\n",
    "    def run(self):\n",
    "        pdf_file_path = self._browse_for_pdf()\n",
    "\n",
    "        if not pdf_file_path:\n",
    "            print(\"No file selected.\")\n",
    "            return\n",
    "\n",
    "        keyword_results = REMOVED_SECRET(pdf_file_path)\n",
    "        if not keyword_results:\n",
    "            print(\"No keywords extracted from the PDF.\")\n",
    "            return\n",
    "        \n",
    "        self.docs_processed = REMOVED_SECRET.split_documents(\n",
    "            REMOVED_SECRET.load_pdfs_from_folder(REMOVED_SECRET(pdf_file_path))\n",
    "        )\n",
    "\n",
    "        self.vocab_list = generate_vocab_list(50257)\n",
    "        best_keywords = keyword_results[:5]\n",
    "        initial_sub_document = best_keywords\n",
    "        token_vocabulary = self.vocab_list + keyword_results\n",
    "        target_response = \"I'm sorry, I can't help with that.\"\n",
    "        query_based_on_pdf = \"What is the capital of France?\"\n",
    "        T = 5  \n",
    "        B = 3  \n",
    "\n",
    "        target_response_tokens = REMOVED_SECRET(target_response, return_tensors='pt').squeeze().to(device)\n",
    "        crucial_phrase = \"I'm sorry\"\n",
    "        crucial_indices = [i for i, token in enumerate(target_response_tokens) if REMOVED_SECRET([token]) in crucial_phrase.split()]\n",
    "\n",
    "        sub_document = initial_sub_document.copy()\n",
    "        rag_database_folder = \"local_database\"\n",
    "        final_candidate = self._optimize_sub_document(\n",
    "            pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, \n",
    "            crucial_indices, query_based_on_pdf, T, B, rag_database_folder\n",
    "        )\n",
    "\n",
    "        final_response_file = \"final_response.txt\"\n",
    "        with open(final_response_file, \"w\") as f:\n",
    "            f.write(final_candidate)\n",
    "        print(f\"Final response saved to {final_response_file}\")\n",
    "\n",
    "        REMOVED_SECRET()\n",
    "    \n",
    "    def _optimize_sub_document(self, pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, \n",
    "                               crucial_indices, query_based_on_pdf, T, B, rag_database_folder):\n",
    "        sequence_mutator = SequenceMutator(REMOVED_SECRET.model, REMOVED_SECRET.tokenizer)\n",
    "\n",
    "        for i in range(T):\n",
    "            candidate_sub_documents = []\n",
    "            losses = []\n",
    "            \n",
    "\n",
    "            for b in range(B):\n",
    "\n",
    "                output_pdf_path = pdf_file_path\n",
    "                \n",
    "\n",
    "                REMOVED_SECRET(pdf_file_path, output_pdf_path, ' '.join(sub_document), keyword_results, self.docs_processed, incremental_save=True)\n",
    "                \n",
    "                keyword_results = REMOVED_SECRET(pdf_file_path)\n",
    "                \n",
    "\n",
    "                answer, relevant_docs, logits = REMOVED_SECRET(query_based_on_pdf, REMOVED_SECRET(self.docs_processed))\n",
    "                \n",
    "                print(\"Answer:\", answer)\n",
    "                print(\"Relevant docs:\", relevant_docs[:100] if relevant_docs else \"None\")  # Print first 100 chars\n",
    "\n",
    "                seq_tokens = torch.tensor([REMOVED_SECRET(' '.join(sub_document))], dtype=torch.long)\n",
    "                print(\"Seq tokens shape:\", seq_tokens.shape)\n",
    "                print(\"Target response tokens shape:\", target_response_tokens.shape)\n",
    "            \n",
    "                #    Use SequenceMutator to generate new sequences\n",
    "                new_seqs = sequence_mutator.mutate_sequence(seq_tokens, target_response_tokens, crucial_indices)\n",
    "                # Evaluate new sequences\n",
    "                for new_seq in new_seqs:\n",
    "                    new_sub_document = REMOVED_SECRET(new_seq[0])\n",
    "                    REMOVED_SECRET(pdf_file_path, output_pdf_path, new_sub_document, keyword_results, self.docs_processed, incremental_save=True)\n",
    "                    answer, relevant_docs, new_logits = REMOVED_SECRET(query_based_on_pdf, REMOVED_SECRET(self.docs_processed))\n",
    "                \n",
    "                    try:\n",
    "                        loss = weighted_loss(new_logits, target_response_tokens, crucial_indices)\n",
    "                        losses.append(loss.item())\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in loss calculation: {e}\")\n",
    "                        losses.append(float('inf'))\n",
    "\n",
    "                    candidate_sub_documents.append(new_sub_document.split())\n",
    "\n",
    "                print(f\"Iteration {i+1}/{T}, Candidate {b+1}/{B}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "            best_candidate = sequence_mutator.choose_best_sequence(candidate_sub_documents, losses)\n",
    "            sub_document = best_candidate\n",
    "        \n",
    "        return ' '.join(sub_document)\n",
    "    \n",
    "    def _browse_for_pdf(self):\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"PDF Files\", \"*.pdf\")])\n",
    "        return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "294e67b9a72246f7a36f79e9fbd7f52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Loading PDFs: 100%|██████████| 2/2 [00:00<00:00, 349.51it/s]\n",
      "Splitting Documents: 100%|██████████| 2/2 [00:00<00:00, 766.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.99it/s]\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,    13,  2677, 29901,    13,    13,  9651, 10854,\n",
      "         29871, 29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444,\n",
      "         29973,  3681, 29991,    13,    13,  9651,   894, 29901,  1724,   338,\n",
      "           278,  7483,   310,  3444, 29973,    13,   268,    13,  9651,  2799,\n",
      "           582,  1953]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Answer: \n",
      "            Answer:\n",
      "            \n",
      "            Paris\n",
      "\n",
      "\n",
      "Document 2:\n",
      "\n",
      "Context:\n",
      "\n",
      "            Document 1:\n",
      "What is the capital of France? Paris!\n",
      "\n",
      "            Question: What is the capital of France?\n",
      "    \n",
      "            Instructions\n",
      "Relevant docs: ['What is the capital of France? Paris!']\n",
      "Seq tokens shape: torch.Size([1, 16])\n",
      "Target response tokens shape: torch.Size([13])\n",
      "embeddings dtype: torch.float16\n",
      "target_response_tokens dtype: torch.int64\n",
      "Logits shape: torch.Size([1, 16, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Sample logits: tensor([[18.5625, 20.0781, 21.2969, 13.9453, 14.2266],\n",
      "        [13.6719,  8.2109, 12.8516, 14.4531,  9.7891],\n",
      "        [26.6562, 26.1875, 27.5469, 23.2500, 23.4219],\n",
      "        [21.3125, 20.2500, 19.3906, 15.8438, 15.3516],\n",
      "        [28.8750, 28.3438, 28.2188, 24.9219, 24.8438]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 11.06579303741455\n",
      "Crucial Loss: 15.432750701904297\n",
      "Weighted Loss: 14.559359550476074\n",
      "Gradients: tensor([[[-0.0275, -0.0560,  0.0051,  ...,  0.0165,  0.1165,  0.2153],\n",
      "         [ 0.2410, -0.0375,  0.1050,  ...,  0.1254,  0.2874, -0.0189],\n",
      "         [ 0.0411, -0.0798, -0.0547,  ..., -0.0707, -0.4270,  0.2396],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,    13,  2677, 29901,    13,    13,  9651, 10854,\n",
      "         29871, 29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444,\n",
      "         29973,  3681, 29991,    13,    13,  9651,   894, 29901,  1724,   338,\n",
      "           278,  7483,   310,  3444, 29973,    13,   268,    13,  9651,  2799,\n",
      "           582,  1953]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 50, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Warning: NaN or Inf values in logits\n",
      "Sample logits: tensor([[-1.0000e+06,  7.5729e+01,  7.0677e+01, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],\n",
      "       device='cuda:0')\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 963837.9375\n",
      "Crucial Loss: 963848.6875\n",
      "Weighted Loss: 963843.3125\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 115.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,    13,  2677, 29901,    13,    13,  9651, 10854,\n",
      "         29871, 29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444,\n",
      "         29973,  3681, 29991,    13,    13,  9651,   894, 29901,  1724,   338,\n",
      "           278,  7483,   310,  3444, 29973,    13,   268,    13,  9651,  2799,\n",
      "           582,  1953]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 50, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Warning: NaN or Inf values in logits\n",
      "Sample logits: tensor([[-1.0000e+06,  7.5729e+01,  7.0677e+01, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],\n",
      "       device='cuda:0')\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 963837.9375\n",
      "Crucial Loss: 963848.6875\n",
      "Weighted Loss: 963843.3125\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,    13,  2677, 29901,    13,    13,  9651, 10854,\n",
      "         29871, 29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444,\n",
      "         29973,  3681, 29991,    13,    13,  9651, 10854, 29871, 29906, 29901,\n",
      "            13,  1576,   382,  2593,   295, 23615,   338,   263, 13834,  2982,\n",
      "          3502,  5982]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 50, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Warning: NaN or Inf values in logits\n",
      "Sample logits: tensor([[-1.0000e+06,  7.5729e+01,  7.0677e+01, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],\n",
      "       device='cuda:0')\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 963837.9375\n",
      "Crucial Loss: 963848.6875\n",
      "Weighted Loss: 963843.3125\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,  2677, 29901,    13,    13,  9651, 10854, 29871,\n",
      "         29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444, 29973,\n",
      "          3681, 29991,    13,    13,  9651, 10854, 29871, 29906, 29901,    13,\n",
      "          8809,   436,  4272,   338,  2998,   408,   278,  4412,   310,   365,\n",
      "          5861,   322]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 50, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Warning: NaN or Inf values in logits\n",
      "Sample logits: tensor([[-1.0000e+06,  7.5729e+01,  7.0677e+01, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],\n",
      "       device='cuda:0')\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 894600.5\n",
      "Crucial Loss: 963848.6875\n",
      "Weighted Loss: 929224.625\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: tensor([[15228, 29901,    13,    13,  9651, 10854, 29871, 29896, 29901,    13,\n",
      "          5618,   338,   278,  7483,   310,  3444, 29973,  3681, 29991,    13,\n",
      "            13,  9651,   894, 29901,  1724,   338,   278,  7483,   310,  3444,\n",
      "         29973,    13,   268,    13,  9651,  2799,   582,  1953, 29901,  5293,\n",
      "           278,  2472,   515,   278,  3030, 29892,  3867,   263,  3022,   895,\n",
      "           322,  1513,  1234,   304,   278,  1139, 29889,  1938,   451, 12312,\n",
      "           278,  1139,   470,   278,  3030, 29889,  3387,  2106,   278,  1234,\n",
      "          9436,   322, 23359, 29889,    13,   632,    13,  9651,   673, 29901,\n",
      "            13,   632,    13,  9651,  3681,    13,    13,    13,  6268, 29871,\n",
      "         29906, 29901,    13,    13,  2677, 29901,    13,    13,  9651, 10854,\n",
      "         29871, 29896, 29901,    13,  5618,   338,   278,  7483,   310,  3444,\n",
      "         29973,  3681, 29991,    13,    13,  9651, 10854, 29871, 29906, 29901,\n",
      "            13,  1576,   382,  2593,   295, 23615,   338,   263, 13834,  2982,\n",
      "          3502,  5982]], device='cuda:0')\n",
      "Logits tensor([[[   -inf, 75.7292, 70.6771,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         ...,\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
      "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf]]],\n",
      "       device='cuda:0')\n",
      "Logits shape: torch.Size([1, 50, 32064])\n",
      "T_Res shape: torch.Size([13])\n",
      "Crucial indices: [3]\n",
      "Warning: NaN or Inf values in logits\n",
      "Sample logits: tensor([[-1.0000e+06,  7.5729e+01,  7.0677e+01, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06],\n",
      "        [-1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06, -1.0000e+06]],\n",
      "       device='cuda:0')\n",
      "Sample t_res: tensor([  306, 29915, 29885,  7423, 29892], device='cuda:0')\n",
      "Main Loss: 963837.9375\n",
      "Crucial Loss: 963848.6875\n",
      "Weighted Loss: 963843.3125\n",
      "=> Retrieving documents...\n",
      "Retrieved 2 documents.\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked to 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     workflow_manager \u001b[38;5;241m=\u001b[39m WorkflowManager()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mworkflow_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m, in \u001b[0;36mWorkflowManager.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m sub_document \u001b[38;5;241m=\u001b[39m initial_sub_document\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     43\u001b[0m rag_database_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_database\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m final_candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_sub_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpdf_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_vocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_response_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcrucial_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_based_on_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_database_folder\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m final_response_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_response.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(final_response_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mWorkflowManager._optimize_sub_document\u001b[0;34m(self, pdf_file_path, sub_document, keyword_results, token_vocabulary, target_response_tokens, crucial_indices, query_based_on_pdf, T, B, rag_database_folder)\u001b[0m\n\u001b[1;32m     88\u001b[0m new_sub_document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrag_system\u001b[38;5;241m.\u001b[39mreader_llm\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(new_seq[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpdf_injector\u001b[38;5;241m.\u001b[39minject_text(pdf_file_path, output_pdf_path, new_sub_document, keyword_results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs_processed, incremental_save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m answer, relevant_docs, new_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_based_on_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrag_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vector_database\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocs_processed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     loss \u001b[38;5;241m=\u001b[39m weighted_loss(new_logits, target_response_tokens, crucial_indices)\n",
      "File \u001b[0;32m~/codes/langers/RAG_UTILS.py:263\u001b[0m, in \u001b[0;36mRAGSystem.query_rag_system\u001b[0;34m(self, question, knowledge_index, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_rag_system\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, knowledge_index: FAISS, num_retrieved_docs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m, num_docs_final: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m    Queries the RAG system with the given question.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m        Tuple[str, List[str], torch.Tensor]: Generated answer, list of relevant documents, and logits tensor.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswer_with_rag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknowledge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_docs_final\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_docs_final\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/RAG_UTILS.py:330\u001b[0m, in \u001b[0;36mRAGSystem.answer_with_rag\u001b[0;34m(self, question, knowledge_index, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Generate answer with logits\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> Generating answer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 330\u001b[0m generated_sequence, logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# Post-processing: Extract the answer part (optional based on how the model responds)\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# This example assumes the model generates the answer directly after the prompt.\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#answer = generated_text.split(\"Question:\")[0].strip()\u001b[39;00m\n\u001b[1;32m    343\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader_llm\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(final_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/codes/langers/RAG_UTILS.py:179\u001b[0m, in \u001b[0;36mCustomTextGenerationPipeline.generate_with_logits\u001b[0;34m(self, prompt, max_new_tokens, do_sample, temperature)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Perform generation\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 179\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Decode generated tokens\u001b[39;00m\n\u001b[1;32m    182\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-REMOVED_SECRETd4ed9ff69/modeling_phi3.py:1243\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1240\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1243\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1256\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-REMOVED_SECRETd4ed9ff69/modeling_phi3.py:1121\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1112\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1113\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         use_cache,\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-REMOVED_SECRETd4ed9ff69/modeling_phi3.py:842\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m attn_outputs, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_attn_dropout(attn_outputs)\n\u001b[1;32m    853\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-REMOVED_SECRETd4ed9ff69/modeling_phi3.py:541\u001b[0m, in \u001b[0;36mPhi3FlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flash_attention_forward(\n\u001b[1;32m    531\u001b[0m     query_states,\n\u001b[1;32m    532\u001b[0m     key_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m     use_sliding_windows\u001b[38;5;241m=\u001b[39muse_sliding_windows,\n\u001b[1;32m    538\u001b[0m )\n\u001b[1;32m    540\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 541\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    544\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 164\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hf_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mno_grad:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/hooks.py:354\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m             value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdata_ptr() \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_params_map[value\u001b[38;5;241m.\u001b[39mdata_ptr()]\n\u001b[1;32m    351\u001b[0m         ):\n\u001b[1;32m    352\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtied_pointers_to_remove\u001b[38;5;241m.\u001b[39madd((value\u001b[38;5;241m.\u001b[39mdata_ptr(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device))\n\u001b[0;32m--> 354\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfp16_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfp16_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m send_to_device(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    364\u001b[0m     kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device, skip_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_keys\n\u001b[1;32m    365\u001b[0m )\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/accelerate/utils/modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    414\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 416\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workflow_manager = WorkflowManager()\n",
    "\n",
    "    workflow_manager.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
