{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "a56dbd509f2d43c5b75dab05528948f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "503a17ba23af4a58aa6a9bfc9420183c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "36d3309ee21b4e8bb485253910db89b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Got through retrieval. ['What is the capital of France? Paris!', 'What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· · ·a·p·p·e·a·l·s· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· · ·a·p·p·e·a·l·s· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! at s t e c a p t a a c e p a s a p p e a s a c e p a s p a s a c ecap ta o a\\n\\nWhat is the capital of France? Paris!']\n",
      "Relevant Docs:, ['What is the capital of France? Paris!', 'What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· · ·a·p·p·e·a·l·s· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· · ·a·p·p·e·a·l·s· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! at s t e c a p t a a c e p a s a p p e a s a c e p a s p a s a c ecap ta o a\\n\\nWhat is the capital of France? Paris!']\n",
      "['What is the capital of France? Paris!']\n",
      "Got to context.\n",
      "=> Generating answer...\n",
      "The capital of France is Paris (Document 0).<|end|><|endoftext|>\n",
      "All_Logits: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Logits in beginning: torch.Size([1, 416832])\n",
      "T_Res shape: torch.Size([13])\n",
      "Logits view: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "T_res: tensor([  306, 29915, 29885,  7423, 29892,   306,   508, 29915, 29873,  1371,\n",
      "          411,   393, 29889], device='cuda:0')\n",
      "Logits after reshape torch.Size([13, 50257])\n",
      "Loss during weighted loss function: tensor(5482.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1])\n",
      "Crucial Loss at end: tensor(10159.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Weighted loss from loss func: tensor(7821.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "lm_head.weight grad: None\n",
      "Input Gradients: None\n",
      "Logits: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Input Gradients: None\n",
      "==================================Answer==================================\n",
      "The capital of France is Paris (Document 0).<|end|>\n",
      "==================================Source docs==================================\n",
      "Document 0------------------------------------------------------------\n",
      "What is the capital of France? Paris!\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import fitz\n",
    "import tiktoken\n",
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from memory_profiler import profile\n",
    "from rag_for_notebook_sunday import READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RERANKER, CustomTextGenerationPipeline, answer_with_rag, docs_processed\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary and Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Token ID: 32000\n",
      "Estimated Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = READER_LLM.model\n",
    "#model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map='cuda', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "padding_token__id = tokenizer.pad_token_id\n",
    "print(\"Padding Token ID:\", padding_token__id)\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\") #openai embedding model (used for vocab)\n",
    "\n",
    "vocab_size = 50257 #random but large guess at vocab size\n",
    "vocab_list = []\n",
    "\n",
    "# iterate through a range of possible token indices\n",
    "for token_id in range(vocab_size):\n",
    "    try:\n",
    "        #decode the token_id to get the corresponding token string\n",
    "        token = encoding.decode([token_id])\n",
    "        # add the token to the vocabulary list\n",
    "        vocab_list.append(token)\n",
    "    except KeyError:\n",
    "        #if decoding fails, it's likely an out-of-vocabulary token or a special token\n",
    "        \n",
    "        pass\n",
    "\n",
    "    \n",
    "print(f\"Estimated Vocabulary size: {len(vocab_list)}\")\n",
    "#^ get vocabulary from tiktoken (openai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_rag_system(question, llm=READER_LLM, knowledge_index = KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER):\n",
    "    \"\"\"\n",
    "    Queries the RAG system with the given question.\n",
    "\n",
    "    Args:\n",
    "        question: The question to ask the RAG system.\n",
    "        llm: The language model to use for answer generation (default: READER_LLM).\n",
    "        knowledge_index: The vector store containing the document embeddings (default: KNOWLEDGE_VECTOR_DATABASE).\n",
    "        reranker: The reranker model (optional, default: RERANKER).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - answer: The generated answer from the RAG system.\n",
    "            - relevant_docs: The list of retrieved relevant documents.\n",
    "            - logits: The logits of the generated answer.\n",
    "    \"\"\"\n",
    "    #print(f\"Type of question: {type(question)}\")\n",
    "    #print(f\"Type of knowledge_index: {type(knowledge_index)}\")\n",
    "    #print(f\"Type of llm: {type(llm)}\")\n",
    "    #print(f\"Type of reranker: {type(reranker)}\")\n",
    "    # query the RAG system and get the answer, relevant docs, and logits\n",
    "    try:\n",
    "        answer, relevant_docs, logits = answer_with_rag(\n",
    "            question=question,\n",
    "            llm=llm,\n",
    "            knowledge_index=knowledge_index,\n",
    "            reranker=reranker\n",
    "        )\n",
    "\n",
    "        return answer, relevant_docs, logits\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError in answer_with_rag: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def find_strongest_keyword(keywords, chunk_text):\n",
    "    \"\"\"\n",
    "    Finds the keyword with the highest average semantic similarity to the chunk's context.\n",
    "    \"\"\"\n",
    "\n",
    "    # encode the chunk and keywords using the embedding model\n",
    "    chunk_embedding = embedding_model.encode(chunk_text, convert_to_tensor=True)\n",
    "    keyword_embeddings = [embedding_model.encode(kw, convert_to_tensor=True) for kw in keywords]\n",
    "\n",
    "    # calculate cosine similarity between each keyword and the chunk\n",
    "    keyword_similarities = {kw: 0 for kw in keywords}  # Initialize similarities\n",
    "    for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "        if kw in chunk_text:  # Check if the keyword is present in the chunk\n",
    "            similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "            keyword_similarities[kw] = similarity\n",
    "\n",
    "    # Find the keyword with the highest similarity\n",
    "    if keyword_similarities:\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get)\n",
    "        return strongest_keyword\n",
    "    else:\n",
    "        return None   # No keywords found in the chunk\n",
    "\n",
    "\n",
    "def inject_text_into_pdf(input_pdf_path, output_pdf_path, text_to_inject, keywords_list, docs_processed):\n",
    "    \"\"\"\n",
    "    Injects adversarial text into the selected PDF.\n",
    "\n",
    "    Args:\n",
    "    input_pdf_path: selected PDF path from browse_for_pdf function\n",
    "    output_pdf_path: desired location of new PDF\n",
    "    text_to_inject: the adversarial sequence to be injected\n",
    "    keywords_list: a list of keywords from the selected PDF to push into find_strongest_keyword\n",
    "    docs_processed: embedded processed documents (one pdf can be multiple docs)\n",
    "    \n",
    "    \"\"\"\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "\n",
    "    # Create the zero-width version of the injected word\n",
    "    zero_width_inject_word = \"\\u200B\".join(list(text_to_inject))\n",
    "\n",
    "    for doc in docs_processed:\n",
    "       # page_num = doc.metadata['page'] \n",
    "        page_num = 0\n",
    "        page = pdf_document[page_num] \n",
    "        original_text = page.get_text(\"text\")\n",
    "\n",
    "        # Find keywords within this chunk\n",
    "        chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "\n",
    "        if chunk_keywords:\n",
    "            # Find the keyword with the highest semantic strength (you'll need to implement this)\n",
    "            strongest_keyword = find_strongest_keyword(chunk_keywords, doc.page_content) \n",
    "\n",
    "            # Inject before the strongest keyword\n",
    "            new_text = original_text.replace(strongest_keyword, f\"{zero_width_inject_word}{strongest_keyword}\")\n",
    "\n",
    "            page.clean_contents()\n",
    "            page.insert_text((0, 0), new_text, fontsize=12)\n",
    "\n",
    "       # print(f\"Processing page {page_num + 1} of {len(pdf_document)}\")\n",
    "\n",
    "    pdf_document.save(output_pdf_path)\n",
    "    pdf_document.close()\n",
    "    print(\"Injection complete!\")\n",
    "\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=50):\n",
    "\n",
    "    keywords_list = []\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]\n",
    "        kw_model = KeyBERT()\n",
    "        keywords = kw_model.extract_keywords(document.page_content, keyphrase_ngram_range=(1, 3), top_n=num_keywords)\n",
    "        keywords_list = [keyword for keyword, score in keywords]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "    return keywords_list\n",
    "\n",
    "def browse_for_pdf():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "from loss_functions import weighted_loss\n",
    "\n",
    "def mutate_seq_with_gradient(seq_tokens, logits, target_response_tokens, crucial_indices, input_gradients, weight=0.8, k=32, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Mutates the sequence based on the gradient of the weighted loss.\n",
    "\n",
    "    Args:\n",
    "        seq_tokens: The current attack sequence (tokenized) as FloatTensor with requires_grad=True.\n",
    "        logits: The raw output of the LLM before the final softmax layer.\n",
    "        target_response_tokens: The targeted malicious response (tokenized).\n",
    "        crucial_indices: Indices of the crucial tokens in the target response.\n",
    "        weight: The weight assigned to the crucial loss component.\n",
    "        k: The number of new sequences to generate.\n",
    "        learning_rate: The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        A list of k mutated sequences.\n",
    "    \"\"\"\n",
    "    #for param in REMOVED_SECRET():\n",
    "        #param.requires_grad = True\n",
    "    # Convert seq_tokens to FloatTensor with requires_grad=True if it's not already\n",
    "    seq_tokens = seq_tokens.float().requires_grad_(True)\n",
    "    seq_tokens.retain_grad()\n",
    "    # Get the embedding of the sequence tokens\n",
    "    REMOVED_SECRET().weight.requires_grad = True\n",
    "\n",
    "    #embedding_layer = REMOVED_SECRET()\n",
    "\n",
    "   \n",
    "    seq_embeddings = REMOVED_SECRET()(seq_tokens.long())\n",
    "    seq_embeddings.retain_grad()\n",
    "    print(f\"seq_embeddings.requires_grad: {seq_embeddings.requires_grad}\")\n",
    "\n",
    "    #seq_embeddings = REMOVED_SECRET()(seq_tokens.long())\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate the weighted loss and its gradient\n",
    "    loss = weighted_loss(logits, target_response_tokens, crucial_indices, weight)\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    #loss.backward(retain_graph=True) \n",
    "    if logits.grad is None:\n",
    "        raise RuntimeError(\"Logits gradient is None after backpropagation\")\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "    for name, param in REMOVED_SECRET():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name} grad: {param.grad}\")\n",
    "\n",
    "    # Get the gradient with respect to the embedded input sequence\n",
    "    grad = seq_embeddings.grad\n",
    "    print(f\"Grad: {grad}\")\n",
    "\n",
    "    if grad is None:\n",
    "       raise RuntimeError(\"Gradient computation failed; grad is None\")\n",
    "\n",
    "    new_seqs = []\n",
    "    for _ in range(k):\n",
    "        # Randomly select a token to mutate\n",
    "        mutate_index = torch.randint(0, seq_embeddings.shape[1], (1,)).item()\n",
    "\n",
    "        # Mutate the token's embedding based on the gradient\n",
    "        mutated_embedding = seq_embeddings[0][mutate_index] - learning_rate * grad[0][mutate_index]\n",
    "\n",
    "        # Find the closest token in the vocabulary to the mutated embedding\n",
    "        distances = torch.norm(REMOVED_SECRET().weight.data - mutated_embedding, dim=1)\n",
    "        closest_token_id = torch.argmin(distances).item()\n",
    "\n",
    "        # Create a new sequence with the mutated token\n",
    "        new_seq = seq_tokens.clone()  # No detach here; we want to retain the graph\n",
    "        new_seq[0][mutate_index] = closest_token_id\n",
    "        new_seq.requires_grad_(True)  # Ensure the new sequence retains the gradient\n",
    "\n",
    "        new_seqs.append(new_seq)\n",
    "\n",
    "    # Zero out the gradients for the next iteration\n",
    "    REMOVED_SECRET()\n",
    "\n",
    "    return new_seqs\n",
    "\n",
    "\n",
    "def choose_best_sequence(new_seqs, losses):\n",
    "    \"\"\"\n",
    "    Chooses the best sequence from the mutated candidates based on their pre-calculated losses.\n",
    "\n",
    "    Args:\n",
    "        new_seqs: A list of mutated sequences (tokenized).\n",
    "        losses: A list of corresponding loss values for each sequence.\n",
    "\n",
    "    Returns:\n",
    "        The best sequence (tokenized) with the lowest loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_index = np.argmin(losses)\n",
    "    return new_seqs[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injection complete!\n",
      "Type of query_based_on_pdf: <class 'str'>\n",
      "=> Retrieving documents...\n",
      "<class 'list'>\n",
      "Got through retrieval. ['What is the capital of France? Paris!', 'What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· ·(·a·u·t·o· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! What is the c·a·p·i·t·a·l· ·f·r·a·n·c·e· ·p·a·r·i·s· ·(·a·u·t·o· ·f·r·a·n·c·e· ·p·a·r·i·s· ·p·a·r·i·s· ·f·r·a·n·c·ecapital of France? Paris! at s t e c a p t a a c e p a s ( a u t o a c e p a s p a s a c ecap ta o a ce\\n\\nWhat is the capital of France? Paris!']\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 92.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Docs:, ['What is the capital of France? Paris!']\n",
      "['What is the capital of France? Paris!']\n",
      "Got to context.\n",
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. (Document 0)<|end|><|endoftext|>\n",
      "All_Logits: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Logits in beginning: torch.Size([1, 448896])\n",
      "T_Res shape: torch.Size([13])\n",
      "Logits view: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "T_res: tensor([  306, 29915, 29885,  7423, 29892,   306,   508, 29915, 29873,  1371,\n",
      "          411,   393, 29889], device='cuda:0')\n",
      "Logits after reshape torch.Size([13, 50257])\n",
      "Loss during weighted loss function: tensor(6259.2847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1])\n",
      "Crucial Loss at end: tensor(10159.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Weighted loss from loss func: tensor(8209.3770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logits: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0',\n",
      "       grad_fn=<CatBackward0>)\n",
      "Iteration 1/5, Batch 1/3: Candidate response: The capital of France is Paris. (Document 0)<|end|>\n",
      "Logits in beginning: torch.Size([1, 448896])\n",
      "T_Res shape: torch.Size([13])\n",
      "Logits view: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "T_res: tensor([  306, 29915, 29885,  7423, 29892,   306,   508, 29915, 29873,  1371,\n",
      "          411,   393, 29889], device='cuda:0')\n",
      "Logits after reshape torch.Size([13, 50257])\n",
      "Loss during weighted loss function: tensor(6259.2847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1])\n",
      "Crucial Loss at end: tensor(10159.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Weighted loss from loss func: tensor(8209.3770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss: 8209.3770\n",
      "candidate_tokens requires_grad: True\n",
      "Candidate Tokens Grad: None\n",
      "seq_embeddings.requires_grad: True\n",
      "Logits in beginning: torch.Size([1, 448896])\n",
      "T_Res shape: torch.Size([13])\n",
      "Logits view: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n",
      "T_res: tensor([  306, 29915, 29885,  7423, 29892,   306,   508, 29915, 29873,  1371,\n",
      "          411,   393, 29889], device='cuda:0')\n",
      "Logits after reshape torch.Size([13, 50257])\n",
      "Loss during weighted loss function: tensor(6259.2847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "torch.Size([1, 50257])\n",
      "torch.Size([1])\n",
      "Crucial Loss at end: tensor(10159.4688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Weighted loss from loss func: tensor(9379.4316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Loss: 9379.431640625\n",
      "Loss: 9379.431640625\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET.REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "REMOVED_SECRET grad: None\n",
      "lm_head.weight grad: None\n",
      "Grad: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Gradient computation failed; grad is None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_tokens requires_grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidate_tokens\u001b[38;5;241m.\u001b[39mrequires_grad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should print True\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate Tokens Grad:\u001b[39m\u001b[38;5;124m\"\u001b[39m, candidate_tokens\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m---> 72\u001b[0m new_seqs \u001b[38;5;241m=\u001b[39m \u001b[43mmutate_seq_with_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_response_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrucial_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Store the loss value\u001b[39;00m\n\u001b[1;32m     75\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[3], line 177\u001b[0m, in \u001b[0;36mmutate_seq_with_gradient\u001b[0;34m(seq_tokens, logits, target_response_tokens, crucial_indices, weight, k, learning_rate)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m    \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradient computation failed; grad is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m new_seqs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# Randomly select a token to mutate\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Gradient computation failed; grad is None"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main Workflow\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "#\n",
    "if pdf_file_path:\n",
    "\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "\n",
    "    if keyword_results:\n",
    "        best_keywords = keyword_results[:5]\n",
    "        initial_sub_document = best_keywords\n",
    "        initial_vocab = vocab_list\n",
    "        token_vocabulary = initial_vocab + keyword_results\n",
    "        target_response = \"I'm sorry, I can't help with that.\"\n",
    "        query_based_on_pdf = \"What is the capital of France?\"\n",
    "\n",
    "        T = 5  \n",
    "        B = 3  \n",
    "\n",
    "        target_response_tokens = tokenizer.encode(target_response, return_tensors='pt').squeeze().to(model.device)\n",
    "        crucial_phrase = \"I'm sorry\"\n",
    "        crucial_indices = [i for i, token in enumerate(target_response_tokens) if tokenizer.decode([token]) in crucial_phrase.split()]\n",
    "\n",
    "        sub_document = initial_sub_document.copy()\n",
    "        rag_database_folder = \"local_database\"\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        for i in range(T):\n",
    "            l = random.randint(0, len(sub_document) - 1)\n",
    "            candidate_sub_documents = []\n",
    "            losses = []\n",
    "\n",
    "            for b in range(B):\n",
    "                new_token = random.choice(token_vocabulary)\n",
    "                candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "                \n",
    "                # Inject candidate into the PDF at the strongest keyword locations\n",
    "                output_pdf_path = REMOVED_SECRET(rag_database_folder, f\"updated_pdf_{i}_{b}.pdf\")\n",
    "                inject_text_into_pdf(pdf_file_path, output_pdf_path, ' '.join(candidate), keyword_results, docs_processed)\n",
    "\n",
    "                #re-load modified pdf and update keywords & vocab\n",
    "                pdf_file_path = output_pdf_path\n",
    "                keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "                token_vocabulary = initial_vocab + keyword_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #query RAG system and get logits\n",
    "                print(f\"Type of query_based_on_pdf: {type(query_based_on_pdf)}\")\n",
    "\n",
    "                answer, relevant_docs, logits = query_rag_system(query_based_on_pdf)\n",
    "                print(f\"Iteration {i+1}/{T}, Batch {b+1}/{B}: Candidate response: {answer}\")\n",
    "\n",
    "                # Calculate weighted loss\n",
    "                loss = weighted_loss(logits, target_response_tokens, crucial_indices)\n",
    "                print(f\"Loss: {loss:.4f}\")\n",
    "               \n",
    "                \n",
    "                \n",
    "                # Mutate the sequence based on the gradient and loss\n",
    "                #candidate_tokens = tokenizer.encode(' '.join(candidate))\n",
    "                #candidate_tokens = torch.tensor([candidate_tokens], dtype=torch.float32, device=model.device, requires_grad=True)       \n",
    "                candidate_tokens = torch.tensor([tokenizer.encode(' '.join(candidate))], dtype=torch.float32, device=model.device)           \n",
    "                candidate_tokens.requires_grad_(True)\n",
    "                candidate_tokens.retain_grad()\n",
    "                \n",
    "                print(f\"candidate_tokens requires_grad: {candidate_tokens.requires_grad}\")  # Should print True\n",
    "                print(\"Candidate Tokens Grad:\", candidate_tokens.grad)\n",
    "                new_seqs = mutate_seq_with_gradient(candidate_tokens, logits, target_response_tokens, crucial_indices)\n",
    "\n",
    "                # Store the loss value\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                best_seq = choose_best_sequence(new_seqs) \n",
    "\n",
    "                # Update candidate based on the best sequence\n",
    "                candidate = tokenizer.decode(best_seq[0], skip_special_tokens=True).split()\n",
    "                candidate_sub_documents.append(candidate)\n",
    "               \n",
    "\n",
    "            # Select the best candidate based on loss (lower loss is better)\n",
    "            best_candidate_index = np.argmin(losses)\n",
    "            sub_document = candidate_sub_documents[best_candidate_index]\n",
    "            print(f\"Iteration {i+1}/{T}: Best candidate sub-document: {' '.join(sub_document)} (Loss: {losses[best_candidate_index]:.4f})\")\n",
    "            # Early stopping if loss is below a threshold (optional)\n",
    "            if losses[best_candidate_index] < 0.4:  # Adjust the threshold as needed\n",
    "                    break\n",
    "\n",
    "        final_sub_document_text = ' '.join(sub_document)\n",
    "        print(f\"Final optimized sub-document: {final_sub_document_text}\")\n",
    "\n",
    "        final_response_file = \"final_response.txt\"\n",
    "        with open(final_response_file, \"w\") as f:\n",
    "            f.write(final_sub_document_text)\n",
    "        print(f\"Final response saved to {final_response_file}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No keywords extracted from the PDF.\")\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_142180/1996318517.py\u001b[0m(203)\u001b[0;36mmutate_seq_with_gradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    201 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    202 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 203 \u001b[0;31m    \u001b[0mseq_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"seq_embeddings.requires_grad: {seq_embeddings.requires_grad}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    205 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "*** NameError: name 'seq_embeddings' is not defined\n",
      "> \u001b[0;32m/tmp/ipykernel_142180/2363524555.py\u001b[0m(72)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     70 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"candidate_tokens requires_grad: {candidate_tokens.requires_grad}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Should print True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Candidate Tokens Grad:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 72 \u001b[0;31m                \u001b[0mnew_seqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutate_seq_with_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_response_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrucial_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     73 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m                \u001b[0;31m# Store the loss value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "tensor([[7483., 2524.,  346.,  610.,  275., 7483., 2524.,  346., 2524.,  346.,\n",
      "          610.,  275.,  610.,  275.,  396.,   13.,   13.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "*** SyntaxError: invalid syntax\n",
      "tensor([[7483., 2524.,  346.,  610.,  275., 7483., 2524.,  346., 2524.,  346.,\n",
      "          610.,  275.,  610.,  275.,  396.,   13.,   13.]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "None\n",
      "*** AttributeError: 'Tensor' object has no attribute 'require_grad'. Did you mean: 'requires_grad'?\n",
      "*** SyntaxError: invalid syntax\n",
      "True\n",
      "True\n",
      "> \u001b[0;32m/tmp/ipykernel_142180/1996318517.py\u001b[0m(203)\u001b[0;36mmutate_seq_with_gradient\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    201 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    202 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 203 \u001b[0;31m    \u001b[0mseq_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    204 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"seq_embeddings.requires_grad: {seq_embeddings.requires_grad}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    205 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1;32m    198 \u001b[0m    \u001b[0mseq_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    199 \u001b[0m    \u001b[0;31m# Get the embedding of the sequence tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    200 \u001b[0m    \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREADER_LLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    201 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    202 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 203 \u001b[0;31m    \u001b[0mseq_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    204 \u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"seq_embeddings.requires_grad: {seq_embeddings.requires_grad}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    205 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    206 \u001b[0m    \u001b[0;31m#seq_embeddings = REMOVED_SECRET()(seq_tokens.long())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    207 \u001b[0m    \u001b[0mseq_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Retain gradients for the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    208 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "*** NameError: name 'ls' is not defined\n",
      "Embedding(32064, 3072, padding_idx=32000)\n",
      "None\n",
      "*** SyntaxError: invalid syntax\n",
      "None\n",
      "*** NameError: name 'seq_embeddings' is not defined\n",
      "*** AttributeError: 'NoneType' object has no attribute 'grad'\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
