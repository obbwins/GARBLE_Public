{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "cb856855d9824f4bada3cc65e332967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Vocabulary size: 50257\n",
      "ERROR: Could not find file /tmp/ipykernel_865858/1170920067.py\n",
      "Processing page 1 of 1\n",
      "Processing page 1 of 1\n",
      "Injection complete!\n",
      "ERROR: Could not find file /tmp/ipykernel_865858/1170920067.py\n",
      "Type of query_based_on_pdf: <class 'str'>\n",
      "Type of question: <class 'str'>\n",
      "Type of knowledge_index: <class 'REMOVED_SECRET.FAISS'>\n",
      "Type of llm: <class 'rag_for_notebook_sunday.CustomTextGenerationPipeline'>\n",
      "Type of reranker: <class 'REMOVED_SECRET'>\n",
      "=> Retrieving documents...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 332\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m#query RAG system and get logits\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of query_based_on_pdf: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(query_based_on_pdf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m answer, relevant_docs, logits \u001b[38;5;241m=\u001b[39m \u001b[43mquery_rag_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_based_on_pdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Candidate response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Calculate weighted loss\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m, in \u001b[0;36mquery_rag_system\u001b[0;34m(question, llm, knowledge_index, reranker)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# query the RAG system and get the answer, relevant docs, and logits\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     answer, relevant_docs, logits \u001b[38;5;241m=\u001b[39m \u001b[43manswer_with_rag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mknowledge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mknowledge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer, relevant_docs, logits\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/codes/langers/rag_for_notebook_sunday.py:122\u001b[0m, in \u001b[0;36manswer_with_rag\u001b[0;34m(question, llm, knowledge_index, reranker, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer_with_rag\u001b[39m(\n\u001b[1;32m    113\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    114\u001b[0m     llm: CustomTextGenerationPipeline,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, List[LangchainDocument], torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Gather documents with retriever\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> Retrieving documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m     relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mknowledge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(relevant_docs))\n\u001b[1;32m    124\u001b[0m     relevant_docs \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m relevant_docs] \u001b[38;5;66;03m# Keep only the text\u001b[39;00m\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:402\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    404\u001b[0m         embedding,\n\u001b[1;32m    405\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:154\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:113\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    105\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:94\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), texts))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[0;32m---> 94\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_multi_process_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mencode_multi_process(texts, pool)\n\u001b[1;32m     96\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n",
      "File \u001b[0;32m~/codes/langers/.venv/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:429\u001b[0m, in \u001b[0;36mSentenceTransformer.start_multi_process_pool\u001b[0;34m(self, target_devices)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m device_id \u001b[38;5;129;01min\u001b[39;00m target_devices:\n\u001b[1;32m    424\u001b[0m     p \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mProcess(\n\u001b[1;32m    425\u001b[0m         target\u001b[38;5;241m=\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39m_encode_multi_process_worker,\n\u001b[1;32m    426\u001b[0m         args\u001b[38;5;241m=\u001b[39m(device_id, \u001b[38;5;28mself\u001b[39m, input_queue, output_queue),\n\u001b[1;32m    427\u001b[0m         daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    428\u001b[0m     )\n\u001b[0;32m--> 429\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     processes\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_queue, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_queue, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m: processes}\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import fitz\n",
    "import tiktoken\n",
    "import torch\n",
    "import REMOVED_SECRET as F\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "from tkinter import filedialog\n",
    "import tkinter as tk\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from memory_profiler import profile\n",
    "from rag_for_notebook_sunday import READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RERANKER, CustomTextGenerationPipeline, answer_with_rag, docs_processed\n",
    "#test comment from laptop\n",
    "embedding_model = SentenceTransformer('thenlper/gte-small')\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map='cuda', trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\") #openai embedding model (used for vocab)\n",
    "%load_ext memory_profiler\n",
    "vocab_size = 50257 #random but large guess at vocab size\n",
    "vocab_list = []\n",
    "\n",
    "# iterate through a range of possible token indices\n",
    "for token_id in range(vocab_size):\n",
    "    try:\n",
    "        #decode the token_id to get the corresponding token string\n",
    "        token = encoding.decode([token_id])\n",
    "        # add the token to the vocabulary list\n",
    "        vocab_list.append(token)\n",
    "    except KeyError:\n",
    "        #if decoding fails, it's likely an out-of-vocabulary token or a special token\n",
    "        \n",
    "        pass\n",
    "\n",
    "print(f\"Estimated Vocabulary size: {len(vocab_list)}\")\n",
    "#^ get vocabulary from tiktoken (openai)\n",
    "\n",
    "\n",
    "\n",
    "def query_rag_system(question, llm=READER_LLM, knowledge_index = KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER):\n",
    "    \"\"\"\n",
    "    Queries the RAG system with the given question.\n",
    "\n",
    "    Args:\n",
    "        question: The question to ask the RAG system.\n",
    "        llm: The language model to use for answer generation (default: READER_LLM).\n",
    "        knowledge_index: The vector store containing the document embeddings (default: KNOWLEDGE_VECTOR_DATABASE).\n",
    "        reranker: The reranker model (optional, default: RERANKER).\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - answer: The generated answer from the RAG system.\n",
    "            - relevant_docs: The list of retrieved relevant documents.\n",
    "            - logits: The logits of the generated answer.\n",
    "    \"\"\"\n",
    "    print(f\"Type of question: {type(question)}\")\n",
    "    print(f\"Type of knowledge_index: {type(knowledge_index)}\")\n",
    "    print(f\"Type of llm: {type(llm)}\")\n",
    "    print(f\"Type of reranker: {type(reranker)}\")\n",
    "    # query the RAG system and get the answer, relevant docs, and logits\n",
    "    try:\n",
    "        answer, relevant_docs, logits = answer_with_rag(\n",
    "            question=question,\n",
    "            llm=llm,\n",
    "            knowledge_index=knowledge_index,\n",
    "            reranker=reranker\n",
    "        )\n",
    "\n",
    "        return answer, relevant_docs, logits\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError in answer_with_rag: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def find_strongest_keyword(keywords, chunk_text):\n",
    "    \"\"\"\n",
    "    Finds the keyword with the highest average semantic similarity to the chunk's context.\n",
    "    \"\"\"\n",
    "\n",
    "    # encode the chunk and keywords using the embedding model\n",
    "    chunk_embedding = embedding_model.encode(chunk_text, convert_to_tensor=True)\n",
    "    keyword_embeddings = [embedding_model.encode(kw, convert_to_tensor=True) for kw in keywords]\n",
    "\n",
    "    # calculate cosine similarity between each keyword and the chunk\n",
    "    keyword_similarities = {kw: 0 for kw in keywords}  # Initialize similarities\n",
    "    for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "        if kw in chunk_text:  # Check if the keyword is present in the chunk\n",
    "            similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "            keyword_similarities[kw] = similarity\n",
    "\n",
    "    # Find the keyword with the highest similarity\n",
    "    if keyword_similarities:\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get)\n",
    "        return strongest_keyword\n",
    "    else:\n",
    "        return None   # No keywords found in the chunk\n",
    "\n",
    "\n",
    "def inject_text_into_pdf(input_pdf_path, output_pdf_path, text_to_inject, keywords_list, docs_processed):\n",
    "    \"\"\"\n",
    "    Injects adversarial text into the selected PDF.\n",
    "\n",
    "    Args:\n",
    "    input_pdf_path: selected PDF path from browse_for_pdf function\n",
    "    output_pdf_path: desired location of new PDF\n",
    "    text_to_inject: the adversarial sequence to be injected\n",
    "    keywords_list: a list of keywords from the selected PDF to push into find_strongest_keyword\n",
    "    docs_processed: embedded processed documents (one pdf can be multiple docs)\n",
    "    \n",
    "    \"\"\"\n",
    "    pdf_document = fitz.open(input_pdf_path)\n",
    "\n",
    "    # Create the zero-width version of the injected word\n",
    "    zero_width_inject_word = \"\\u200B\".join(list(text_to_inject))\n",
    "\n",
    "    for doc in docs_processed:\n",
    "       # page_num = doc.metadata['page'] \n",
    "        page_num = 0\n",
    "        page = pdf_document[page_num] \n",
    "        original_text = page.get_text(\"text\")\n",
    "\n",
    "        # Find keywords within this chunk\n",
    "        chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "\n",
    "        if chunk_keywords:\n",
    "            # Find the keyword with the highest semantic strength (you'll need to implement this)\n",
    "            strongest_keyword = find_strongest_keyword(chunk_keywords, doc.page_content) \n",
    "\n",
    "            # Inject before the strongest keyword\n",
    "            new_text = original_text.replace(strongest_keyword, f\"{zero_width_inject_word}{strongest_keyword}\")\n",
    "\n",
    "            page.clean_contents()\n",
    "            page.insert_text((0, 0), new_text, fontsize=12)\n",
    "\n",
    "        print(f\"Processing page {page_num + 1} of {len(pdf_document)}\")\n",
    "\n",
    "    pdf_document.save(output_pdf_path)\n",
    "    pdf_document.close()\n",
    "    print(\"Injection complete!\")\n",
    "\n",
    "@profile\n",
    "def extract_keywords_from_pdf(pdf_path, num_keywords=50):\n",
    "\n",
    "    keywords_list = []\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        document = loader.load()[0]\n",
    "        kw_model = KeyBERT()\n",
    "        keywords = kw_model.extract_keywords(document.page_content, keyphrase_ngram_range=(1, 3), top_n=num_keywords)\n",
    "        keywords_list = [keyword for keyword, score in keywords]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "    return keywords_list\n",
    "\n",
    "def browse_for_pdf():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    return file_path\n",
    "\n",
    "def weighted_loss(logits, t_res, crucial_indices, weight=0.5):\n",
    "    \"\"\"\n",
    "    Calculates the weighted loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    #logits = torch.cat(logits, dim=0) \n",
    "    print(\"Logits in beginning:\", logits.shape)\n",
    "    print(\"T_Res shape:\", t_res.shape)\n",
    "    #slice logits to match length of t_res\n",
    "    #logits = logits[:, :t_res.shape[0], :]\n",
    "\n",
    "    generated_sqnc_length = logits.shape[1] // vocab_size # Divide by vocab size to get actual sequence length\n",
    "    target_sqnc_length = t_res.shape[0]\n",
    "\n",
    "    if generated_sqnc_length < target_sqnc_length:\n",
    "        padding_length = (target_sqnc_length - generated_sqnc_length) * vocab_size\n",
    "        padding = torch.zeros((1, padding_length), device=logits.device)\n",
    "        logits = torch.cat([logits, padding], dim=1)\n",
    "\n",
    "    elif generated_sqnc_length > target_sqnc_length:\n",
    "        print(\"Logits Shape after length greater\", logits.shape)\n",
    "        logits = logits[:, :target_sqnc_length * vocab_size]\n",
    "\n",
    "    remainder = logits.shape[1] % vocab_size\n",
    "    if remainder != 0:\n",
    "        logits = logits[:, :-remainder]\n",
    "        \n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    t_res = t_res.view(-1)\n",
    "    print(\"Logits after reshape\", logits.shape)\n",
    "    loss = F.cross_entropy(logits, t_res)\n",
    "\n",
    "    crucial_logits = logits[crucial_indices]\n",
    "    print(crucial_logits.shape)\n",
    "    crucial_t_res = t_res[crucial_indices]\n",
    "    print(crucial_t_res.shape)\n",
    "    crucial_loss = F.cross_entropy(crucial_logits, crucial_t_res)\n",
    "    weighted_loss = loss * (1 - weight) + crucial_loss * weight\n",
    "    return weighted_loss\n",
    "\n",
    "def mutate_seq_with_gradient(seq_tokens, logits, target_response_tokens, crucial_indices, weight=0.8, k=32, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Mutates the sequence based on the gradient of the weighted loss.\n",
    "\n",
    "    Args:\n",
    "        seq_tokens: The current attack sequence (tokenized).\n",
    "        logits: The raw output of the LLM before the final softmax layer.\n",
    "        target_response_tokens: The targeted malicious response (tokenized).\n",
    "        crucial_indices: Indices of the crucial tokens in the target response.\n",
    "        weight: The weight assigned to the crucial loss component.\n",
    "        k: The number of new sequences to generate.\n",
    "        learning_rate: The learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "        A list of k mutated sequences.\n",
    "    \"\"\"\n",
    "    seq_embeddings = REMOVED_SECRET()(seq_tokens) \n",
    "    seq_embeddings.retain_grad()\n",
    "    # Calculate the weighted loss and its gradient\n",
    "    loss = weighted_loss(logits, target_response_tokens, crucial_indices, weight)\n",
    "    loss.backward() \n",
    "\n",
    "    # Get the gradient with respect to the embedded input sequence\n",
    "    \n",
    "\n",
    "    #grad = REMOVED_SECRET.clone()\n",
    "    grad = seq_embeddings.grad\n",
    "\n",
    "    if grad is not None:\n",
    "        grad = REMOVED_SECRET()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Gradient computation failed; grad is None\") \n",
    "\n",
    "    new_seqs = []\n",
    "    for _ in range(k):\n",
    "        # Randomly select a token to mutate\n",
    "        mutate_index = torch.randint(0, len(seq_tokens[0]), (1,)).item()\n",
    "\n",
    "        # Mutate the token's embedding based on the gradient\n",
    "        mutated_embedding = seq_embeddings[0][mutate_index] - learning_rate * grad[0][mutate_index]\n",
    "\n",
    "        # Find the closest token in the vocabulary to the mutated embedding\n",
    "        distances = torch.norm(REMOVED_SECRET().weight.data - mutated_embedding, dim=1)\n",
    "        closest_token_id = torch.argmin(distances).item()\n",
    "\n",
    "        # Create a new sequence with the mutated token\n",
    "        new_seq = seq_tokens.clone()\n",
    "        new_seq[0][mutate_index] = closest_token_id\n",
    "        new_seqs.append(new_seq)\n",
    "\n",
    "    # Zero out the gradients for the next iteration\n",
    "    REMOVED_SECRET()\n",
    "\n",
    "    return new_seqs\n",
    "\n",
    "\n",
    "def choose_best_sequence(new_seqs, losses):\n",
    "    \"\"\"\n",
    "    Chooses the best sequence from the mutated candidates based on their pre-calculated losses.\n",
    "\n",
    "    Args:\n",
    "        new_seqs: A list of mutated sequences (tokenized).\n",
    "        losses: A list of corresponding loss values for each sequence.\n",
    "\n",
    "    Returns:\n",
    "        The best sequence (tokenized) with the lowest loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_index = np.argmin(losses)\n",
    "    return new_seqs[best_index]\n",
    "\n",
    "# Main Workflow\n",
    "pdf_file_path = browse_for_pdf()\n",
    "\n",
    "#\n",
    "if pdf_file_path:\n",
    "\n",
    "    keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "\n",
    "    if keyword_results:\n",
    "        best_keywords = keyword_results[:5]\n",
    "        initial_sub_document = best_keywords\n",
    "        initial_vocab = vocab_list\n",
    "        token_vocabulary = initial_vocab + keyword_results\n",
    "        target_response = \"I'm sorry, I can't help with that.\"\n",
    "        query_based_on_pdf = \"What is the capital of France?\"\n",
    "\n",
    "        T = 5  \n",
    "        B = 3  \n",
    "\n",
    "        target_response_tokens = tokenizer.encode(target_response, return_tensors='pt').squeeze().to(model.device)\n",
    "        crucial_phrase = \"I cannot provide an answer\"\n",
    "        crucial_indices = [i for i, token in enumerate(target_response_tokens) if tokenizer.decode([token]) in crucial_phrase.split()]\n",
    "\n",
    "        sub_document = initial_sub_document.copy()\n",
    "        rag_database_folder = \"local_database\"\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        for i in range(T):\n",
    "            l = random.randint(0, len(sub_document) - 1)\n",
    "            candidate_sub_documents = []\n",
    "            losses = []\n",
    "\n",
    "            for b in range(B):\n",
    "                new_token = random.choice(token_vocabulary)\n",
    "                candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "                \n",
    "                # Inject candidate into the PDF at the strongest keyword locations\n",
    "                output_pdf_path = REMOVED_SECRET(rag_database_folder, f\"updated_pdf_{i}_{b}.pdf\")\n",
    "                inject_text_into_pdf(pdf_file_path, output_pdf_path, ' '.join(candidate), keyword_results, docs_processed)\n",
    "\n",
    "                #re-load modified pdf and update keywords & vocab\n",
    "                pdf_file_path = output_pdf_path\n",
    "                keyword_results = extract_keywords_from_pdf(pdf_file_path)\n",
    "                token_vocabulary = initial_vocab + keyword_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                #query RAG system and get logits\n",
    "                print(f\"Type of query_based_on_pdf: {type(query_based_on_pdf)}\")\n",
    "\n",
    "                answer, relevant_docs, logits = query_rag_system(query_based_on_pdf)\n",
    "                print(f\"Iteration {i+1}/{T}, Batch {b+1}/{B}: Candidate response: {answer}\")\n",
    "\n",
    "                # Calculate weighted loss\n",
    "                loss = weighted_loss(logits, target_response_tokens, crucial_indices)\n",
    "                print(f\"Loss: {loss:.4f}\")\n",
    "                \n",
    "\n",
    "                # Mutate the sequence based on the gradient and loss\n",
    "                candidate_tokens = tokenizer.encode(' '.join(candidate))\n",
    "                candidate_tokens = torch.tensor([candidate_tokens]).to(model.device)\n",
    "                new_seqs = mutate_seq_with_gradient(candidate_tokens, logits, target_response_tokens, crucial_indices)\n",
    "                 # Store the loss value\n",
    "                losses.append(loss.item())\n",
    "                \n",
    "                best_seq = choose_best_sequence(new_seqs) \n",
    "\n",
    "                # Update candidate based on the best sequence\n",
    "                candidate = tokenizer.decode(best_seq[0], skip_special_tokens=True).split()\n",
    "                candidate_sub_documents.append(candidate)\n",
    "               \n",
    "\n",
    "            # Select the best candidate based on loss (lower loss is better)\n",
    "            best_candidate_index = np.argmin(losses)\n",
    "            sub_document = candidate_sub_documents[best_candidate_index]\n",
    "            print(f\"Iteration {i+1}/{T}: Best candidate sub-document: {' '.join(sub_document)} (Loss: {losses[best_candidate_index]:.4f})\")\n",
    "            # Early stopping if loss is below a threshold (optional)\n",
    "            if losses[best_candidate_index] < 0.4:  # Adjust the threshold as needed\n",
    "                    break\n",
    "\n",
    "        final_sub_document_text = ' '.join(sub_document)\n",
    "        print(f\"Final optimized sub-document: {final_sub_document_text}\")\n",
    "\n",
    "        final_response_file = \"final_response.txt\"\n",
    "        with open(final_response_file, \"w\") as f:\n",
    "            f.write(final_sub_document_text)\n",
    "        print(f\"Final response saved to {final_response_file}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No keywords extracted from the PDF.\")\n",
    "else:\n",
    "    print(\"No file selected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
