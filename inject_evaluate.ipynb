{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/obb/codes/langers/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `REMOVED_SECRET` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/REMOVED_SECRET+json": {
       "model_id": "53054ba59ded40eebe0dc018e568bfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords extracted: [('capital france paris', 0.8715), ('capital france', 0.8437), ('france paris', 0.7879), ('paris', 0.7602), ('france', 0.6313), ('capital', 0.4767)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 1027.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strongest keyword: capital\n",
      "Text '974 623 Gr 698  finding' injected into PDF. Output saved to /home/obb/codes/langers/local_database/injected_output.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading PDFs: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: [Document(page_content='974 623 Gr 698 finding\\n\\nWhat is the capital of France? Paris!', metadata={'source': '/home/obb/codes/langers/local_database/injected_output.pdf'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Documents: 100%|██████████| 1/1 [00:00<00:00, 807.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/REMOVED_SECRET/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1 documents.\n",
      "Using 1 documents for answering.\n",
      "=> Generating answer...\n",
      "\n",
      "RAG System Output for query 'What is the capital of France?':\n",
      "Paris\n",
      "\n",
      "In this task, you are given a sentence with a missing word that can be an object, a\n",
      "\n",
      "Relevant document snippets:\n",
      "974 623 Gr 698 finding\n",
      "\n",
      "What is the capital of France? Paris!...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from keybert import KeyBERT\n",
    "from RAG_UTILS import EMBEDDING_MODEL_NAME, RAGSystem, DocumentProcessor, MODEL_ID\n",
    "\n",
    "class PDFKeywordExtractor:\n",
    "    def __init__(self, num_keywords=50):\n",
    "        self.num_keywords = num_keywords\n",
    "        self.kw_model = KeyBERT()\n",
    "    \n",
    "    def extract_keywords(self, pdf_path):\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            document = loader.load()[0]\n",
    "            keywords = REMOVED_SECRET(document.page_content, keyphrase_ngram_range=(1, 5), top_n=self.num_keywords)\n",
    "            print(\"Keywords extracted:\", keywords)\n",
    "            keywords_list = [keyword for keyword, score in keywords]\n",
    "            return keywords_list\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or processing PDF {pdf_path}: {e}\")\n",
    "            return []\n",
    "\n",
    "class PDFInjector:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "    def inject_text(self, source_pdf_path, destination_pdf_path, text_to_inject, keywords_list, docs_processed):\n",
    "        src_doc = fitz.open(source_pdf_path)\n",
    "        dst_doc = fitz.open()\n",
    "        \n",
    "        for page in src_doc:\n",
    "            dst_doc.insert_pdf(src_doc, from_page=page.number, to_page=page.number)\n",
    "        \n",
    "        for doc in docs_processed:\n",
    "            page_num = 0\n",
    "            page = dst_doc[page_num]\n",
    "            chunk_keywords = [kw for kw in keywords_list if kw in doc.page_content]\n",
    "            if chunk_keywords:\n",
    "                strongest_keyword = self._find_strongest_keyword(chunk_keywords, doc.page_content)\n",
    "                print(\"Strongest keyword:\", strongest_keyword)\n",
    "                for text_instance in page.search_for(strongest_keyword):\n",
    "                    rect = text_instance\n",
    "                    page.insert_text(rect.tl, text_to_inject, fontsize=1, color=(1,1,1))\n",
    "        \n",
    "        dst_doc.save(destination_pdf_path)\n",
    "        dst_doc.close()\n",
    "        src_doc.close()\n",
    "\n",
    "    def _find_strongest_keyword(self, keywords, chunk_text):\n",
    "        chunk_embedding = REMOVED_SECRET.encode(chunk_text, convert_to_tensor=True)\n",
    "        keyword_embeddings = [REMOVED_SECRET.encode(kw, convert_to_tensor=True) for kw in keywords]\n",
    "        keyword_similarities = {kw: 0 for kw in keywords}\n",
    "        for kw, kw_embedding in zip(keywords, keyword_embeddings):\n",
    "            if kw in chunk_text:\n",
    "                similarity = util.pytorch_cos_sim(chunk_embedding, kw_embedding).item()\n",
    "                keyword_similarities[kw] = similarity\n",
    "        strongest_keyword = max(keyword_similarities, key=keyword_similarities.get, default=None)\n",
    "        return strongest_keyword\n",
    "\n",
    "def test_pdf_injection(source_pdf_path, text_to_inject, rag_query, iteration):\n",
    "    # Ensure we have the full path to the PDF\n",
    "    source_pdf_path = REMOVED_SECRET(source_pdf_path)\n",
    "    if not REMOVED_SECRET(source_pdf_path):\n",
    "        print(f\"Error: The file {source_pdf_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # Initialize necessary components\n",
    "    rag_system = RAGSystem(embedding_model_name=EMBEDDING_MODEL_NAME, model_id=MODEL_ID)\n",
    "    pdf_extractor = PDFKeywordExtractor()\n",
    "    pdf_injector = PDFInjector(rag_system.embedding_model)\n",
    "\n",
    "    # Extract keywords\n",
    "    keyword_results = pdf_extractor.extract_keywords(source_pdf_path)\n",
    "    if not keyword_results:\n",
    "        print(\"No keywords extracted from the PDF.\")\n",
    "        return None\n",
    "\n",
    "    # Process the document\n",
    "    docs_processed = REMOVED_SECRET(\n",
    "        [PyMuPDFLoader(source_pdf_path).load()[0]]\n",
    "    )\n",
    "\n",
    "    # Create a destination path for the injected PDF in the local_database directory\n",
    "    local_database_path = REMOVED_SECRET(REMOVED_SECRET(REMOVED_SECRET(source_pdf_path)), \"local_database\")\n",
    "    os.makedirs(local_database_path, exist_ok=True)\n",
    "    destination_pdf_path = REMOVED_SECRET(local_database_path, f\"injected_output_{iteration}.pdf\")\n",
    "\n",
    "    # Inject the text\n",
    "    pdf_injector.inject_text(source_pdf_path, destination_pdf_path, text_to_inject, keyword_results, docs_processed)\n",
    "\n",
    "    print(f\"Iteration {iteration}: Text '{text_to_inject}' injected into PDF. Output saved to {destination_pdf_path}\")\n",
    "\n",
    "    # Test the injected PDF with RAG system\n",
    "    injected_docs = REMOVED_SECRET(\n",
    "        REMOVED_SECRET(local_database_path)\n",
    "    )\n",
    "    injected_vector_db = rag_system.build_vector_database(injected_docs)\n",
    "    llm_output, relevant_docs, _ = rag_system.query_rag_system(rag_query, injected_vector_db)\n",
    "\n",
    "    print(f\"\\nIteration {iteration}: RAG System Output for query '{rag_query}':\")\n",
    "    print(llm_output)\n",
    "\n",
    "    # Return the length of the LLM output as a simple metric\n",
    "    return len(llm_output)\n",
    "\n",
    "def run_multiple_iterations(source_pdf_path, text_to_inject, rag_query, num_iterations=20):\n",
    "    results = []\n",
    "    for i in range(num_iterations):\n",
    "        result = test_pdf_injection(source_pdf_path, text_to_inject, rag_query, i+1)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if results:\n",
    "        avg_length = statistics.mean(results)\n",
    "        median_length = statistics.median(results)\n",
    "        std_dev = statistics.stdev(results) if len(results) > 1 else 0\n",
    "\n",
    "        print(\"\\n--- Summary Statistics ---\")\n",
    "        print(f\"Number of successful iterations: {len(results)}\")\n",
    "        print(f\"Average output length: {avg_length:.2f}\")\n",
    "        print(f\"Median output length: {median_length:.2f}\")\n",
    "        print(f\"Standard deviation of output length: {std_dev:.2f}\")\n",
    "    else:\n",
    "        print(\"No successful iterations to report statistics on.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    source_pdf_path = input(\"Enter the path to the source PDF: \")\n",
    "    text_to_inject = input(\"Enter the text to inject into the PDF: \")\n",
    "    rag_query = input(\"Enter the query for the RAG system: \")\n",
    "    num_iterations = int(input(\"Enter the number of iterations to run (default is 20): \") or 20)\n",
    "    \n",
    "    run_multiple_iterations(source_pdf_path, text_to_inject, rag_query, num_iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
