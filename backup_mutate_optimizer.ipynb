{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def optimize(self, rag_system, pdf_injector, pdf_manager, temp_pdf_path, initial_sequence, keyword_results, \n",
    "                 token_vocabulary, target_response_tokens, crucial_indices, query_based_on_pdf, docs_processed,\n",
    "                 population_size, num_generations):\n",
    "        \n",
    "     \n",
    "        creator.create(\"FitnessMulti\", base.Fitness, weights=(-1.0, 1.0, 1.0))  # Minimize loss, maximize coherence and relevance\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "        toolbox = base.Toolbox()\n",
    "       \n",
    "        # Parallel processing\n",
    "        #pool = Pool()\n",
    "        #toolbox.register(\"map\", pool.map)\n",
    "\n",
    "        # Define how to create an individual and the population\n",
    "        toolbox.register(\"attr_str\", random.choice, token_vocabulary)\n",
    "        toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_str, n=len(initial_sequence))\n",
    "        toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "        # Define the genetic operators\n",
    "        toolbox.register(\"evaluate\", self.evaluate_sequence, rag_system=rag_system, pdf_injector=pdf_injector,\n",
    "                         pdf_manager=pdf_manager, temp_pdf_path=temp_pdf_path, keyword_results=keyword_results, \n",
    "                         target_response_tokens=target_response_tokens, crucial_indices=crucial_indices, \n",
    "                         query_based_on_pdf=query_based_on_pdf, docs_processed=docs_processed)\n",
    "        toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "        toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.05)\n",
    "        toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "        # Create initial population\n",
    "        population = toolbox.population(n=population_size)\n",
    "        print(f\"Initial population size: {len(population)}\")\n",
    "\n",
    "        # Add the initial sequence to the population\n",
    "        initial_individual = creator.Individual(initial_sequence)\n",
    "        print(f\"Evaluating initial individual: {' '.join(initial_individual)}\")\n",
    "        initial_fitness = toolbox.evaluate(initial_individual)\n",
    "        print(f\"Initial individual fitness: {initial_fitness}\")\n",
    "        if not isinstance(initial_fitness, tuple) or len(initial_fitness) == 0:\n",
    "            raise ValueError(\"Invalid fitness returned from evaluation function\")\n",
    "        REMOVED_SECRET = initial_fitness\n",
    "        population.append(initial_individual)\n",
    "\n",
    "        # Define adaptive mutation and crossover rates\n",
    "        cxpb, mutpb = 0.5, 0.2\n",
    "        \n",
    "        # Run the NSGA-II algorithm with early stopping\n",
    "        best_fitness = float('inf')\n",
    "        generations_no_improve = 0\n",
    "        for gen in range(num_generations):\n",
    "            print(f\"\\nStarting Generation {gen}\")\n",
    "            # Vary the population\n",
    "            offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n",
    "\n",
    "            # Evaluate the individuals with an invalid fitness\n",
    "            invalid_ind = [ind for ind in offspring if not REMOVED_SECRET]\n",
    "            print(f\"Number of individuals to evaluate: {len(invalid_ind)}\")\n",
    "            fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                if not isinstance(fit, tuple) or len(fit) == 0:\n",
    "                    print(f\"Invalid fitness: {fit}\")\n",
    "                    print(f\"Warning: Invalid fitness returned for individual {ind}: {fit}\")\n",
    "                    continue\n",
    "                REMOVED_SECRET = fit\n",
    "\n",
    "            # Select the next generation population\n",
    "            population = toolbox.select(population + offspring, k=len(population))\n",
    "\n",
    "            if not population:\n",
    "                raise ValueError(\"Population is empty after selection\")\n",
    "            \n",
    "            valid_individuals = [ind for ind in population if REMOVED_SECRET]\n",
    "            print(f\"Population size after selection: {len(population)}\")\n",
    "            print(f\"Number of valid individuals: {len(valid_individuals)}\")\n",
    "            if not valid_individuals:\n",
    "                raise ValueError(\"no valid individuals in population\")\n",
    "            \n",
    "            \n",
    "            current_best = min(REMOVED_SECRET[0] for ind in valid_individuals)\n",
    "            print(f\"Best fitness in generation {gen}: {current_best}\")\n",
    "            REMOVED_SECRET({\n",
    "                'generation': gen,\n",
    "                'population': population.copy(),\n",
    "                'best_fitness':current_best\n",
    "            })\n",
    "\n",
    "            print(f\"Generation {gen} complete: Best Loss = {current_best}\")\n",
    "\n",
    "            # Check for improvement\n",
    "            if current_best < best_fitness:\n",
    "                best_fitness = current_best\n",
    "                generations_no_improve = 0\n",
    "            else:\n",
    "                generations_no_improve += 1\n",
    "\n",
    "            # Update adaptive rates\n",
    "            #if gen % 10 == 0:\n",
    "                #cxpb = max(0.1, cxpb * (1 + (best_fitness - min(REMOVED_SECRET[0] for ind in population)) / best_fitness))\n",
    "                #mutpb = max(0.1, mutpb * (1 + (best_fitness - min(REMOVED_SECRET[0] for ind in population)) / best_fitness))\n",
    "\n",
    "         \n",
    "            \n",
    "\n",
    "          \n",
    "\n",
    "            # Early stopping\n",
    "            if generations_no_improve >= 20:\n",
    "                print(f\"Stopping early at generation {gen} due to no improvement\")\n",
    "                break\n",
    "\n",
    "        \n",
    "        # Select the best individual\n",
    "        best_individual = tools.selBest(valid_individuals, k=1)[0]\n",
    "        best_sequence = best_individual\n",
    "        best_loss = REMOVED_SECRET[0]\n",
    "\n",
    "        print(f\"Optimization complete. Best loss: {best_loss}\")\n",
    "        \n",
    "        #self.visualize_fitness_progression()\n",
    "        #pool.close()\n",
    "        return ' '.join(best_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Tuple\n",
    "import REMOVED_SECRET as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, GenerationConfig\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from REMOVED_SECRET import DistanceStrategy\n",
    "from REMOVED_SECRET import Document as LangchainDocument\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredPDFLoader\n",
    "from ragatouille import RAGPretrainedModel\n",
    "import multiprocessing\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# OPTIONS\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "torch.set_printoptions(threshold=None)\n",
    "\n",
    "\n",
    "# CONSTANTS\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "\n",
    "# Template for RAG Prompt\n",
    "# New Chat-Based Prompt Template\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"\n",
    "    \n",
    "        Handles loading and processing of PDFs.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str, chunk_size: int = 512):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def load_pdfs_from_folder(self, folder_path: str) -> list[LangchainDocument]:\n",
    "        \"\"\"\n",
    "        Loads all PDFs from the specified folder.\n",
    "\n",
    "        Args: folder_path (str): Path to the folder containing PDF files.\n",
    "\n",
    "        Returns: list[LangchainDocument]: List of loaded documents.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".pdf\")]\n",
    "        documents = []\n",
    "        for pdf_file in tqdm(pdf_files, desc=\"Loading PDFs\"):\n",
    "            pdf_path = REMOVED_SECRET(folder_path, pdf_file)\n",
    "            try:\n",
    "                if REMOVED_SECRET(pdf_path) == 0:\n",
    "                    print(f\"Skipping empty file: {pdf_file}\")\n",
    "                    continue #move to next file\n",
    "\n",
    "                loader = UnstructuredPDFLoader(pdf_path)\n",
    "                loaded_docs = loader.load()\n",
    "                \n",
    "                documents.extend(\n",
    "                    LangchainDocument(\n",
    "                        page_content=doc.page_content, metadata={\"source\": REMOVED_SECRET(\"source\", pdf_file)}\n",
    "                    )\n",
    "                    for doc in loaded_docs\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {pdf_file}: {e}\")\n",
    "        print(\"Documents:\", documents)\n",
    "        return documents\n",
    "    \n",
    "\n",
    "    def split_documents(self, knowledge_base: List[LangchainDocument]) -> List[LangchainDocument]:\n",
    "        \"\"\"\n",
    "        Splits documents into smaller chunks based on token count.\n",
    "\n",
    "        Args:\n",
    "            knowledge_base (List[LangchainDocument]): List of documents to split.\n",
    "\n",
    "        Returns:\n",
    "            List[LangchainDocument]: List of processed document chunks.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_name)\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "            tokenizer,\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=int(self.chunk_size / 10),\n",
    "            add_start_index=True,\n",
    "            strip_whitespace=True,\n",
    "        )\n",
    "\n",
    "        docs_processed = []\n",
    "        for doc in tqdm(knowledge_base, desc=\"Splitting Documents\"):\n",
    "            split_docs = text_splitter.split_documents([doc])\n",
    "            docs_processed.extend(split_docs)\n",
    "\n",
    "        # Remove duplicates\n",
    "        unique_texts = set()\n",
    "        docs_processed_unique = []\n",
    "        for doc in docs_processed:\n",
    "            if doc.page_content not in unique_texts:\n",
    "                unique_texts.add(doc.page_content)\n",
    "                docs_processed_unique.append(doc)\n",
    "\n",
    "        return docs_processed_unique\n",
    "    \n",
    "\n",
    "class CustomTextGenerationPipeline:\n",
    "    \"\"\"\n",
    "    Custom pipeline for text generation with access to logits and gradients.\n",
    "    \"\"\"\n",
    "#\"mps\" if REMOVED_SECRET.is_available() else\n",
    "    def __init__(self, model_id: str):\n",
    "        self.device = torch.device(\"cuda\" if REMOVED_SECRET() else \"cpu\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            torch_dtype=torch.float16 if REMOVED_SECRET() else torch.float32,\n",
    "            device_map='auto' if REMOVED_SECRET() else None,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation = 'flash_attention_2' if REMOVED_SECRET() else 'eager',\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        REMOVED_SECRET()\n",
    "\n",
    "    def get_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Retrieves input embeddings for the given input IDs.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Token IDs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Input embeddings.\n",
    "        \"\"\"\n",
    "        return REMOVED_SECRET()(input_ids)\n",
    "    \n",
    "    def generate_with_logits(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 50,\n",
    "        do_sample: bool = True,\n",
    "        temperature: float = 0.3,\n",
    "    ) -> Tuple[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generates text and returns both the generated text and logits.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt.\n",
    "            max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 50.\n",
    "            do_sample (bool, optional): Whether to use sampling. Defaults to True.\n",
    "            temperature (float, optional): Sampling temperature. Defaults to 0.3.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, torch.Tensor]: Generated text and logits tensor.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Set up generation configuration\n",
    "        gen_config = GenerationConfig(\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "        # Perform generation\n",
    "        with torch.no_grad():\n",
    "            outputs = REMOVED_SECRET(**inputs, generation_config=gen_config)\n",
    "\n",
    "        # Decode generated tokens\n",
    "        generated_sequence = outputs.sequences\n",
    "        \n",
    "        #generated_text = REMOVED_SECRET(self.generated_sequence, skip_special_tokens=True)\n",
    "        #print(\"Generated Sequence:\", generated_sequence)\n",
    "        #print(\"Generated text:\", generated_text)\n",
    "        # Extract logits\n",
    "        logits = torch.stack(outputs.scores, dim=1)  # Shape: (sequence_length, vocab_size)\n",
    "        #print(\"Logits\", logits)\n",
    "        return generated_sequence, logits\n",
    "    \n",
    "\n",
    "    def forward_pass(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass and returns logits.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): Input prompt.\n",
    "            labels (Optional[torch.Tensor], optional): Labels for computing loss. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits from the model.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        if labels is not None:\n",
    "            inputs[\"labels\"] = labels.to(self.device)\n",
    "\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Retrieval-Augmented Generation (RAG) system integrating document retrieval, optional reranking, and text generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str, model_id: str, reranker_model: Optional[str] = None):\n",
    "        self.document_processor = DocumentProcessor(embedding_model_name)\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,\n",
    "            multi_process=True,\n",
    "            encode_kwargs={\"normalize_embeddings\": True},\n",
    "        )\n",
    "        self.reader_llm = CustomTextGenerationPipeline(model_id)\n",
    "        self.reranker = RAGPretrainedModel.from_pretrained(reranker_model) if reranker_model else None\n",
    "\n",
    "\n",
    "    def build_vector_database(self, documents: List[LangchainDocument]) -> FAISS:\n",
    "        \"\"\"\n",
    "        Builds a FAISS vector database from the provided documents.\n",
    "\n",
    "        Args:\n",
    "            documents (List[LangchainDocument]): List of documents.\n",
    "\n",
    "        Returns:\n",
    "            FAISS: FAISS vector database.\n",
    "        \"\"\"\n",
    "        return FAISS.from_documents(\n",
    "            documents,\n",
    "            self.embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "    \n",
    "    def query_rag_system(self, question: str, knowledge_index: FAISS, num_retrieved_docs: int = 30, num_docs_final: int = 1) -> Tuple[str, List[str], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Queries the RAG system with the given question.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to ask.\n",
    "            knowledge_index (FAISS): The FAISS index to search.\n",
    "            num_retrieved_docs (int, optional): Number of documents to retrieve. Defaults to 30.\n",
    "            num_docs_final (int, optional): Number of documents to use after reranking. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List[str], torch.Tensor]: Generated answer, list of relevant documents, and logits tensor.\n",
    "        \"\"\"\n",
    "        return self.answer_with_rag(\n",
    "            question=question,\n",
    "            knowledge_index=knowledge_index,\n",
    "            num_retrieved_docs=num_retrieved_docs,\n",
    "            num_docs_final=num_docs_final\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_chat_prompt(context, question):\n",
    "            prompt = f\"\"\"Context:\n",
    "\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "    \n",
    "            Instructions: Using the information from the context, provide a concise and direct answer to the question. Do not repeat the question or the context. Just state the answer clearly and briefly.\n",
    "            \n",
    "            Answer:\n",
    "            \"\"\" \n",
    "            return prompt\n",
    "    def answer_with_rag(\n",
    "        self,\n",
    "        question: str,\n",
    "        knowledge_index: FAISS,\n",
    "        num_retrieved_docs: int = 30,\n",
    "        num_docs_final: int = 1,\n",
    "    ) -> Tuple[str, List[str], torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generates an answer to the given question using RAG.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to answer.\n",
    "            knowledge_index (FAISS): The FAISS vector database for retrieval.\n",
    "            num_retrieved_docs (int, optional): Number of documents to retrieve. Defaults to 30.\n",
    "            num_docs_final (int, optional): Number of documents to use after reranking. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, List[str], torch.Tensor]: Generated answer, list of relevant documents, and logits tensor.\n",
    "        \"\"\"\n",
    "        # Retrieve relevant documents\n",
    "        print(\"=> Retrieving documents...\")\n",
    "        relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "        relevant_texts = [doc.page_content for doc in relevant_docs]\n",
    "        print(f\"Retrieved {len(relevant_texts)} documents.\")\n",
    "\n",
    "        # Optional reranking\n",
    "        if self.reranker:\n",
    "            print(\"=> Reranking documents...\")\n",
    "            reranked = REMOVED_SECRET(question, relevant_texts, k=num_docs_final)\n",
    "            # Assume reranked is a list of dicts with 'content' key\n",
    "            relevant_texts = [doc[\"content\"] for doc in reranked]\n",
    "            print(f\"Reranked to {len(relevant_texts)} documents.\")\n",
    "\n",
    "        # Limit to the desired number of documents\n",
    "        relevant_texts = relevant_texts[:num_docs_final]\n",
    "        print(f\"Using {len(relevant_texts)} documents for answering.\")\n",
    "\n",
    "        # Construct context\n",
    "        context = \"\\n\".join([f\"Document {i}:\\n{doc}\" for i, doc in enumerate(relevant_texts, 1)])\n",
    "\n",
    "        # Create the final prompt\n",
    "        final_prompt = self.generate_chat_prompt(context=context, question=question)\n",
    "\n",
    "        \n",
    "\n",
    "        # Generate answer with logits\n",
    "        print(\"=> Generating answer...\")\n",
    "        generated_sequence, logits = REMOVED_SECRET(\n",
    "            prompt=final_prompt,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        # Post-processing: Extract the answer part (optional based on how the model responds)\n",
    "        # This example assumes the model generates the answer directly after the prompt.\n",
    "        #answer = generated_text.split(\"Question:\")[0].strip()\n",
    "\n",
    "\n",
    "\n",
    "        prompt_tokens = REMOVED_SECRET.encode(final_prompt, return_tensors=\"pt\")[0]\n",
    "        #print(\"prompt tokens\", prompt_tokens)\n",
    "        #print(\"generated sequence\", REMOVED_SECRET)\n",
    "       \n",
    "        try:\n",
    "            prompt_end_index = (generated_sequence[0] == prompt_tokens[-1]).nonzero(as_tuple=True)[0][0].item() + 1\n",
    "        except IndexError:\n",
    "            prompt_end_index = 0\n",
    "\n",
    "\n",
    "        answer = REMOVED_SECRET.decode(generated_sequence[0][prompt_end_index:], skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "        endoftext_index = answer.find(\"<|endoftext|>\")\n",
    "        if endoftext_index != -1:\n",
    "            answer = answer[:endoftext_index].rstrip() \n",
    "        \n",
    "\n",
    "        return answer, relevant_texts, logits\n",
    "    \n",
    "\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"\n",
    "        Clears memory by deleting large objects and invoking garbage collection.\n",
    "        \"\"\"\n",
    "        del self.document_processor\n",
    "        del self.embedding_model\n",
    "        del self.reader_llm\n",
    "        if self.reranker:\n",
    "            del self.reranker\n",
    "        REMOVED_SECRET()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def main(question: str, pdf_folder_path: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Main function to execute the RAG pipeline.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer.\n",
    "        pdf_folder_path (str): Path to the folder containing PDF documents.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: Generated answer and list of relevant documents.\n",
    "    \"\"\"\n",
    "    rag_system = RAGSystem(\n",
    "        embedding_model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_id=MODEL_ID,\n",
    "        reranker_model=RERANKER_MODEL,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load and process documents\n",
    "        raw_documents = REMOVED_SECRET(pdf_folder_path)\n",
    "        processed_documents = REMOVED_SECRET(raw_documents)\n",
    "\n",
    "        # Build vector database\n",
    "        knowledge_index = rag_system.build_vector_database(processed_documents)\n",
    "\n",
    "        # Generate answer\n",
    "        answer, relevant_docs, _ = rag_system.answer_with_rag(question, knowledge_index)\n",
    "    finally:\n",
    "        # Ensure memory is cleared even if an error occurs\n",
    "        rag_system.clear_memory()\n",
    "\n",
    "    return answer, relevant_docs\n",
    "\n",
    "\n",
    "\n",
    "def generate_vocab_list(vocab_size=50257):\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "    vocab_list = []\n",
    "    for token_id in range(vocab_size):\n",
    "        try:\n",
    "            token = encoding.decode([token_id])\n",
    "            vocab_list.append(token)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vocab_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"What is the capital of France?\"\n",
    "\n",
    "    pdf_folder_path = \"local_database\" \n",
    "\n",
    "    answer, relevant_docs = main(question, pdf_folder_path)\n",
    "\n",
    "    print(\"==================================Answer==================================\")\n",
    "    print(answer)\n",
    "    print(\"==================================Source Docs==================================\")\n",
    "    for i, doc in enumerate(relevant_docs, 1):\n",
    "        print(f\"Document {i}------------------------------------------------------------\")\n",
    "        print(doc)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
