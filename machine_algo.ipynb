{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10: Best candidate sub-document: he lo\n",
      "Iteration 2/10: Best candidate sub-document: he lo\n",
      "Iteration 3/10: Best candidate sub-document: he so\n",
      "Iteration 4/10: Best candidate sub-document: he so\n",
      "Iteration 5/10: Best candidate sub-document: he to\n",
      "Iteration 6/10: Best candidate sub-document: he to\n",
      "Iteration 7/10: Best candidate sub-document: h  to\n",
      "Iteration 8/10: Best candidate sub-document: h oto\n",
      "Iteration 9/10: Best candidate sub-document: h oto\n",
      "Iteration 10/10: Best candidate sub-document: h ot \n",
      "\n",
      "Optimized sub-document: h ot \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "# Load pre-trained embedding model and tokenizer\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Simulated RAG system response function (simplified)\n",
    "def RAG_system(query: str, sub_document: str) -> str:\n",
    "    \"\"\"Simulate the RAG system returning a response based on the sub-document.\"\"\"\n",
    "    # In a real-world scenario, this would involve querying a trained RAG model\n",
    "    return f\"response_based_on_{sub_document}_for_{query}\"\n",
    "\n",
    "# Black-box optimization function\n",
    "def black_box_optimization(initial_sub_document: List[str], token_vocabulary: List[str], \n",
    "                           target_response: str, T: int, B: int) -> List[str]:\n",
    "    sub_document = initial_sub_document[:]\n",
    "\n",
    "    for i in range(T):\n",
    "        # Step 1: Sample an index l to replace\n",
    "        l = random.randint(0, len(sub_document) - 1)\n",
    "\n",
    "        candidate_sub_documents = []\n",
    "        similarities = []\n",
    "\n",
    "        for b in range(B):\n",
    "            # Step 2: Sample a new token\n",
    "            new_token = random.choice(token_vocabulary)\n",
    "\n",
    "            # Step 3: Replace the l-th token to create a candidate sub-document\n",
    "            candidate = sub_document[:l] + [new_token] + sub_document[l+1:]\n",
    "            candidate_sub_documents.append(candidate)\n",
    "\n",
    "            # Step 4: Query the RAG system and obtain a response\n",
    "            response = RAG_system(\"query\", ''.join(candidate))\n",
    "\n",
    "            # Step 5: Measure similarity with the target response using the embedding model\n",
    "            response_embedding = embedding_model.encode(response, convert_to_tensor=True)\n",
    "            target_embedding = embedding_model.encode(target_response, convert_to_tensor=True)\n",
    "            similarity = util.pytorch_cos_sim(response_embedding, target_embedding).item()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Step 6: Select the candidate with the highest similarity\n",
    "        best_candidate_index = similarities.index(max(similarities))\n",
    "        sub_document = candidate_sub_documents[best_candidate_index]\n",
    "\n",
    "        print(f\"Iteration {i+1}/{T}: Best candidate sub-document: {''.join(sub_document)}\")\n",
    "\n",
    "    return sub_document\n",
    "\n",
    "# Example usage\n",
    "initial_sub_document = [\"h\", \"e\", \"l\", \"l\", \"o\"]\n",
    "token_vocabulary = [\"h\", \"e\", \"l\", \"o\", \" \", \"r\", \"s\", \"p\", \"n\", \"t\"]  # Expanded vocabulary\n",
    "target_response = \"target_response_based_on_some_context\"\n",
    "T = 10  # Number of iterations\n",
    "B = 5   # Batch size\n",
    "\n",
    "optimized_sub_document = black_box_optimization(initial_sub_document, token_vocabulary, target_response, T, B)\n",
    "print(\"\\nOptimized sub-document:\", ''.join(optimized_sub_document))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from REMOVED_SECRET import TfidfVectorizer\n",
    "from typing import List\n",
    "\n",
    "# Function to load and extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Function to extract keywords from text using TF-IDF\n",
    "def extract_keywords(text: str, num_keywords: int = 10) -> List[str]:\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray()[0]\n",
    "    keyword_indices = scores.argsort()[-num_keywords:][::-1]\n",
    "    keywords = [feature_names[i] for i in keyword_indices]\n",
    "    return keywords\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "REMOVED_SECRET"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
